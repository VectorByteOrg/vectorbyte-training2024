[
  {
    "objectID": "VB_IntroTimeDepData.html#learning-objectives",
    "href": "VB_IntroTimeDepData.html#learning-objectives",
    "title": "VectorByte Methods Training",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand basic concepts in how we understand and model time-dependent population data in VBD applications\nReview basic idea of forecasting\nOverview of course"
  },
  {
    "objectID": "VB_IntroTimeDepData.html#population-dynamics-of-disease",
    "href": "VB_IntroTimeDepData.html#population-dynamics-of-disease",
    "title": "VectorByte Methods Training",
    "section": "Population dynamics of disease",
    "text": "Population dynamics of disease\nThe number of hosts, vectors, pathogens, and infected individuals change over time.\nWe use models models of various types to: - understand relationships - forecast/predict possible future outcomes"
  },
  {
    "objectID": "VB_IntroTimeDepData.html#what-types-of-models",
    "href": "VB_IntroTimeDepData.html#what-types-of-models",
    "title": "VectorByte Methods Training",
    "section": "What types of models?",
    "text": "What types of models?\n\n\n\n\n\n\nFocus on describing/quantifying patterns\nStatistical Models, e.g. regression, time series\n\n\n\nSeek to understand mechanisms, less prediction\nODEs, dynamical systems, Stochastic DEs, Individual Based Models\n\n\n\nWe’ll focus on the tactical end of things here (i.e., no dif eqs)"
  },
  {
    "objectID": "VB_IntroTimeDepData.html#simple-example-a-first-deterministic-model",
    "href": "VB_IntroTimeDepData.html#simple-example-a-first-deterministic-model",
    "title": "VectorByte Methods Training",
    "section": "Simple example: A first (deterministic) model",
    "text": "Simple example: A first (deterministic) model\n\n\nSuppose we model a population in discrete time as \\[\\begin{align*}\nN(t+1) = s N(t) + b(t).\n\\end{align*}\\]\nHere \\(s\\) is the fraction of individuals surviving each time step and \\(b(t)\\) is the number of new births."
  },
  {
    "objectID": "VB_IntroTimeDepData.html#a-first-stochastic-model",
    "href": "VB_IntroTimeDepData.html#a-first-stochastic-model",
    "title": "VectorByte Methods Training",
    "section": "A first (stochastic) model",
    "text": "A first (stochastic) model\nIn that simplest model the population can’t go extinct!\n\nWhat if the number of births vary? \\[\\begin{align*}\nN(t+1) = s N(t) + b(t) +W(t)\n\\end{align*}\\]\nHere \\(W(t)\\) is process uncertainty/stochasticity: we assume it is drawn from a particular distribution, and each year/time we observe a particular value \\(w(t)\\). E.g., \\(W(t) \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2)\\).\n\nWhat might we see? When would the population go extinct?"
  },
  {
    "objectID": "VB_IntroTimeDepData.html#observation-models",
    "href": "VB_IntroTimeDepData.html#observation-models",
    "title": "VectorByte Methods Training",
    "section": "Observation Models",
    "text": "Observation Models\nWe also have to go out into the field and take some observations of the populations.\nLet’s say that we observe \\(N_{\\mathrm{obs}}(t)\\) individuals at time \\(t\\). How does this relate to the true population size? One possibility is: \\[\\begin{align*}\nN_{\\mathrm{obs}}(t) = N(t) + V(t)\n\\end{align*}\\] where \\(V(t)\\) is our “observation uncertainty”, and all together this equation describes our observation model."
  },
  {
    "objectID": "VB_IntroTimeDepData.html#observation-process-matters",
    "href": "VB_IntroTimeDepData.html#observation-process-matters",
    "title": "VectorByte Methods Training",
    "section": "Observation process matters",
    "text": "Observation process matters\nAnalysis approach depends on not only the goal of the modeling exercise (understanding vs forecasting/prediction) but also on details of the way that data are observed\n\nevenly or unevenly spaced observations\nconsistent sampling locations\ntypes of observation/instrument (automated? trap type?)\n\nAlthough we focus on tools this week, we’ll also talk about general modeling considerations and how to think some of these as we conduct an analyses."
  },
  {
    "objectID": "VB_IntroTimeDepData.html#forecasting-process",
    "href": "VB_IntroTimeDepData.html#forecasting-process",
    "title": "VectorByte Methods Training",
    "section": "Forecasting process",
    "text": "Forecasting process\nsomething about forecasting"
  },
  {
    "objectID": "VB_IntroTimeDepData.html#outline-of-course",
    "href": "VB_IntroTimeDepData.html#outline-of-course",
    "title": "VectorByte Methods Training",
    "section": "Outline of Course",
    "text": "Outline of Course\n\nIntroductions and Goals\nRegression refresher focusing on diagnostics and transformations\nRegression approaches for time dependent data – basics plus time dependent predictors, transformations, simple AR\nAnalysis of evenly-spaced data: basic time-series methods\nAbundance data from VecDyn and NEON + climate and meteorological variables\nAdvanced modeling with Gaussian Process Models"
  },
  {
    "objectID": "GP_Solutions.html",
    "href": "GP_Solutions.html",
    "title": "GP_Solutions",
    "section": "",
    "text": "Libraries\n\nlibrary(mvtnorm)\nlibrary(laGP)\nlibrary(hetGP)\nlibrary(ggplot2)\n\n\n\nHetGP (sin wave eg)\n\n# Your turn\nset.seed(26)\nn &lt;- 8 # number of points\nX &lt;- matrix(seq(0, 2*pi, length= n), ncol=1) # build inputs \ny &lt;- 5*sin(X) + rnorm(n, 0 , 2) # response with some noise\n\n# Predict on this set\nXX &lt;- matrix(seq(-0.5, 2*pi + 0.5, length= 100), ncol=1)\n\n# Data visualization\nplot(X, y)\n\n\n\n\n\n\n\n# ------ Solutions ------------------------------\n\nhet_fit &lt;- hetGP::mleHetGP(X, y)\nhet_pred &lt;- predict(het_fit, XX)\n\nmean &lt;- het_pred$mean\ns2 &lt;- het_pred$sd2 + het_pred$nugs\n\nyy &lt;- 5*sin(XX)\n\npar(mfrow = c(1, 1), mar = c(4, 4, 4, 1))\nplot(X, y, ylim = c(-10, 10))\nlines(XX, yy, col = 3)\nlines(XX, mean, col = 2)\nlines(XX, mean + 2 * sqrt(s2), col = 4)\nlines(XX, mean - 2 * sqrt(s2), col = 4)\n\n\n\n\n\n\n\n# You can check the nuggets (each one will be different)\nnugs &lt;- het_pred$nugs\nsummary(nugs)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.904   3.927   3.931   3.936   3.949   3.971 \n\n\n\n\nChallenges\nWe need to load the data and the functions\n\n# Pulling the data from the NEON data base. \ntarget &lt;- readr::read_csv(\"https://data.ecoforecast.org/neon4cast-targets/ticks/ticks-targets.csv.gz\", guess_max = 1e1)\n\nRows: 587 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): site_id, variable, iso_week\ndbl  (1): observation\ndate (1): datetime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# transforms y\nf &lt;- function(x) {\n  y &lt;- log(x + 1)\n  return(y)\n}\n\n# This function back transforms the input argument\nfi &lt;- function(y) {\n  x &lt;- exp(y) - 1\n  return(x)\n}\n\n# This function tells us the iso-week number given the date\nfx.iso_week &lt;- function(datetime){\n  # Gives ISO-week in the format yyyy-w## and we extract the ##\n  x1 &lt;- as.numeric(stringr::str_sub(ISOweek::ISOweek(datetime), 7, 8)) # find iso week #\n  return(x1)\n}\n\nfx.sin &lt;- function(datetime, f1 = fx.iso_week){\n  # identify iso week#\n  x &lt;- f1(datetime) \n  # calculate sin value for that week\n  x2 &lt;- (sin(2*pi*x/106))^2 \n  return(x2)\n}\n\n\nFit a GP Model for the location “SERC” i.e. site_number = 7.\nJust change site = 7\n\nsite_number &lt;- 7 # (site_number = 4) for the other challenge\n\n# Obtaining site name\nsite_names &lt;- unique(target$site_id)\n\n# Subsetting all the data at that location\ndf &lt;- subset(target, target$site_id == site_names[site_number])\n\n# extracting only the datetime and obs columns\ndf &lt;- df[, c(\"datetime\", \"observation\")]\n\n# Selecting a date before which we consider everything as training data and after this is testing data.\ncutoff = as.Date('2020-12-31')\ndf_train &lt;- subset(df, df$datetime &lt;= cutoff)\ndf_test &lt;- subset(df, df$datetime &gt; cutoff)\n\n# Setting up iso-week and sin wave predictors by calling the functions\nX1 &lt;- fx.iso_week(df_train$datetime) # range is 1-53\nX2 &lt;- fx.sin(df_train$datetime) # range is 0 to 1\n\n# Centering the iso-week by diving by 53\nX1c &lt;- X1/ 53\n\n# We combine columns centered X1 and X2, into a matrix as our input space\nX &lt;- as.matrix(cbind.data.frame(X1c, X2))\nhead(X)\n\n           X1c        X2\n[1,] 0.3584906 0.8150439\n[2,] 0.3962264 0.8974272\n[3,] 0.4528302 0.9782005\n[4,] 0.5094340 0.9991219\n[5,] 0.6226415 0.8587536\n[6,] 0.6792453 0.7150326\n\ny_obs &lt;- df_train$observation\ny &lt;- f(y_obs) # transform y\n\n# A very small value for stability\neps &lt;- sqrt(.Machine$double.eps) \n  \n# Priors for theta and g. \nd &lt;- darg(list(mle=TRUE, min =eps, max=5), X)\ng &lt;- garg(list(mle=TRUE, min = eps, max = 1), y)\n\n# Fitting a GP with our data, and some starting values for theta and g\ngpi &lt;- newGPsep(X, y, d = 0.1, g = 1, dK = T)\n\n# Jointly infer MLE for all parameters\nmle &lt;- jmleGPsep(gpi, drange = c(d$min, d$max), grange = c(g$min, g$max), \n                 dab = d$ab, gab=  g$ab)\n\n# Create a grid from start date in our data set to one year in future (so we forecast for next season)\nstartdate &lt;- as.Date(min(df$datetime))# identify start week\ngrid_datetime &lt;- seq.Date(startdate, Sys.Date() + 365, by = 7) # create sequence\n\n# Build the input space for the predictive space (All weeks from 04-2014 to 07-2025)\nXXt1 &lt;- fx.iso_week(grid_datetime)\nXXt2 &lt;- fx.sin(grid_datetime)\n\n# Standardize\nXXt1c &lt;- XXt1/53\n\n# Store inputs as a matrix\nXXt &lt;- as.matrix(cbind.data.frame(XXt1c, XXt2))\n\n# Make predictions using predGP with the gp object and the predictive set\nppt &lt;- predGPsep(gpi, XXt) \n\n# Now we store the mean as our predicted response i.e. density along with quantiles\nyyt &lt;- ppt$mean\nq1t &lt;- ppt$mean + qnorm(0.025,0,sqrt(diag(ppt$Sigma))) #lower bound\nq2t &lt;- ppt$mean + qnorm(0.975,0,sqrt(diag(ppt$Sigma))) # upper bound\n\n# Back transform our data to original\ngp_yy &lt;- fi(yyt)\ngp_q1 &lt;- fi(q1t)\ngp_q2 &lt;- fi(q2t)\n\n# Plot the observed points\nplot(as.Date(df$datetime), df$observation,\n       main = paste(site_names[site_number]), col = \"black\",\n       xlab = \"Dates\" , ylab = \"Abundance\",\n       # xlim = c(as.Date(min(df$datetime)), as.Date(cutoff)),\n       ylim = c(min(df_train$observation, gp_yy, gp_q1), max(df_train$observation, gp_yy, gp_q2)* 1.05))\n\n# Plot the testing set data \npoints(as.Date(df_test$datetime), df_test$observation, col =\"black\", pch = 19)\n\n# Line to indicate seperation between train and test data\nabline(v = as.Date(cutoff), lwd = 2)\n\n# Add the predicted response and the quantiles\nlines(grid_datetime, gp_yy, col = 4, lwd = 2)\nlines(grid_datetime, gp_q1, col = 4, lwd = 1.2, lty = 2)\nlines(grid_datetime, gp_q2, col = 4, lwd = 1.2, lty =2)\n\n\n\n\n\n\n\n# Obtain true observed values for testing set\nyt_true &lt;- f(df_test$observation)\n\n# FInd corresponding predictions from our model in the grid we predicted on\nyt_pred &lt;- yyt[which(grid_datetime  %in% df_test$datetime)]\n\n# calculate RMSE\nrmse &lt;- sqrt(mean((yt_true - yt_pred)^2))\nrmse\n\n[1] 0.9553652\n\n\n\n\nUse an environmental predictor in your model. Following is a function fx.green that creates the variable given the datetime and the location.\nHere is a snippet of the supporting file that you will use; You can look into the data.frame and try to plot ker for one site at a time and see what it yields.\n\nsource('code/df_spline.R') # sources the cript to make greenness predictor\nhead(df_green) # how the dataset looks\n\n  site iso ker\n1 BLAN   1   0\n2 BLAN   2   0\n3 BLAN   3   0\n4 BLAN   4   0\n5 BLAN   5   0\n6 BLAN   6   0\n\n# The function to create the environmental predictor similar to iso-week and sin wave\nfx.green &lt;- function(datetime, site, site_info = df_green){\n  ker &lt;- NULL\n  iso &lt;- fx.iso_week(datetime) # identify iso week\n  df.iso &lt;- cbind.data.frame(datetime, iso) # combine date with iso week\n  sites.ker &lt;- subset(site_info, site == site)[,2:3] # obtain kernel for location\n  df.green &lt;- df.iso %&gt;% left_join(sites.ker, by = 'iso') # join dataframes by iso week\n  ker &lt;- df.green$ker # return kernel\n  return(ker)\n}\n\n\nChoose a site\nset up X3 using fx_green\nScale X3\n\nSetting up the target dataframe\n\n# Obtaining site name\nsite_names &lt;- unique(target$site_id)\n\n# Subsetting all the data at that location\ndf &lt;- subset(target, target$site_id == site_names[site_number])\n\n# extracting only the datetime and obs columns\ndf &lt;- df[, c(\"datetime\", \"observation\")]\n\n# Selecting a date before which we consider everything as training data and after this is testing data.\ncutoff = as.Date('2020-12-31')\ndf_train &lt;- subset(df, df$datetime &lt;= cutoff)\ndf_test &lt;- subset(df, df$datetime &gt; cutoff)\n\nAdding Greenness\n\n# Choose location \nsite_number = 7\ndf_green_site2 &lt;- subset(df_green, site == site_names[site_number])\n\n# Setting up iso-week and sin wave predictors by calling the functions\nX1 &lt;- fx.iso_week(df_train$datetime) # range is 1-53\nX2 &lt;- fx.sin(df_train$datetime) # range is 0 to 1\n\n# you need datetime, site name and the df_green dataset.\nX3 &lt;- fx.green(df_train$datetime, site = site_names[site_number], site_info = df_green_site2)\n\n# Centering the iso-week by diving by 53\nX1c &lt;- X1/ 53\n\n# Scale X3\nX3c &lt;- (X3 - min(X3))/ (max(X3)- min(X3))\n\n# We combine columns centered X1 and X2, into a matrix as our input space\nX &lt;- as.matrix(cbind.data.frame(X1c, X2, X3c))\nhead(X)\n\n           X1c        X2        X3c\n[1,] 0.3584906 0.8150439 0.84585476\n[2,] 0.3962264 0.8974272 0.97749530\n[3,] 0.4528302 0.9782005 0.96806621\n[4,] 0.5094340 0.9991219 0.75535447\n[5,] 0.6226415 0.8587536 0.23957183\n[6,] 0.6792453 0.7150326 0.09822483\n\ny_obs &lt;- df_train$observation\ny &lt;- f(y_obs) # transform y\n\n# A very small value for stability\neps &lt;- sqrt(.Machine$double.eps) \n  \n# Priors for theta and g. \nd &lt;- darg(list(mle=TRUE, min =eps, max=5), X)\ng &lt;- garg(list(mle=TRUE, min = eps, max = 1), y)\n\n# Fitting a GP with our data, and some starting values for theta and g\ngpi &lt;- newGPsep(X, y, d = 0.1, g = 1, dK = T)\n\n# Jointly infer MLE for all parameters\nmle &lt;- jmleGPsep(gpi, drange = c(d$min, d$max), grange = c(g$min, g$max), \n                 dab = d$ab, gab=  g$ab)\n\n# Create a grid from start date in our data set to one year in future (so we forecast for next season)\nstartdate &lt;- as.Date(min(df$datetime))# identify start week\ngrid_datetime &lt;- seq.Date(startdate, Sys.Date() + 365, by = 7) # create sequence\n\n# Build the input space for the predictive space (All weeks from 04-2014 to 07-2025)\nXXt1 &lt;- fx.iso_week(grid_datetime)\nXXt2 &lt;- fx.sin(grid_datetime)\nXXt3 &lt;- fx.green(grid_datetime, site = site_names[site_nunber], site_info = df_green_site2)\n\n# Standardize\nXXt1c &lt;- XXt1/53\nXXt3 &lt;- (XXt3 - min(XXt3))/ (max(XXt3)- min(XXt3))\n\n# Store inputs as a matrix\nXXt &lt;- as.matrix(cbind.data.frame(XXt1c, XXt2, XXt3))\n\n# Make predictions using predGP with the gp object and the predictive set\nppt &lt;- predGPsep(gpi, XXt) \n\n# Now we store the mean as our predicted response i.e. density along with quantiles\nyyt &lt;- ppt$mean\nq1t &lt;- ppt$mean + qnorm(0.025,0,sqrt(diag(ppt$Sigma))) #lower bound\nq2t &lt;- ppt$mean + qnorm(0.975,0,sqrt(diag(ppt$Sigma))) # upper bound\n\n# Back transform our data to original\ngp_yy &lt;- fi(yyt)\ngp_q1 &lt;- fi(q1t)\ngp_q2 &lt;- fi(q2t)\n\n# Plot the observed points\nplot(as.Date(df$datetime), df$observation,\n       main = paste(site_names[site_number]), col = \"black\",\n       xlab = \"Dates\" , ylab = \"Abundance\",\n       # xlim = c(as.Date(min(df$datetime)), as.Date(cutoff)),\n       ylim = c(min(df_train$observation, gp_yy, gp_q1), max(df_train$observation, gp_yy, gp_q2)* 1.05))\n\n# Plot the testing set data \npoints(as.Date(df_test$datetime), df_test$observation, col =\"black\", pch = 19)\n\n# Line to indicate seperation between train and test data\nabline(v = as.Date(cutoff), lwd = 2)\n\n# Add the predicted response and the quantiles\nlines(grid_datetime, gp_yy, col = 4, lwd = 2)\nlines(grid_datetime, gp_q1, col = 4, lwd = 1.2, lty = 2)\nlines(grid_datetime, gp_q2, col = 4, lwd = 1.2, lty =2)\n\n\n\n\n\n\n\n# Obtain true observed values for testing set\nyt_true &lt;- f(df_test$observation)\n\n# FInd corresponding predictions from our model in the grid we predicted on\nyt_pred &lt;- yyt[which(grid_datetime  %in% df_test$datetime)]\n\n# calculate RMSE\nrmse &lt;- sqrt(mean((yt_true - yt_pred)^2))\nrmse\n\n[1] 0.9532001\n\n\n\n\nFit a GP Model for all the locations (More advanced).\n\n# GP function. This can be varied but easiest way is to just take in X, y, XX and return the predicted means and bounds. \n\ngpfit &lt;- function(X, y , XXt){\n  eps &lt;- sqrt(.Machine$double.eps) \n  \n  # Priors for theta and g. \n  d &lt;- darg(list(mle=TRUE, min =eps, max=5), X)\n  g &lt;- garg(list(mle=TRUE, min = eps, max = 1), y)\n\n  # Fitting a GP with our data, and some starting values for theta and g\n  gpi &lt;- newGPsep(X, y, d = 0.1, g = 1, dK = T)\n\n  # Jointly infer MLE for all parameters\n  mle &lt;- jmleGPsep(gpi, drange = c(d$min, d$max), grange = c(g$min, g$max), \n                 dab = d$ab, gab=  g$ab)\n\n  ppt &lt;- predGPsep(gpi, XXt) \n\n  # Now we store the mean as our predicted response i.e. density along with quantiles\n  yyt &lt;- ppt$mean\n  q1t &lt;- ppt$mean + qnorm(0.025,0,sqrt(diag(ppt$Sigma))) #lower bound\n  q2t &lt;- ppt$mean + qnorm(0.975,0,sqrt(diag(ppt$Sigma))) # upper bound\n\n  # Back transform our data to original\n  gp_yy &lt;- fi(yyt)\n  gp_q1 &lt;- fi(q1t)\n  gp_q2 &lt;- fi(q2t)\n  \n  return(list(mean = gp_yy, s2 = diag(ppt$Sigma), q1 = gp_q1, q2 = gp_q2))\n}\n\n\nsite_number &lt;- 7 # (site_number = 4) for the other challenge\n\n# Obtaining site name\nsite_names &lt;- unique(target$site_id)\n\n# extracting only the datetime and obs columns\ndf &lt;- target[, c(\"datetime\", \"site_id\", \"observation\")]\n\ncutoff = as.Date('2020-12-31')\n\n# This was always be prediction set\nstartdate &lt;- as.Date(min(df$datetime))# identify start week\ngrid_datetime &lt;- seq.Date(startdate, Sys.Date() + 365, by = 7) # create sequence\n\n# You can pre process to have y transformed or have it in the loop.\nrmse &lt;- matrix(nrow = length(site_names), ncol = 1) # if rmse\n\nfor(i in 1:length(site_names)){\n  \n  df_site &lt;- subset(df, site_id == site_names[i])\n  \n  # cutoff for sites\n  df_train &lt;- subset(df_site, df_site$datetime &lt;= cutoff)\n  df_test &lt;- subset(df_site, df_site$datetime &gt; cutoff)\n  \n  df_green_site &lt;- subset(df_green, site == site_names[i])\n  \n  X1 &lt;- fx.iso_week(df_train$datetime) # range is 1-53\n  X2 &lt;- fx.sin(df_train$datetime) # range is 0 to 1\n  X3 &lt;- fx.green(df_train$datetime, site = site_names[site_number], site_info = df_green_site) # optional add\n\n  X1c &lt;- X1/ 53\n  X3c &lt;- (X3 - min(X3))/ (max(X3)- min(X3))\n  X &lt;- as.matrix(cbind.data.frame(X1c, X2, X3c))\n\n  y_obs &lt;- df_train$observation # only at this location\n  y &lt;- f(y_obs) # transform y\n\n  XXt1 &lt;- fx.iso_week(grid_datetime)\n  XXt2 &lt;- fx.sin(grid_datetime)\n  XXt3 &lt;- fx.green(grid_datetime, site = site_names[site_nunber], site_info =\n                   df_green_site)\n\n  # Standardize\n  XXt1c &lt;- XXt1/53\n  XXt3 &lt;- (XXt3 - min(XXt3))/ (max(XXt3)- min(XXt3))\n  XXt &lt;- as.matrix(cbind.data.frame(XXt1c, XXt2, XXt3))\n \n  fit &lt;- gpfit(X = X, y = y, XX = XXt)\n  \n  # Make plots\n  plot(as.Date(df_site$datetime), df_site$observation,\n       main = paste(site_names[i]), col = \"black\",\n       xlab = \"Dates\" , ylab = \"Abundance\",\n       # xlim = c(as.Date(min(df$datetime)), as.Date(cutoff)),\n       ylim = c(min(df_train$observation, df_test$observation, fit$q1),\n                max(df_train$observation,df_test$observation, fit$q2)* 1.05))\n\n  points(as.Date(df_test$datetime), df_test$observation, col =\"black\", pch = 19)\n  abline(v = as.Date(cutoff), lwd = 2)\n\n  # Add the predicted response and the quantiles\n  lines(grid_datetime, fit$mean, col = 4, lwd = 2)\n  lines(grid_datetime, fit$q1, col = 4, lwd = 1.2, lty = 2)\n  lines(grid_datetime, fit$q2, col = 4, lwd = 1.2, lty =2)\n  \n  \n  yt_true &lt;- f(df_test$observation)\n  yt_pred &lt;- f(fit$mean[which(grid_datetime  %in% df_test$datetime)])\n\n  # calculate RMSE\n  rmse[i, ] &lt;- sqrt(mean((yt_true - yt_pred)^2))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrownames(rmse) &lt;- site_names\nprint(rmse)\n\n          [,1]\nBLAN 1.1173576\nKONZ 0.7779042\nLENO 0.6851390\nORNL 0.9205952\nOSBS 1.2057049\nSCBI 0.8610973\nSERC 0.9532001\nTALL 1.0072034\nUKFS 1.5292849"
  },
  {
    "objectID": "Stats_review_soln.html",
    "href": "Stats_review_soln.html",
    "title": "VectorByte Methods Training 2024",
    "section": "",
    "text": "Main materials\nBack to stats review\n\nQuestion 1: For the six-sided fair die, what is f_k if k=7? k=1.5?\n\nAnswer: both of these are zero, because the die cannot take these values.\n    \n\n\nQuestion 2: For the fair 6-sided die, what is F(3)? F(7)? F(1.5)?\nAnswer: The CDF total probability of having a value less than or equal to its argument. Thus F(3)= 1/2, F(7)=1, and F(1.5)=1/6\n    \n\n\nQuestion 3: For a normal distribution with mean 0, what is F(0)?\n\nAnswer: The normal distribution is symmetric around its mean, with half of its probability on each side. Thus, F(0)=1/2\n    \n\n\nQuestion 4: Summation Notation Practice\n\n\n\ni\n1\n2\n3\n4\n\n\n\n\nZ_i\n2.0\n-2.0\n3.0\n-3.0\n\n\n\n\nCompute \\sum_{i=1}^{4}{z_i} = 0 \nCompute \\sum_{i=1}^4{(z_i - \\bar{z})^2} = 26 \nWhat is the sample variance? Assume that the z_i are i.i.d.. Note that i.i.d.~stands for “independent and identically distributed”. \n\nSolution: \ns^2= \\frac{\\sum_{i=1}^N(Y_i - \\bar{Y})^2}{N-1} = \\frac{26}{3}\n= 8\\times \\frac{2}{3}\n \n\nFor a general set of N numbers, \\{X_1, X_2, \\dots, X_N \\} and \\{Y_1, Y_2, \\dots, Y_N \\} show that \n\\sum_{i=1}^N{(X_i - \\bar{X})(Y_i - \\bar{Y})} = \\sum_{i=1}^N{(X_i-\\bar{X})Y_i}\n\n\n Solution: First, we multiply through and distribute: \n\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y}) = \\sum_{i=1}^N(X_i-\\bar{X})Y_i\n- \\sum_{i=1}^N(X_i-\\bar{X})\\bar{Y}\n Next note that \\bar{Y} (the mean of the Y_is) doesn’t depend on i so we can pull it out of the summation: \n\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y}) = \\sum_{i=1}^N(X_i-\\bar{X})Y_i\n- \\bar{Y} \\sum_{i=1}^N(X_i-\\bar{X}).\n Finally, the last sum must be zero because \n\\sum_{i=1}^N(X_i-\\bar{X}) = \\sum_{i=1}^N X_i- \\sum_{i=1}^N \\bar{X} = N\\bar{X} - N\\bar{X}=0.\n Thus \\begin{align*}\n\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y}) &= \\sum_{i=1}^N(X_i-\\bar{X})Y_i - \\bar{Y}\\times 0\\\\\n& = \\sum_{i=1}^N(X_i-\\bar{X})Y_i.\n\\end{align*}\n    \n\n\nQuestion 5: Properites of Expected Values\nUsing the definition of an expected value above and with X and Y having the same probability distribution, show that:\n\\begin{align*}\n\\text{E}[X+Y]  & = \\text{E}[X] + \\text{E}[Y]\\\\  \n& \\text{and} \\\\\n\\text{E}[cX]  & = c\\text{E}[X]. \\\\\n\\end{align*}\nGiven these, and the fact that \\mu=\\text{E}[X], show that:\n\\begin{align*}\n\\text{E}[(X-\\mu)^2]  = \\text{E}[X^2] - (\\text{E}[X])^2\n\\end{align*}\nThis gives a formula for calculating variances (since \\text{Var}(X)= \\text{E}[(X-\\mu)^2]).\nSolution: Assuming X and Y are both i.i.d. with distribution f(x). The expectation of X+Y is defined as \\begin{align*}\n\\text{E}[X+Y]  & =  \\int (X+Y) f(x)dx \\\\\n              & =  \\int (X f(x) +Y f(x))dx  \\\\\n              & =  \\int X f(x)dx  +\\int Y f(x)dx  \\\\\n               & = \\text{E}[X] + \\text{E}[Y]  \n\\end{align*} Similarly \\begin{align*}\n\\text{E}[cX]   & =  \\int cXf(x)dx \\\\\n              & =  c \\int Xf(x) dx  \\\\\n              & = c\\text{E}[X]. \\\\\n\\end{align*} Thus we can re-write: \\begin{align*}\n\\text{E}[(X-\\mu)^2]  & = \\text{E}[ X^2 - 2X\\mu + \\mu^2] \\\\\n                        & = \\text{E}[X^2] - 2\\mu\\text{E}[X] + \\mu^2 \\\\\n                        & = \\text{E}[X^2] -2\\mu^2 + \\mu^2 \\\\\n                        & = \\text{E}[X^2] - \\mu^2 \\\\\n& = \\text{E}[X^2] - (\\text{E}[X])^2.\n\\end{align*}\n   \n\n\nQuestion 6: Functions of Random Variables\nSuppose that \\mathrm{E}[X]=\\mathrm{E}[Y]=0, \\mathrm{var}(X)=\\mathrm{var}(Y)=1, and \\mathrm{corr}(X,Y)=0.5.\n\nCompute \\mathrm{E}[3X-2Y]; and\n\\mathrm{var}(3X-2Y).\nCompute \\mathrm{E}[X^2].\n\nSolution:\n\nUsing the properties of expectations, we can re-write this as: \\begin{align*}\n\\mathrm{E}[3X-2Y] & = \\mathrm{E}[3X] + \\mathrm{E}[-2Y]\\\\\n& = 3 \\mathrm{E}[X] -2 \\mathrm{E}[Y]\\\\\n& = 3 \\times 0 -2 \\times 0\\\\\n&=0\n\\end{align*}\n\n\nUsing the properties of variances, we can re-write this as: \\begin{align*}\n\\mathrm{var}(3X-2Y) & = 3^2\\text{Var}(X) + (-2)^2\\text{Var}(Y) + 2(3)(-2)\\text{Cov}(XY)\\\\\n& =  9 \\times 1 + 4 \\times 1 -12 \\text{Corr}(XY)\\sqrt{\\text{Var}(X)\\text{Var}(Y)}\\\\\n& = 9+4 -12 \\times 0.5\\times1\\\\\n&=7\n\\end{align*}\n\n\nRecalling from Question 5 that the variance is \\mathrm{var}(X) = \\text{E}[X^2] - (\\text{E}[X])^2, we can re-arrange to obtain: \\begin{align*}\n\\mathrm{E}[X^2] & = \\mathrm{var}(X) + (\\mathrm{E}[X])^2\\\\\n& = 1+(0)^2 \\\\\n& =1\n\\end{align*}\n\n\n\nThe Sampling Distribution\nSuppose we have a random sample \\{Y_i, i=1,\\dots,N \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,4) for i=1,\\ldots,N.\n\nWhat is the variance of the sample mean?\n\n\\displaystyle \\mathrm{Var}(\\bar{Y}) =\n\\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i=1}^N Y_i\\right) =\n\\frac{N}{N^2}\\mathrm{Var}(Y) =\\frac{4}{N}.\nThis is the derivation for the variance of the sampling distribution.\n \n\nWhat is the expectation of the sample mean?\n\n\\displaystyle\\mathrm{E}[\\bar{Y}] = \\frac{N}{N}\\mathrm{E}(Y) = \\mu. This is the mean of the sampling distribution.\n\n\nWhat is the variance for another i.i.d. realization Y_{ N+1}?\n\n\\displaystyle \\mathrm{Var}(Y) = 4, because this is a sample directly from the population distribution.\n \n\nWhat is the standard error of \\bar{Y}?\n\nHere, again, we are looking at the distribution of the sample mean, so we must consider the sampling distribution, and the standard error (aka the standard distribution) is just the square root of the variance from part i.\n\\displaystyle \\mathrm{se}(\\bar{Y}) = \\sqrt{\\mathrm{Var}(\\bar{Y})} =\\frac{2}{\\sqrt{N}}.\n\n\nHypothesis Testing and Confidence Intervals\nSuppose we sample some data \\{Y_i, i=1,\\dots,n \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,\\sigma^2) for i=1,\\ldots,n, and that you want to test the null hypothesis H_0: ~\\mu=12 vs. the alternative H_a: \\mu \\neq ` r m`, at the 0.05 significance level.\n\nWhat test statistic would you use? How do you estimate \\sigma?\nWhat is the distribution for this test statistic if the null is true?\nWhat is the distribution for the test statistic if the null is true and n \\rightarrow \\infty?\nDefine the test rejection region. (I.e., for what values of the test statistic would you reject the null?)\nHow would compute the p-value associated with a particular sample?\nWhat is the 95% confidence interval for \\mu? How should one interpret this interval?\nIf \\bar{Y} = 11, s_y = 1, and n=9, what is the test result? What is the 95% CI for \\mu?\n\n  \nThis question is asking you think about the hypothesis that the mean of your distribution is equal to 12. I give you the distribution of the data themselves (i.e., that they’re normal). To test the hypothesis, you work with the sampling distribution (i.e., the distribution of the sample mean) which is: \\bar{Y}\\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right).\n\nIf we knew \\sigma, we could use as our test statistic z=\\displaystyle \\frac{\\bar{y} - 12}{\\sigma/\\sqrt{n}}. However, here we need to estimate \\sigma so we use z=\\displaystyle \\frac{\\bar{y} - 12}{s_y/\\sqrt{n}} where \\displaystyle s_{y} = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\bar{y})^2}{n-1}}.\n\n\nIf the null is true, the z \\sim t_{n-1}(0,1). Since we estimate the mean frm the data, the degrees of freedom is n-1.\n\n\nAs n approaches infinity, t_{n-1}(0,1) \\rightarrow N(0,1).\n\n\nYou reject the null for \\{z: |z| &gt; t_{n-1,\\alpha/2}\\}.\n\n\nThe p-value is 2\\Pr(Z_{n-1} &gt;|z|). \n\n\nThe 95% CI is \\bar{Y} \\pm \\frac{s_{y}}{\\sqrt{n}} t_{n-1,\\alpha/2}.\n\nFor 19 out of 20 different samples, an interval constructed in this way will include the true value of the mean, \\mu. \n\nz = (11-12)/(1/3) = -3 and 2\\Pr(Z_{8} &gt;|z|) = .017, so we do reject the null.  The 95% CI for \\mu is 11 \\pm \\frac{1}{3}2.3 = (10.23, 11.77)."
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "VectorByte Training Materials 2024",
    "section": "",
    "text": "We will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 3.6 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and tested before the workshop.\nWe will also be using Slack for additional support during the training. Please have these installed in advance. We will have a channel on Slack dedicated to software/hardware issues and troubleshooting.\n\n\n\nWe are assuming familiarity with R basics as well as at least introductory statistics, including up through simple linear regression. If you would like materials to review, we recommend that you do the following:\nReview of R\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter.\nIn addition / alternatively to pre-work element (1), here are some resources for brushing up on R at the end of the Intro R Chapter you can try. There are many more resources online (e.g., this and this ) – pick something that suits your learning style.\n\nReview of statistics\n\nReview background on introductory probability and statistics (solutions to exercises). You can also use the resources on The Multilingual Quantitative Biologist - Basic Data Analyses and Statistics through into linear models.\nReview the conceptual basics of Simple Linear Regression. We’ll refresh fitting linear models in R as well as transformations and diagnostic plots during the workshop."
  },
  {
    "objectID": "materials.html#hardware-and-software",
    "href": "materials.html#hardware-and-software",
    "title": "VectorByte Training Materials 2024",
    "section": "",
    "text": "We will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 3.6 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and tested before the workshop.\nWe will also be using Slack for additional support during the training. Please have these installed in advance. We will have a channel on Slack dedicated to software/hardware issues and troubleshooting."
  },
  {
    "objectID": "materials.html#pre-requisites",
    "href": "materials.html#pre-requisites",
    "title": "VectorByte Training Materials 2024",
    "section": "",
    "text": "We are assuming familiarity with R basics as well as at least introductory statistics, including up through simple linear regression. If you would like materials to review, we recommend that you do the following:\nReview of R\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter.\nIn addition / alternatively to pre-work element (1), here are some resources for brushing up on R at the end of the Intro R Chapter you can try. There are many more resources online (e.g., this and this ) – pick something that suits your learning style.\n\nReview of statistics\n\nReview background on introductory probability and statistics (solutions to exercises). You can also use the resources on The Multilingual Quantitative Biologist - Basic Data Analyses and Statistics through into linear models.\nReview the conceptual basics of Simple Linear Regression. We’ll refresh fitting linear models in R as well as transformations and diagnostic plots during the workshop."
  },
  {
    "objectID": "materials.html#introduction-to-time-dependent-abundance-data",
    "href": "materials.html#introduction-to-time-dependent-abundance-data",
    "title": "VectorByte Training Materials 2024",
    "section": "Introduction to Time Dependent Abundance Data",
    "text": "Introduction to Time Dependent Abundance Data"
  },
  {
    "objectID": "materials.html#introduction-to-the-vecdyn-database1",
    "href": "materials.html#introduction-to-the-vecdyn-database1",
    "title": "VectorByte Training Materials 2024",
    "section": "Introduction to the VecDyn database1",
    "text": "Introduction to the VecDyn database1\n\nThis component will be delivered live & synchronously. The VecDyn website can be found here. It might be an idea to explore this prior to the workshop."
  },
  {
    "objectID": "materials.html#regression-in-r-refresher",
    "href": "materials.html#regression-in-r-refresher",
    "title": "VectorByte Training Materials 2024",
    "section": "Regression in R Refresher",
    "text": "Regression in R Refresher"
  },
  {
    "objectID": "materials.html#time-dependent-regression-analysis",
    "href": "materials.html#time-dependent-regression-analysis",
    "title": "VectorByte Training Materials 2024",
    "section": "Time dependent regression analysis",
    "text": "Time dependent regression analysis"
  },
  {
    "objectID": "materials.html#basics-of-time-series-using-r",
    "href": "materials.html#basics-of-time-series-using-r",
    "title": "VectorByte Training Materials 2024",
    "section": "Basics of Time Series using R",
    "text": "Basics of Time Series using R"
  },
  {
    "objectID": "materials.html#gaussian-process-models-gps-for-time-dependent-data",
    "href": "materials.html#gaussian-process-models-gps-for-time-dependent-data",
    "title": "VectorByte Training Materials 2024",
    "section": "Gaussian Process models (GPs) for Time Dependent Data",
    "text": "Gaussian Process models (GPs) for Time Dependent Data"
  },
  {
    "objectID": "materials.html#footnotes",
    "href": "materials.html#footnotes",
    "title": "VectorByte Training Materials 2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhat is the difference between VectorBiTE and VectorByte? We are glad you asked! VectorBiTE was an RCN or a research coordination network funded by a 5 year grant from the BBSRC. VectorByte is hosting this training which is a newly funded NSF grant to establish a global open access data platform to study disease vectors. All the databases have transitioned to VectorByte but the legacy options will still be available on the VectorBiTE website.↩︎"
  },
  {
    "objectID": "materials_temp.html",
    "href": "materials_temp.html",
    "title": "VectorByte Training Materials 2024",
    "section": "",
    "text": "We will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 3.6 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and tested before the workshop.\nWe will also be using Slack for additional support during the training. Please have these installed in advance. We will have a channel on Slack dedicated to software/hardware issues and troubleshooting.\n\n\n\nWe are assuming familiarity with R basics as well as at least introductory statistics, including up through simple linear regression. If you would like materials to review, we recommend that you do the following:\nReview of R\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter.\nIn addition / alternatively to pre-work element (1), here are some resources for brushing up on R at the end of the Intro R Chapter you can try. There are many more resources online (e.g., this and this ) – pick something that suits your learning style.\n\nReview of statistics\n\nReview background on introductory probability and statistics (solutions to exercises). You can also use the resources on The Multilingual Quantitative Biologist - Basic Data Analyses and Statistics through into linear models.\nReview the conceptual basics of Simple Linear Regression. We’ll refresh fitting linear models in R as well as transformations and diagnostic plots during the workshop."
  },
  {
    "objectID": "materials_temp.html#hardware-and-software",
    "href": "materials_temp.html#hardware-and-software",
    "title": "VectorByte Training Materials 2024",
    "section": "",
    "text": "We will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 3.6 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and tested before the workshop.\nWe will also be using Slack for additional support during the training. Please have these installed in advance. We will have a channel on Slack dedicated to software/hardware issues and troubleshooting."
  },
  {
    "objectID": "materials_temp.html#pre-requisites",
    "href": "materials_temp.html#pre-requisites",
    "title": "VectorByte Training Materials 2024",
    "section": "",
    "text": "We are assuming familiarity with R basics as well as at least introductory statistics, including up through simple linear regression. If you would like materials to review, we recommend that you do the following:\nReview of R\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter.\nIn addition / alternatively to pre-work element (1), here are some resources for brushing up on R at the end of the Intro R Chapter you can try. There are many more resources online (e.g., this and this ) – pick something that suits your learning style.\n\nReview of statistics\n\nReview background on introductory probability and statistics (solutions to exercises). You can also use the resources on The Multilingual Quantitative Biologist - Basic Data Analyses and Statistics through into linear models.\nReview the conceptual basics of Simple Linear Regression. We’ll refresh fitting linear models in R as well as transformations and diagnostic plots during the workshop."
  },
  {
    "objectID": "materials_temp.html#introduction-to-the-workshop-and-time-dependent-abundance-data",
    "href": "materials_temp.html#introduction-to-the-workshop-and-time-dependent-abundance-data",
    "title": "VectorByte Training Materials 2024",
    "section": "Introduction to the Workshop and Time Dependent Abundance Data",
    "text": "Introduction to the Workshop and Time Dependent Abundance Data\n\nLecture (coming soon)"
  },
  {
    "objectID": "materials_temp.html#regression-in-r-refresher-transformations-and-diagnostics",
    "href": "materials_temp.html#regression-in-r-refresher-transformations-and-diagnostics",
    "title": "VectorByte Training Materials 2024",
    "section": "Regression in R Refresher (Transformations and Diagnostics)",
    "text": "Regression in R Refresher (Transformations and Diagnostics)\n\nLecture Slides\nPractical\nDataset: transforms.csv"
  },
  {
    "objectID": "materials_temp.html#time-dependent-regression-analysis",
    "href": "materials_temp.html#time-dependent-regression-analysis",
    "title": "VectorByte Training Materials 2024",
    "section": "Time dependent regression analysis",
    "text": "Time dependent regression analysis\n\nLecture Slides\nPractical\nDataset: Walton Co, FL Mosquito data"
  },
  {
    "objectID": "materials_temp.html#basics-of-forecasting-time-series-using-r",
    "href": "materials_temp.html#basics-of-forecasting-time-series-using-r",
    "title": "VectorByte Training Materials 2024",
    "section": "Basics of Forecasting Time Series using R",
    "text": "Basics of Forecasting Time Series using R\n\nLecture\nPractical\nDatasets:\n\nNEONphenologyClean.csv\nphenologyWeather.csv\nNEONbeetlesChallenge.csv\ntinyForecastingChallengeCovs.csv"
  },
  {
    "objectID": "materials_temp.html#introduction-to-the-vecdyn-database",
    "href": "materials_temp.html#introduction-to-the-vecdyn-database",
    "title": "VectorByte Training Materials 2024",
    "section": "Introduction to the VecDyn database",
    "text": "Introduction to the VecDyn database\n\nThe VecDyn website.\nAbout the VecDyn API\nOther Materials Coming Soon."
  },
  {
    "objectID": "materials_temp.html#gaussian-process-models-gps-for-time-dependent-data",
    "href": "materials_temp.html#gaussian-process-models-gps-for-time-dependent-data",
    "title": "VectorByte Training Materials 2024",
    "section": "Gaussian Process models (GPs) for Time Dependent Data",
    "text": "Gaussian Process models (GPs) for Time Dependent Data\n\nLecture Slides\nLecture Notes\nPractical\nadditional R code – df_spline.R"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the 2024 VectorByte Training Workshop",
    "section": "",
    "text": "Check out our about and schedule: coming soon! pages to continue.\nInformation about pre-workshop preparation – including software installation, expectations for what you should already be familiar with, and review materials – is available in the pre-work portion of the materials page."
  },
  {
    "objectID": "VB_TimeDepData_practical.html#exploring-the-data",
    "href": "VB_TimeDepData_practical.html#exploring-the-data",
    "title": "VectorByte Methods Training: Regression Methods for Time Dependent Data (practical)",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nAs always, we first want to take a look at the data, to make sure we understand it, and that we don’t have missing or weird values.\n\nmozData&lt;-read.csv(\"data/Culex_erraticus_walton_covariates_aggregated.csv\")\nsummary(mozData)\n\n   Month_Yr          sample_value        MaxTemp          Precip      \n Length:36          Min.   :0.00000   Min.   :16.02   Min.   : 0.000  \n Class :character   1st Qu.:0.04318   1st Qu.:22.99   1st Qu.: 2.162  \n Mode  :character   Median :0.73001   Median :26.69   Median : 4.606  \n                    Mean   :0.80798   Mean   :26.23   Mean   : 5.595  \n                    3rd Qu.:1.22443   3rd Qu.:30.70   3rd Qu.: 7.864  \n                    Max.   :3.00595   Max.   :33.31   Max.   :18.307  \n\n\nWe can see that the minimum observed average number of mosquitoes it zero, and max is only 3 (there are likely many zeros averaged over many days in the month). There don’t appear to be any NAs in the data. In this case the dataset itself is small enough that we can print the whole thing to ensure it’s complete:\n\nmozData\n\n   Month_Yr sample_value  MaxTemp       Precip\n1   2015-01  0.000000000 17.74602  3.303991888\n2   2015-02  0.018181818 17.87269 16.544265802\n3   2015-03  0.468085106 23.81767  2.405651215\n4   2015-04  1.619047619 26.03559  8.974406168\n5   2015-05  0.821428571 30.01602  0.567960943\n6   2015-06  3.005952381 31.12094  4.841342729\n7   2015-07  2.380952381 32.81130  3.849010353\n8   2015-08  1.826347305 32.56245  5.562845324\n9   2015-09  0.648809524 30.55155 10.409724627\n10  2015-10  0.988023952 27.22605  0.337750269\n11  2015-11  0.737804878 24.86768 18.306749680\n12  2015-12  0.142857143 22.46588  5.621475377\n13  2016-01  0.000000000 16.02406  3.550622029\n14  2016-02  0.020202020 19.42057 11.254680803\n15  2016-03  0.015151515 23.13610  4.785664728\n16  2016-04  0.026143791 24.98082  4.580424519\n17  2016-05  0.025252525 28.72884  0.053057634\n18  2016-06  0.833333333 30.96990  6.155417473\n19  2016-07  1.261363636 33.30509  4.496368193\n20  2016-08  1.685279188 32.09633 11.338749182\n21  2016-09  2.617142857 31.60575  2.868288451\n22  2016-10  1.212121212 29.14275  0.000000000\n23  2016-11  1.539772727 24.48482  0.005462681\n24  2016-12  0.771573604 20.46054 11.615521725\n25  2017-01  0.045454545 18.35473  0.000000000\n26  2017-02  0.036363636 23.65584  3.150710053\n27  2017-03  0.194285714 22.53573  1.430094952\n28  2017-04  0.436548223 26.15299  0.499381616\n29  2017-05  1.202020202 28.00173  6.580562663\n30  2017-06  0.834196891 29.48951 13.333939858\n31  2017-07  1.765363128 32.25135  7.493927035\n32  2017-08  0.744791667 31.86476  6.082113434\n33  2017-09  0.722222222 30.60566  4.631037395\n34  2017-10  0.142131980 27.73453 11.567112214\n35  2017-11  0.289772727 23.23140  1.195760473\n36  2017-12  0.009174312 18.93603  4.018254442"
  },
  {
    "objectID": "VB_TimeDepData_practical.html#plotting-the-data",
    "href": "VB_TimeDepData_practical.html#plotting-the-data",
    "title": "VectorByte Methods Training: Regression Methods for Time Dependent Data (practical)",
    "section": "Plotting the data",
    "text": "Plotting the data\nFirst we’ll examine the data itself, including the predictors:\n\nmonths&lt;-dim(mozData)[1]\nt&lt;-1:months ## counter for months in the data set\npar(mfrow=c(3,1))\nplot(t, mozData$sample_value, type=\"l\", lwd=2, \n     main=\"Average Monthly Abundance\", \n     xlab =\"Time (months)\", \n     ylab = \"Average Count\")\nplot(t, mozData$MaxTemp, type=\"l\",\n     col = 2, lwd=2, \n     main=\"Average Maximum Temp\", \n     xlab =\"Time (months)\", \n     ylab = \"Temperature (C)\")\nplot(t, mozData$Precip, type=\"l\",\n     col=\"dodgerblue\", lwd=2,\n     main=\"Average Monthly Precip\", \n     xlab =\"Time (months)\", \n     ylab = \"Precipitation (in)\")\n\n\n\n\n\n\n\n\nVisually we noticed that there may be a bit of clumping in the values for abundance (this is subtle) – in particular, since we have a lot of very small/nearly zero counts, a transform, such as a square root, may spread things out for the abundances. It also looks like both the abundance and temperature data are more cyclical than the precipitation, and thus more likely to be related to each other. There’s also not visually a lot of indication of a trend, but it’s usually worthwhile to consider it anyway. Replotting the abundance data with a transformation:\n\nmonths&lt;-dim(mozData)[1]\nt&lt;-1:months ## counter for months in the data set\nplot(t, sqrt(mozData$sample_value), type=\"l\", lwd=2, \n     main=\"Sqrt Average Monthly Abundance\", \n     xlab =\"Time (months)\", \n     ylab = \"Average Count\")\n\n\n\n\n\n\n\n\nThat looks a little bit better. I suggest we go with this for our response."
  },
  {
    "objectID": "VB_TimeDepData_practical.html#building-a-data-frame",
    "href": "VB_TimeDepData_practical.html#building-a-data-frame",
    "title": "VectorByte Methods Training: Regression Methods for Time Dependent Data (practical)",
    "section": "Building a data frame",
    "text": "Building a data frame\nBefore we get into model building, we always want to build a data frame to contain all of the predictors that we want to consider, at the potential lags that we’re interested in. In the lecture we saw building the AR, sine/cosine, and trend predictors:\n\nt &lt;- 2:months ## to make building the AR1 predictors easier\n\nmozTS &lt;- data.frame(\n  Y=sqrt(mozData$sample_value[t]), # transformed response\n  Yl1=sqrt(mozData$sample_value[t-1]), # AR1 predictor\n  t=t, # trend predictor\n  sin12=sin(2*pi*t/12), \n  cos12=cos(2*pi*t/12) # periodic predictors\n  )\n\nWe will also put in the temperature and precipitation predictors. But we need to think about what might be an appropriate lag. If this were daily or weekly data, we’d probably want to have a fairly sizable lag – mosquitoes take a while to develop, so the number we see today is not likely related to the temperature today. However, since these data are agregated across a whole month, as is the temperature/precipitaion, the current month values are likely to be useful. However, it’s even possible that last month’s values may be so we’ll add those in as well:\n\nmozTS$MaxTemp&lt;-mozData$MaxTemp[t] ## current temps\nmozTS$MaxTempl1&lt;-mozData$MaxTemp[t-1] ## previous temps\nmozTS$Precip&lt;-mozData$Precip[t] ## current precip\nmozTS$Precipl1&lt;-mozData$Precip[t-1] ## previous precip\n\nThus our full dataframe:\n\nsummary(mozTS)\n\n       Y               Yl1               t            sin12         \n Min.   :0.0000   Min.   :0.0000   Min.   : 2.0   Min.   :-1.00000  \n 1st Qu.:0.2951   1st Qu.:0.2951   1st Qu.:10.5   1st Qu.:-0.68301  \n Median :0.8590   Median :0.8590   Median :19.0   Median : 0.00000  \n Mean   :0.7711   Mean   :0.7684   Mean   :19.0   Mean   :-0.01429  \n 3rd Qu.:1.1120   3rd Qu.:1.1120   3rd Qu.:27.5   3rd Qu.: 0.68301  \n Max.   :1.7338   Max.   :1.7338   Max.   :36.0   Max.   : 1.00000  \n     cos12             MaxTemp        MaxTempl1         Precip      \n Min.   :-1.00000   Min.   :16.02   Min.   :16.02   Min.   : 0.000  \n 1st Qu.:-0.68301   1st Qu.:23.18   1st Qu.:23.18   1st Qu.: 1.918  \n Median : 0.00000   Median :27.23   Median :27.23   Median : 4.631  \n Mean   :-0.02474   Mean   :26.47   Mean   :26.44   Mean   : 5.660  \n 3rd Qu.: 0.50000   3rd Qu.:30.79   3rd Qu.:30.79   3rd Qu.: 8.234  \n Max.   : 1.00000   Max.   :33.31   Max.   :33.31   Max.   :18.307  \n    Precipl1     \n Min.   : 0.000  \n 1st Qu.: 1.918  \n Median : 4.631  \n Mean   : 5.640  \n 3rd Qu.: 8.234  \n Max.   :18.307  \n\n\n\nhead(mozTS)\n\n          Y       Yl1 t         sin12         cos12  MaxTemp MaxTempl1\n1 0.1348400 0.0000000 2  8.660254e-01  5.000000e-01 17.87269  17.74602\n2 0.6841675 0.1348400 3  1.000000e+00  6.123234e-17 23.81767  17.87269\n3 1.2724180 0.6841675 4  8.660254e-01 -5.000000e-01 26.03559  23.81767\n4 0.9063270 1.2724180 5  5.000000e-01 -8.660254e-01 30.01602  26.03559\n5 1.7337683 0.9063270 6  1.224647e-16 -1.000000e+00 31.12094  30.01602\n6 1.5430335 1.7337683 7 -5.000000e-01 -8.660254e-01 32.81130  31.12094\n      Precip   Precipl1\n1 16.5442658  3.3039919\n2  2.4056512 16.5442658\n3  8.9744062  2.4056512\n4  0.5679609  8.9744062\n5  4.8413427  0.5679609\n6  3.8490104  4.8413427"
  },
  {
    "objectID": "VB_TimeDepData_practical.html#building-a-first-model",
    "href": "VB_TimeDepData_practical.html#building-a-first-model",
    "title": "VectorByte Methods Training: Regression Methods for Time Dependent Data (practical)",
    "section": "Building a first model",
    "text": "Building a first model\nWe will first build a very simple model – just a trend – to practice building the model, checking diagnostics, and plotting predictions.\n\nmod1&lt;-lm(Y ~ t, data=mozTS)\nsummary(mod1)\n\n\nCall:\nlm(formula = Y ~ t, data = mozTS)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.81332 -0.47902  0.03671  0.37384  0.87119 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.904809   0.178421   5.071  1.5e-05 ***\nt           -0.007038   0.008292  -0.849    0.402    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4954 on 33 degrees of freedom\nMultiple R-squared:  0.02136,   Adjusted R-squared:  -0.008291 \nF-statistic: 0.7204 on 1 and 33 DF,  p-value: 0.4021\n\n\nThe model output indicates that this model is not useful – the trend is not significant and it only explains about 2% of the variability. Let’s plot the predictions:\n\n## plot points and fitted lines\nplot(Y~t, data=mozTS, col=1, type=\"l\")\nlines(t, mod1$fitted, col=\"dodgerblue\", lwd=2)\n\n\n\n\n\n\n\n\nNot good – we’ll definitely need to try something else! Remember that since we’re using a linear model for this, that we should check our residual plots as usual, and then also plot the acf of the residuals:\n\npar(mfrow=c(1,3), mar=c(4,4,2,0.5))   \n\n## studentized residuals vs fitted\nplot(mod1$fitted, rstudent(mod1), col=1,\n     xlab=\"Fitted Values\", \n     ylab=\"Studentized Residuals\", \n     pch=20, main=\"AR 1 only model\")\n\n## qq plot of studentized residuals\nqqnorm(rstudent(mod1), pch=20, col=1, main=\"\" )\nabline(a=0,b=1,lty=2, col=2)\n\n## histogram of studentized residuals\nhist(rstudent(mod1), col=1, \n     xlab=\"Studentized Residuals\", \n     main=\"\", border=8)\n\n\n\n\n\n\n\n\nThis doesn’t look really bad, although the histogram might be a bit weird. Finally the acf\n\nacf(mod1$residuals)\n\n\n\n\n\n\n\n\nThis is where we can see that we definitely aren’t able to capture the pattern. There’s substantial autocorrelation left at a 1 month lag, and around 6 months.\nFinally, for moving forward, we can extract the BIC for this model so that we can compare with other models that you’ll build next.\n\nn&lt;-length(t)\nextractAIC(mod1, k=log(n))[2]\n\n[1] -44.11057"
  },
  {
    "objectID": "schedule2024.html",
    "href": "schedule2024.html",
    "title": "2024 Training Schedule",
    "section": "",
    "text": "Main materials\n\nPre-workshop\nInformation about pre-workshop preparation – including software installation, expectations for what you should already be familiar with, and review materials – is available in the pre-work portion of the materials page.\n  \n\n\n22 July 2024\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n\nArrival\n\n\n\n\n  \n\n\n23 July 2024 (08:30 - 17:30)\n\n\n\n\n\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n8:30\nIntroduction to time dependent data, course overview\n\n\n\n9:30\nGoal setting, lighting presentations\n\n\n\n10:15\nBreak\n\n\n\n10:45\nLecture: Regression Transformations and Diagnostics\n\n\n\n11:45\nPractical: Regression Transformations and Diagnostics\n\n\n\n12:30\nLunch\n\n\n\n13:30\nLecture: Time Dependent Data Analysis\n\n\n\n14:30\nPractical: Time Dependent Data Analysis\n\n\n\n15:15\nBreak\n\n\n\n15:30\nIntegrated Lecture and Practical: Basics of Time Series method\n\n\n\n\n  \n\n\n24 July 2024 (08:30 - 17:30)\n\n\n\n\n\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n8:30\nOverview of the day\n\n\n\n9:00\nIntroduction to VecDyn Database and API\n\n\n\n10:00\nBreak\n\n\n\n10:30\nWorking with VecDyn/time dependent data (data aggregation, covariate/predictor sources)\n\n\n\n12:30\nLunch\n\n\n\n13:30\nLecture: Introduction to Gaussian Process (GP) regression applied to time dependent data\n\n\n\n14:45\nBreak\n\n\n\n15:15\nPractical: GPs in R with laGP and hetGP\n\n\n\n16:30\nBrainstorming planning for projects\n\n\n\n17:00\nLightning talks\n\n\n\n\n  \n\n\n25 July 2024 (08:30 - 17:30)\n\n\n\n\n\n\n\n\nTime.\nActivity\nMaterials\n\n\n\n\n8:30\nOverview of the day\n\n\n\n9:00\nForecasting metrics and challenge introduction\n\n\n\n9:30\nForecasting Challenge\n\n\n\n10:30\nBreak\n\n\n\n11:00\nForecasting Challenge (cont)\n\n\n\n11:45\nPresent and Eval forecasts\n\n\n\n12:30\nLunch\n\n\n\n13:30\nMini projects\n\n\n\n15:00\nBreak\n\n\n\n15:30\nMini projects\n\n\n\n16:30\nPresent mini projects; wrap up\n\n\n\n\n  \n\n\n26 July 2024\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n\nTravel\n\n\n\n\n\n\nPost-workshop\nEnjoy using these new techniques and databases!"
  },
  {
    "objectID": "GP_Practical.html",
    "href": "GP_Practical.html",
    "title": "VectorByte Methods Training: Introduction to Gaussian Processes for Time Dependent Data (Practical)",
    "section": "",
    "text": "This practical will lead you through fitting a few versions of GPs using two R packages: laGP (Gramacy 2016) and hetGP (Binois and Gramacy 2021). We will begin with a toy example from the lecture and then move on to a real data example to forecast tick abundances for a NEON site."
  },
  {
    "objectID": "GP_Practical.html#overview-of-the-data",
    "href": "GP_Practical.html#overview-of-the-data",
    "title": "VectorByte Methods Training: Introduction to Gaussian Processes for Time Dependent Data (Practical)",
    "section": "Overview of the Data",
    "text": "Overview of the Data\n\nObjective: Forecast tick density for 4 weeks into the future\nSites: The data is collected across 9 different sites, each plot was of size 1600m^2 using a drag cloth\nData: Sparse and irregularly spaced. We only have ~650 observations across 10 years at 9 locations\n\nLet’s start with loading all the libraries that we will need, load our data and understand what we have.\n\nlibrary(tidyverse)\nlibrary(laGP)\nlibrary(ggplot2)\n\n# Pulling the data from the NEON data base. \ntarget &lt;- readr::read_csv(\"https://data.ecoforecast.org/neon4cast-targets/ticks/ticks-targets.csv.gz\", guess_max = 1e1)\n\n# Visualizing the data\nhead(target)\n\n# A tibble: 6 × 5\n  datetime   site_id variable             observation iso_week\n  &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;   \n1 2015-04-20 BLAN    amblyomma_americanum        0    2015-W17\n2 2015-05-11 BLAN    amblyomma_americanum        9.82 2015-W20\n3 2015-06-01 BLAN    amblyomma_americanum       10    2015-W23\n4 2015-06-08 BLAN    amblyomma_americanum       19.4  2015-W24\n5 2015-06-22 BLAN    amblyomma_americanum        3.14 2015-W26\n6 2015-07-13 BLAN    amblyomma_americanum        3.66 2015-W29"
  },
  {
    "objectID": "GP_Practical.html#initial-setup",
    "href": "GP_Practical.html#initial-setup",
    "title": "VectorByte Methods Training: Introduction to Gaussian Processes for Time Dependent Data (Practical)",
    "section": "Initial Setup",
    "text": "Initial Setup\n\nFor a GP model, we assume the response (Y) should be normally distributed.\nSince tick density, our response, must be greater than 0, we need to use a transform.\nThe following is the most suitable transform for our application:\n\n\n\\begin{equation}\n\\begin{aligned}\nf(y) \\ & = \\text{log } \\ (y + 1) \\ \\ ; \\ \\ \\ \\\\[2pt]\n\\end{aligned}\n\\end{equation}\nWe pass in (response + 1) into this function to ensure we don’t take a log of 0. We will adjust this in our back transform.\nLet’s write a function for this, as well as the inverse of the transform.\n\n# transforms y\nf &lt;- function(x) {\n  y &lt;- log(x + 1)\n  return(y)\n}\n\n# This function back transforms the input argument\nfi &lt;- function(y) {\n  x &lt;- exp(y) - 1\n  return(x)\n}"
  },
  {
    "objectID": "GP_Practical.html#predictors",
    "href": "GP_Practical.html#predictors",
    "title": "VectorByte Methods Training: Introduction to Gaussian Processes for Time Dependent Data (Practical)",
    "section": "Predictors",
    "text": "Predictors\n\nThe goal is to forecast tick populations for a season so our response (Y) here, is the tick density. However, we do not have a traditional data set with an obvious input space. What is the X? \n\nWe made a few plots earlier to help us identify what can be useful:\n\nX_1 Iso-week: This is the iso-week number\nLet’s convert the iso-week from our target dataset as a numeric i.e. a number. Here is a function to do the same.\n\n# This function tells us the iso-week number given the date\nfx.iso_week &lt;- function(datetime){\n  # Gives ISO-week in the format yyyy-w## and we extract the ##\n  x1 &lt;- as.numeric(stringr::str_sub(ISOweek::ISOweek(datetime), 7, 8)) # find iso week #\n  return(x1)\n}\n\ntarget$week &lt;- fx.iso_week(target$datetime)\nhead(target)\n\n# A tibble: 6 × 6\n  datetime   site_id variable             observation iso_week  week\n  &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 2015-04-20 BLAN    amblyomma_americanum        0    2015-W17    17\n2 2015-05-11 BLAN    amblyomma_americanum        9.82 2015-W20    20\n3 2015-06-01 BLAN    amblyomma_americanum       10    2015-W23    23\n4 2015-06-08 BLAN    amblyomma_americanum       19.4  2015-W24    24\n5 2015-06-22 BLAN    amblyomma_americanum        3.14 2015-W26    26\n6 2015-07-13 BLAN    amblyomma_americanum        3.66 2015-W29    29\n\n\n\nX_2 Sine wave: We use this to give our model phases. We can consider this as a proxy to some other variables such as temperature which would increase from Jan to about Jun-July and then decrease. We use the following sin wave\n\nX_2 = \\left( \\text{sin} \\ \\left( \\frac{2 \\ \\pi \\ X_1}{106} \\right) \\right)^2 where, X_1 is the iso-week.\nUsually, a Sin wave for a year would have the periodicity of 53 to indicate 53 weeks. Why have we chosen 106 as our period? And we do we square it?\nLet’s use a visual to understand that.\n\nx &lt;- c(1:106)\nsin_53 &lt;- sin(2*pi*x/53)\nsin_106 &lt;- (sin(2*pi*x/106))\nsin_106_2 &lt;- (sin(2*pi*x/106))^2\n\npar(mfrow=c(1, 3), mar = c(4, 5, 4, 1), cex.axis = 2, cex.lab = 2, cex.main = 3, font.lab = 2)\nplot(x, sin_53, col = 2, pch = 19, ylim = c(-1, 1), ylab = \"sin wave\", main = \"period = 53\")\nabline(h = 0, lwd = 2)\nplot(x, sin_106, col = 3, pch = 19, ylim = c(-1, 1), ylab = \"sin wave\", main = \"period = 106\")\nabline(h = 0, lwd = 2)\nplot(x, sin_106_2, col = 4, pch = 19, ylim = c(-1, 1), ylab = \"sin wave\", main = \"period = 106 squared\")\nabline(h = 0, lwd = 2)\n\n\n\n\n\n\n\n\nSome observations:\n\nThe sin wave (period 53) goes increases from (0, 1) and decreases all the way to -1 before coming back to 0, all within the 53 weeks in the year. But this is not what we want to achieve.\nWe want the function to increase from Jan - Jun and then start decreasing till Dec. This means, we need a regular sin-wave to span 2 years so we can see this.\nWe also want the next year to repeat the same pattern i.e. we want to restrict it to [0, 1] interval. Thus, we square the sin wave.\n\n\nfx.sin &lt;- function(datetime, f1 = fx.iso_week){\n  # identify iso week#\n  x &lt;- f1(datetime) \n  # calculate sin value for that week\n  x2 &lt;- (sin(2*pi*x/106))^2 \n  return(x2)\n}\n\nFor a GP, it’s also useful to ensure that all our X’s are between 0 and 1. Usually this is done by using the following method\nX_i^* = \\frac{X_i - \\min(X)}{\\max(X) - \\min(X) } where X = (X_1, X_2 ...X_n)\nX^* = (X_1^*, X_2^* ... X_n^*) will be the standarized X’s with all X_i^* in the interval [0, 1].\nWe can either write a function for this, or in our case, we can just divide Iso-week by 53 since that would result effectively be the same. Our Sin Predictor already lies in the interval [0, 1]."
  },
  {
    "objectID": "GP_Practical.html#model-fitting",
    "href": "GP_Practical.html#model-fitting",
    "title": "VectorByte Methods Training: Introduction to Gaussian Processes for Time Dependent Data (Practical)",
    "section": "Model Fitting",
    "text": "Model Fitting\nNow, let’s start with modelling. We will start with one random location out of the 9 locations.\n\n# Choose a random site number: Anything between 1-9.\nsite_number &lt;- 6\n\n# Obtaining site name\nsite_names &lt;- unique(target$site_id)\n\n# Subsetting all the data at that location\ndf &lt;- subset(target, target$site_id == site_names[site_number])\nhead(df)\n\n# A tibble: 6 × 6\n  datetime   site_id variable             observation iso_week  week\n  &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 2014-06-09 SCBI    amblyomma_americanum       75.9  2014-W24    24\n2 2014-06-30 SCBI    amblyomma_americanum       28.3  2014-W27    27\n3 2014-07-21 SCBI    amblyomma_americanum        0    2014-W30    30\n4 2014-07-28 SCBI    amblyomma_americanum       10.1  2014-W31    31\n5 2014-08-11 SCBI    amblyomma_americanum        4.94 2014-W33    33\n6 2014-10-20 SCBI    amblyomma_americanum        0    2014-W43    43\n\n\nWe will also select only those columns that we are interested in i.e. datetime and obervation. We don’t need site since we are only using one of them.\n\n# extracting only the datetime and obs columns\ndf &lt;- df[, c(\"datetime\", \"observation\")]\n\nWe will use one site at first and fit a GP and make predictions. For this we first need to divide our data into a training set and a testing set. Since we have time series, we want to divide the data sequentially, i.e. we pick a date and everything before the date is our training set and after is our testing set where we check how well our model performs. We choose the date 2020-12-31.\n\n# Selecting a date before which we consider everything as training data and after this is testing data.\ncutoff = as.Date('2020-12-31')\ndf_train &lt;- subset(df, df$datetime &lt;= cutoff)\ndf_test &lt;- subset(df, df$datetime &gt; cutoff)"
  },
  {
    "objectID": "GP_Practical.html#gp-model",
    "href": "GP_Practical.html#gp-model",
    "title": "VectorByte Methods Training: Introduction to Gaussian Processes for Time Dependent Data (Practical)",
    "section": "GP Model",
    "text": "GP Model\nNow we will setup our X’s. We already have the functions to do this and can simply pass in the datetime. We then combine X_1 and X_2 to create out input matrix X. Remember, everything is ordered as in our dataset.\n\n# Setting up iso-week and sin wave predictors by calling the functions\nX1 &lt;- fx.iso_week(df_train$datetime) # range is 1-53\nX2 &lt;- fx.sin(df_train$datetime) # range is 0 to 1\n\n# Centering the iso-week by diving by 53\nX1c &lt;- X1/ 53\n\n# We combine columns centered X1 and X2, into a matrix as our input space\nX &lt;- as.matrix(cbind.data.frame(X1c, X2))\nhead(X)\n\n           X1c        X2\n[1,] 0.4528302 0.9782005\n[2,] 0.5094340 0.9991219\n[3,] 0.5660377 0.9575728\n[4,] 0.5849057 0.9305218\n[5,] 0.6226415 0.8587536\n[6,] 0.8113208 0.3120862\n\n\nNext step is to tranform the response to ensure it is normal.\n\n# Extract y: observation from our training model. \ny_obs &lt;- df_train$observation\n\n# Transform the response\ny &lt;- f(y_obs)\n\nNow, we can use the laGP library to fit a GP. First, we specify priors using darg and garg. We will specify a minimum and maximum for our arguments. We need to pass the input space for darg and the output vector for garg. You can look into the functions using ?function in R. We set the minimum to a very small value rather than 0 to ensure numeric stability.\n\n# A very small value for stability\neps &lt;- sqrt(.Machine$double.eps) \n  \n# Priors for theta and g. \nd &lt;- darg(list(mle=TRUE, min =eps, max=5), X)\ng &lt;- garg(list(mle=TRUE, min = eps, max = 1), y)\n\nNow, to fit the GP, we use newGPsep. We pass the input matrix and the response vector with some values of the parameters. Then, we use the jmleGPsep function to jointly estimate \\theta and g using MLE method. dK allows the GP object to store derivative information which is needed for MLE calculations. newGPsep will fit a separable GP as opposed to newGP which would fit an isotropic GP.\n\n# Fitting a GP with our data, and some starting values for theta and g\ngpi &lt;- newGPsep(X, y, d = 0.1, g = 1, dK = T)\n\n# Jointly infer MLE for all parameters\nmle &lt;- jmleGPsep(gpi, drange = c(d$min, d$max), grange = c(g$min, g$max), \n                 dab = d$ab, gab=  g$ab)\n\nNow, we will create a grid from the first week in our dataset to 1 year into the future, and predict on the entire time series. We use predGPsep to make predictions.\n\n# Create a grid from start date in our data set to one year in future (so we forecast for next season)\nstartdate &lt;- as.Date(min(df$datetime))# identify start week\ngrid_datetime &lt;- seq.Date(startdate, Sys.Date() + 365, by = 7) # create sequence from \n\n# Build the input space for the predictive space (All weeks from 04-2014 to 07-2025)\nXXt1 &lt;- fx.iso_week(grid_datetime)\nXXt2 &lt;- fx.sin(grid_datetime)\n\n# Standardize\nXXt1c &lt;- XXt1/53\n\n# Store inputs as a matrix\nXXt &lt;- as.matrix(cbind.data.frame(XXt1c, XXt2))\n\n# Make predictions using predGP with the gp object and the predictive set\nppt &lt;- predGPsep(gpi, XXt) \n\nStoring the mean and calculating quantiles.\n\n# Now we store the mean as our predicted response i.e. density along with quantiles\nyyt &lt;- ppt$mean\nq1t &lt;- ppt$mean + qnorm(0.025,0,sqrt(diag(ppt$Sigma))) #lower bound\nq2t &lt;- ppt$mean + qnorm(0.975,0,sqrt(diag(ppt$Sigma))) # upper bound\n\nNow we can plot our data and predictions and see how well our model performed. We need to back transform our predictions to the original scale.\n\n# Back transform our data to original\ngp_yy &lt;- fi(yyt)\ngp_q1 &lt;- fi(q1t)\ngp_q2 &lt;- fi(q2t)\n\n# Plot the observed points\nplot(as.Date(df$datetime), df$observation,\n       main = paste(site_names[site_number]), col = \"black\",\n       xlab = \"Dates\" , ylab = \"Abundance\",\n       # xlim = c(as.Date(min(df$datetime)), as.Date(cutoff)),\n       ylim = c(min(df_train$observation, gp_yy, gp_q1), max(df_train$observation, gp_yy, gp_q2)* 1.05))\n\n# Plot the testing set data \npoints(as.Date(df_test$datetime), df_test$observation, col =\"black\", pch = 19)\n\n# Line to indicate seperation between train and test data\nabline(v = as.Date(cutoff), lwd = 2)\n\n# Add the predicted response and the quantiles\nlines(grid_datetime, gp_yy, col = 4, lwd = 2)\nlines(grid_datetime, gp_q1, col = 4, lwd = 1.2, lty = 2)\nlines(grid_datetime, gp_q2, col = 4, lwd = 1.2, lty =2)\n\n\n\n\n\n\n\n\nThat looks pretty good? We can also look at the RMSE to see how the model performs. It is better o do this on the transformed scale. We will use yyt for this. We need to find those predictions which correspond to the datetime in our testing dataset df_test.\n\n# Obtain true observed values for testing set\nyt_true &lt;- f(df_test$observation)\n\n# FInd corresponding predictions from our model in the grid we predicted on\nyt_pred &lt;- yyt[which(grid_datetime  %in% df_test$datetime)]\n\n# calculate RMSE\nrmse &lt;- sqrt(mean((yt_true - yt_pred)^2))\nrmse\n\n[1] 0.8624903"
  },
  {
    "objectID": "GP_Practical.html#hetgp-model",
    "href": "GP_Practical.html#hetgp-model",
    "title": "VectorByte Methods Training: Introduction to Gaussian Processes for Time Dependent Data (Practical)",
    "section": "HetGP Model",
    "text": "HetGP Model\nNext, we can attempt a hetGP. We are now interested in fitting a vector of nuggets rather than a single value.\nLet’s use the same data we have to fit a hetGP. We already have our data (X, y) as well as our prediction set XXt. We use the mleHetGP command to fit a GP and pass in our data. The default covariance structure is the Squared Exponential structure. We use the predict function in base R and pass the hetGP object i.e. het_gpi to make predictions on our set XXt.\n\n# create predictors\nX1 &lt;- fx.iso_week(df_train$datetime)\nX2 &lt;- fx.sin(df_train$datetime)\n\n# standardize and put into matrix\nX1c &lt;- X1/53\nX &lt;- as.matrix(cbind.data.frame(X1c, X2))\n\n# Build prediction grid (From 04-2014 to 07-2025)\nXXt1 &lt;- fx.iso_week(grid_datetime)\nXXt2 &lt;- fx.sin(grid_datetime)\n\n# standardize and put into matrix\nXXt1c &lt;- XXt1/53\nXXt &lt;- as.matrix(cbind.data.frame(XXt1c, XXt2))\n\n# Transform the training response\ny_obs &lt;- df_train$observation\ny &lt;- f(y_obs)\n\n# Fit a hetGP model. X must be s matrix and nrow(X) should be same as length(y)\nhet_gpi &lt;- hetGP::mleHetGP(X = X, Z = y)\n\n# Predictions using the base R predict command with a hetGP object and new locationss\nhet_ppt &lt;- predict(het_gpi, XXt)\n\nNow we obtain the mean and the confidence bounds as well as transform the data to the original scale.\n\n# Mean density for predictive locations and Confidence bounds\nhet_yyt &lt;- het_ppt$mean\nhet_q1t &lt;- qnorm(0.975, het_ppt$mean, sqrt(het_ppt$sd2 + het_ppt$nugs))\nhet_q2t &lt;- qnorm(0.025, het_ppt$mean, sqrt(het_ppt$sd2 + het_ppt$nugs)) \n\n# Back transforming to original scale\nhet_yy &lt;- fi(het_yyt)\nhet_q1 &lt;- fi(het_q1t)\nhet_q2 &lt;- fi(het_q2t)\n\nWe can now plot the results similar to before. [Uncomment the code lines to see how a GP vs a HetGP fits the data]\n\n# Plot Original data\nplot(as.Date(df$datetime), df$observation,\n       main = paste(site_names[site_number]), col = \"black\",\n       xlab = \"Dates\" , ylab = \"Abundance\",\n       # xlim = c(as.Date(min(df$datetime)), as.Date(cutoff)),\n       ylim = c(min(df_train$observation, het_yy, het_q2), max(df_train$observation, het_yy, het_q1)* 1.2))\n\n# Add testing observations\npoints(as.Date(df_test$datetime), df_test$observation, col =\"black\", pch = 19)\n\n# Line to indicate our cutoff point\nabline(v = as.Date(cutoff), lwd = 2)\n\n# HetGP Model mean predictions and bounds.\nlines(grid_datetime, het_yy, col = 2, lwd = 2)\nlines(grid_datetime, het_q1, col = 2, lwd = 1.2, lty = 2)\nlines(grid_datetime, het_q2, col = 2, lwd = 1.2, lty =2)\n\n## GP model fits for the same data\n# lines(grid_datetime, gp_yy, col = 3, lwd = 2)\n# lines(grid_datetime, gp_q1, col = 3, lwd = 1.2, lty = 2)\n# lines(grid_datetime, gp_q2, col = 3, lwd = 1.2, lty =2)\n\nlegend(\"topleft\", legend = c(\"Train Y\",\"Test Y\", \"GP preds\", \"HetGP preds\"),\n         col = c(1, 1, 2, 3), lty = c(NA, NA, 1, 1),\n         pch = c(1, 19, NA, NA), cex = 0.5)\n\n\n\n\n\n\n\n\nThe mean predictions of a GP are similar to that of a hetGP; But the confidence bounds are different. A hetGP produces sligtly tighter bounds.\nWe can also compare the RMSE’s using the predictions of the hetGP model.\n\nyt_true &lt;- f(df_test$observation) # Original data\nhet_yt_pred &lt;- het_yyt[which(grid_datetime  %in% df_test$datetime)] # model preds\n\n# calculate rmse for hetGP model\nrmse_het &lt;- sqrt(mean((yt_true - het_yt_pred)^2))\nrmse_het\n\n[1] 0.8835813\n\n\nNow that we have learnt how to fit a GP and a hetGP, it’s time for a challenge.\nTry a hetGP on our sin example from before (but this time I have added noise).\n\n# Your turn\nset.seed(26)\nn &lt;- 8 # number of points\nX &lt;- matrix(seq(0, 2*pi, length= n), ncol=1) # build inputs \ny &lt;- 5*sin(X) + rnorm(n, 0 , 2) # response with some noise\n\n# Predict on this set\nXX &lt;- matrix(seq(-0.5, 2*pi + 0.5, length= 100), ncol=1)\n\n# Data visualization\nplot(X, y)\n\n\n\n\n\n\n\n# Add code to fit a hetGP model and visualise it as above"
  },
  {
    "objectID": "GP.html#gaussian-process-introduction",
    "href": "GP.html#gaussian-process-introduction",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Gaussian Process: Introduction",
    "text": "Gaussian Process: Introduction\n\n\nA Gaussian Process model is a non paramteric and flexible regression model.\nIt started being used in the field of spatial statistics, where it is called kriging.\nIt is also widely used in the field of machine learning since it makes fast predictions and gives good uncertainty quantification commonly used as a surrogate model. (Gramacy 2020)"
  },
  {
    "objectID": "GP.html#uses-and-benefits",
    "href": "GP.html#uses-and-benefits",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Uses and Benefits",
    "text": "Uses and Benefits\n\n\nSurrogate Models: Imagine a case where field experiments are infeasible and computer experiments take a long time to run, we can approximate the computer experiments using a surrogate model.\nIt can be used to calibrate computer models w.r.t the field observations or computer simulations.\nThe ability to this model to provide good UQ makes it very useful in other fields such as ecology where the data is sparse and noisy and therefore good uncertainty measures are paramount."
  },
  {
    "objectID": "GP.html#what-is-a-gp",
    "href": "GP.html#what-is-a-gp",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "What is a GP?",
    "text": "What is a GP?\n\n\nHere, we assume that the data come from a Multivariate Normal Distribution (MVN).\nWe then make predictions conditional on the data.\n\n\n\n\nWe are essentially taking a “fancy” average of the data to make predictions\nTo understand how it works, let’s first visualize this concept and then look into the math"
  },
  {
    "objectID": "GP.html#visualizing-a-gp",
    "href": "GP.html#visualizing-a-gp",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Visualizing a GP",
    "text": "Visualizing a GP"
  },
  {
    "objectID": "GP.html#how-does-a-gp-work",
    "href": "GP.html#how-does-a-gp-work",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "How does a GP work",
    "text": "How does a GP work\n\n\nOur “fancy” average was just using data around the new location.\nWe are using points closer to each other to correlate the responses.\nNow we know, we are averaging the data “nearby”… How do you define that?\n\n\n\n\nThis indicates that we are using distance in some way. Where…? We need some math"
  },
  {
    "objectID": "GP.html#mathematically",
    "href": "GP.html#mathematically",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Mathematically",
    "text": "Mathematically\n\n\nAny normal distribution can be described by a mean vector \\mu and a covariance matrix \\Sigma.\nMathematically, we can write it as,\n\nY_{\\ n \\times 1} \\sim N \\ ( \\ \\mu(X)_{\\ n \\times 1} \\ , \\ \\Sigma(X)_{ \\ n \\times n} \\ ) Here, Y is the response of interest and n is the number of observations.\n\nOur goal is to find Y_p \\ \\vert \\ Y, X."
  },
  {
    "objectID": "GP.html#distance",
    "href": "GP.html#distance",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Distance",
    "text": "Distance\n\n\nRecall that in Linear Regression, you have \\ \\Sigma \\  = \\sigma^2 \\mathbb{I}\n\n\n\n\nFor a GP, the covariance matrix ( \\Sigma ) is defined by a kernel.\nConsider,\n\n\\Sigma_n = \\tau^2 C_n where C_n = \\exp \\left( - \\vert \\vert x - x' \\vert \\vert^2 \\right), and x and x' are input locations."
  },
  {
    "objectID": "GP.html#interpretation-of-the-kernel",
    "href": "GP.html#interpretation-of-the-kernel",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Interpretation of the kernel",
    "text": "Interpretation of the kernel\n\n\nC_n = \\exp \\left( - \\vert \\vert x - x' \\vert \\vert^2 \\right)\nThe covariance structure now depends on how close together the inputs. If inputs are close in distance, then the responses are more highly correlated.\nThe covariance will decay at an exponential rate as x moves away from x'."
  },
  {
    "objectID": "GP.html#summarizing-our-data",
    "href": "GP.html#summarizing-our-data",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Summarizing our data",
    "text": "Summarizing our data\n\n\nNow we will learn how to use a GP to make predictions at new locations.\nAs we learnt, we condition on the data. We can think of this as the prior.\n\n\\begin{equation}\nY_n \\ \\vert X_n \\sim \\mathcal{N} \\ ( \\ 0 \\ , \\ \\tau^2 \\ C_n(X_n)  \\ ) \\\\\n\\end{equation}\n\n\nNow, consider, (\\mathcal{X}, \\mathcal{Y}) as the predictive set."
  },
  {
    "objectID": "GP.html#how-to-make-predictions",
    "href": "GP.html#how-to-make-predictions",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "How to make predictions",
    "text": "How to make predictions\n\n\nThe goal is to find the distribution of \\mathcal{Y} \\ \\vert X_n, Y_n which in this case is the posterior distribution.\nBy properties of Normal distribution, the posterior is also normally distributed.\n\n\n\nWe also need to write down the mean and variance of the posterior distribution so it’s ready for use."
  },
  {
    "objectID": "GP.html#how-to-make-predictions-1",
    "href": "GP.html#how-to-make-predictions-1",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "How to make predictions",
    "text": "How to make predictions\n\n\nFirst we will “stack” the predictions and the data.\n\n\\begin{equation*}\n\\begin{bmatrix}\n\\mathcal{Y} \\\\\nY_n \\\\\n\\end{bmatrix}\n\\ \\sim \\ \\mathcal{N}\n\\left(\n\\;\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n\\end{bmatrix}\\; , \\;\n\\begin{bmatrix}\n\\Sigma(\\mathcal{X}, \\mathcal{X}) & \\Sigma(\\mathcal{X}, X_n)\\\\\n\\Sigma({X_n, \\mathcal{X}}) &  \\Sigma_n\\\\\n\\end{bmatrix}\n\\;\n\\right)\n\\\\[5pt]\n\\end{equation*}\n\n\n\nNow, let’s denote the predictive mean with \\mu(\\mathcal{X}) and predictive variance with \\sigma^2(\\mathcal{X})\n\n\\begin{equation}\n\\begin{aligned}\n\\mathcal{Y} \\mid Y_n, X_n \\sim N\\left(\\mu(\\mathcal{X}) \\ , \\ \\sigma^2(\\mathcal{X})\\right)\n\\end{aligned}\n\\end{equation}"
  },
  {
    "objectID": "GP.html#distribution-of-interest",
    "href": "GP.html#distribution-of-interest",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Distribution of Interest!",
    "text": "Distribution of Interest!\n\n\nWe will apply the properties of conditional Normal distributions.\n\n\\begin{equation*}\n\\begin{aligned}\n\\mu(\\mathcal{X}) & = \\Sigma(\\mathcal{X}, X_n) \\Sigma_n^{-1} Y_n \\\\[10pt]  \n\\sigma^2(\\mathcal{X}) & = \\Sigma(\\mathcal{X}, \\mathcal{X}) - \\Sigma(\\mathcal{X}, X_n) \\Sigma_n^{-1} \\Sigma(X_n, \\mathcal{X}) \\\\\n\\end{aligned}\n\\end{equation*}\n\n\n\nEverything we do, relies on these equations. Now, let’s learn more about \\Sigma"
  },
  {
    "objectID": "GP.html#sigma-matrix",
    "href": "GP.html#sigma-matrix",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Sigma Matrix",
    "text": "Sigma Matrix\n\n\n\\Sigma_n = \\tau^2 C_n where C_n is our kernel.\n\n\n\n\nOne of the most common kernels which we will focus on is the squared exponential distance kernel written as\n\nC_n = \\exp{ \\left( -\\frac{\\vert\\vert x - x' \\vert \\vert ^2}{\\theta} \\right ) + g \\mathbb{I_n}} \n\n\n\nWhat’s \\tau^2, g and \\theta though? No more math. We will just conceptually go through these"
  },
  {
    "objectID": "GP.html#hyper-parameters",
    "href": "GP.html#hyper-parameters",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Hyper Parameters",
    "text": "Hyper Parameters\n\nA GP is non parameteric, however, has some hyper-parameters. In this case,\n\n\\tau^2 (Scale): This parameter can be used to adjust the amplitude of the data.\n\\theta (Length-scale): This parameter controls the rate of decay of correlation.\ng (Nugget): This parameter controls the noise in the covariance structure (adds discontinuity)"
  },
  {
    "objectID": "GP.html#scale-amplitude",
    "href": "GP.html#scale-amplitude",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Scale (Amplitude)",
    "text": "Scale (Amplitude)\n\n\nA random draw from a multivariate normal distribution with \\tau^2 = 1 will produce data between -2 and 2.\nNow let’s visualize what happens when we increase \\tau^2 to 25."
  },
  {
    "objectID": "GP.html#length-scale-rate-of-decay-of-correlation",
    "href": "GP.html#length-scale-rate-of-decay-of-correlation",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Length-scale (Rate of decay of correlation)",
    "text": "Length-scale (Rate of decay of correlation)\n\n\nDetermines how “wiggly” a function is\nSmaller \\theta means wigglier functions i.e. visually:"
  },
  {
    "objectID": "GP.html#nugget-noise",
    "href": "GP.html#nugget-noise",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Nugget (Noise)",
    "text": "Nugget (Noise)\n\n\nEnsures discontinuity and prevents interpolation which in turn yields better UQ.\nWe will compare a sample from g ~ 0 (&lt; 1e-8 for numeric stability) vs g = 0.1 to observe what actually happens."
  },
  {
    "objectID": "GP.html#toy-example-1d-example",
    "href": "GP.html#toy-example-1d-example",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Toy Example (1D Example)",
    "text": "Toy Example (1D Example)\n\n\n\nX &lt;- matrix(seq(0, 2*pi, length = 100), ncol =1)\nn &lt;- nrow(X) \ntrue_y &lt;- 5 * sin(X)\nobs_y &lt;- true_y + rnorm(n, sd=1)"
  },
  {
    "objectID": "GP.html#toy-example-1d-example-1",
    "href": "GP.html#toy-example-1d-example-1",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Toy Example (1D Example)",
    "text": "Toy Example (1D Example)\n\n\neps &lt;- sqrt(.Machine$double.eps)\ngpi &lt;- laGP::newGP(X = X,Z = obs_y, d = 0.1, g = 0.1 * var(obs_y), dK = TRUE) \nmle &lt;- laGP::mleGP(gpi = gpi, param = c(\"d\", \"g\"), \n                   tmin= c(eps, eps), tmax= c(10, var(obs_y))) \nXX &lt;- matrix(seq(0, 2*pi, length = 1000), ncol =1)\np &lt;- laGP::predGP(gpi = gpi, XX = XX)"
  },
  {
    "objectID": "GP.html#extentions",
    "href": "GP.html#extentions",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Extentions",
    "text": "Extentions\n\n\nAnisotropic Gaussian Processes: Suppose our data is multi-dimensional, we can control the length-scale (\\theta) for each dimension.\nHeteroskedastic Gaussian Processes: Suppose our data is noisy and the noise is input dependent, then we can use a different nugget for each unique input rather than a scalar g."
  },
  {
    "objectID": "GP.html#anisotropic-gaussian-processes",
    "href": "GP.html#anisotropic-gaussian-processes",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Anisotropic Gaussian Processes",
    "text": "Anisotropic Gaussian Processes\n\nIn this situation, we can rewrite the C_n matrix as,\nC_\\theta(x , x') = \\exp{ \\left( -\\sum_{k=1}^{m} \\frac{ (x_k - x_k')^2 }{\\theta_k} \\right ) + g \\mathbb{I_n}}\nHere, \\theta = (\\theta_1, \\theta_2, …, \\theta_m) is a vector of length-scales, where m is the dimension of the input space."
  },
  {
    "objectID": "GP.html#heteroskedastic-gaussian-processes",
    "href": "GP.html#heteroskedastic-gaussian-processes",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Heteroskedastic Gaussian Processes",
    "text": "Heteroskedastic Gaussian Processes\n\n\nHeteroskedasticity implies that the data is noisy, and the noise is input dependent and irregular. (Binois, Gramacy, and Ludkovski 2018)"
  },
  {
    "objectID": "GP.html#hetgp-setup",
    "href": "GP.html#hetgp-setup",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "HetGP Setup",
    "text": "HetGP Setup\n\n\nLet Y_n be the response vector of size n. \nLet X = (X_1, X_2 ... X_n) be the input space.\n\nThen, a regular GP is written as:\n\n\\begin{align*}\nY_N \\ & \\ \\sim GP \\left( 0 \\ , \\tau^2 C_n  \\right); \\ \\text{where, }\\\\[2pt]\nC_n  & \\ = \\exp{ \\left( -\\frac{\\vert\\vert x - x' \\vert \\vert ^2}{\\theta} \\right ) + g \\mathbb{I_n}}\n\\end{align*}"
  },
  {
    "objectID": "GP.html#hetgp-setup-1",
    "href": "GP.html#hetgp-setup-1",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "HetGP Setup",
    "text": "HetGP Setup\n\nIn case of a hetGP, we have:\n\n\\begin{aligned}\nY_n\\ & \\ \\sim GP \\left( 0 \\ , \\tau^2 C_{n, \\Lambda}  \\right) \\ \\ \\text{where, }\\\\[2pt]\nC_{n, \\Lambda}  & \\ = \\exp{ \\left( -\\frac{\\vert\\vert x - x' \\vert \\vert ^2}{\\theta} \\right ) + \\Lambda_n} \\ \\ \\ \\text{and, }\\ \\ \\\\[2pt]\n\\ \\ \\Lambda_n \\ & \\ = \\ \\text{Diag}(\\lambda_1, \\lambda_2 ... , \\lambda_n) \\\\[2pt]\n\\end{aligned}\n\n\nInstead of one nugget for the GP, we have a vector of nuggets i.e. a unique nugget for each unique input."
  },
  {
    "objectID": "GP.html#hetgp-predictions",
    "href": "GP.html#hetgp-predictions",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "HetGP Predictions",
    "text": "HetGP Predictions\n\n\nRecall, for a GP, we make predictions using the following:\n\n\\begin{equation*}\n\\begin{aligned}\n\\mu(\\mathcal{X}) & = \\Sigma(\\mathcal{X}, X_n) \\Sigma_n^{-1} Y_n \\\\\n\\sigma^2(\\mathcal{X}) & = \\Sigma(\\mathcal{X}, \\mathcal{X}) - \\Sigma(\\mathcal{X}, X_n) \\Sigma_n^{-1} \\Sigma(X_n, \\mathcal{X}) \\\\\n\\end{aligned}\n\\end{equation*}\n\nWe can make predictions using these same equations with \\Sigma(X_n) \\ \\ = \\tau^2 C_{n, \\Lambda}"
  },
  {
    "objectID": "GP.html#toy-example-1d-example-2",
    "href": "GP.html#toy-example-1d-example-2",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Toy Example (1D Example)",
    "text": "Toy Example (1D Example)"
  },
  {
    "objectID": "GP.html#toy-example-1d-example-3",
    "href": "GP.html#toy-example-1d-example-3",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Toy Example (1D Example)",
    "text": "Toy Example (1D Example)"
  },
  {
    "objectID": "GP.html#toy-example-1d-example-4",
    "href": "GP.html#toy-example-1d-example-4",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Toy Example (1D Example)",
    "text": "Toy Example (1D Example)"
  },
  {
    "objectID": "GP.html#intro-to-ticks-problem",
    "href": "GP.html#intro-to-ticks-problem",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Intro to Ticks Problem",
    "text": "Intro to Ticks Problem\n\n\nEFI-RCN held an ecological forecasting challenge NEON Forecasting Challenge (Thomas et al. 2022)\nWe focus on the Tick Populations theme which studies the abundance of the lone star tick (Amblyomma americanum)"
  },
  {
    "objectID": "GP.html#tick-population-forecasting",
    "href": "GP.html#tick-population-forecasting",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Tick Population Forecasting",
    "text": "Tick Population Forecasting\n\nSome details about the challenge:\n\nObjective: Forecast tick density for 4 weeks into the future\nSites: The data is collected across 9 different sites, each plot was of size 1600m^2 using a drag cloth\nData: Sparse and irregularly spaced. We only have ~650 observations across 10 years at 9 locations"
  },
  {
    "objectID": "GP.html#predictors",
    "href": "GP.html#predictors",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Predictors",
    "text": "Predictors\n\n\nX_1 Iso-week: The week in which the tick density was recorded.\nX_2 Sine wave: \\left( \\text{sin} \\ ( \\frac{2 \\ \\pi \\ X_1}{106} ) \\right)^2.\nX_3 Greenness: Environmental predictor (in practical)"
  },
  {
    "objectID": "GP.html#practical",
    "href": "GP.html#practical",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "Practical",
    "text": "Practical\n\n\nSetup these predictors\nTransform the data to normal\nFit a GP to the Data\nMake Predictions on a testing set\nCheck how predictions perform."
  },
  {
    "objectID": "GP.html#references",
    "href": "GP.html#references",
    "title": "Introduction to Gaussian Processes for Time Dependent Data",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBinois, Mickael, Robert B Gramacy, and Mike Ludkovski. 2018. “Practical Heteroscedastic Gaussian Process Modeling for Large Simulation Experiments.” Journal of Computational and Graphical Statistics 27 (4): 808–21.\n\n\nGramacy, Robert B. 2020. Surrogates: Gaussian Process Modeling, Design, and Optimization for the Applied Sciences. Chapman; Hall/CRC.\n\n\nThomas, R Quinn, Carl Boettiger, Cayelan C Carey, Michael C Dietze, Leah R Johnson, Melissa A Kenney, Jason S Mclachlan, et al. 2022. “The NEON Ecological Forecasting Challenge.” Authorea Preprints."
  },
  {
    "objectID": "VB_RegDiagTrans_practical_soln.html#fit-the-linear-regression-model.-plot-the-data-and-fitted-line.",
    "href": "VB_RegDiagTrans_practical_soln.html#fit-the-linear-regression-model.-plot-the-data-and-fitted-line.",
    "title": "VectorByte Methods Training",
    "section": "1. Fit the linear regression model. Plot the data and fitted line.",
    "text": "1. Fit the linear regression model. Plot the data and fitted line.\n\n## fit models\nattach(D &lt;- read.csv(\"data/transforms.csv\"))\nlm1 &lt;- lm(Y1 ~ X1)\nlm2 &lt;- lm(Y2 ~ X2)\nlm3 &lt;- lm(Y3 ~ X3)\nlm4 &lt;- lm(Y4 ~ X4)\n\n## plot points and lines\npar(mfrow=c(2,2), mar=c(3,2,2,1))\nplot(X1, Y1, col=1, main=\"I\"); abline(lm1, col=1)\nplot(X2, Y2, col=2, main=\"II\"); abline(lm2, col=2)\nplot(X3, Y3, col=3, main=\"III\"); abline(lm3, col=3)\nplot(X4, Y4, col=4, main=\"IV\"); abline(lm4, col=4)"
  },
  {
    "objectID": "VB_RegDiagTrans_practical_soln.html#provide-a-scatterplot-normal-q-q-plot-and-histogram-for-the-studentized-regression-residuals.",
    "href": "VB_RegDiagTrans_practical_soln.html#provide-a-scatterplot-normal-q-q-plot-and-histogram-for-the-studentized-regression-residuals.",
    "title": "VectorByte Methods Training",
    "section": "2. Provide a scatterplot, normal Q-Q plot, and histogram for the studentized regression residuals.",
    "text": "2. Provide a scatterplot, normal Q-Q plot, and histogram for the studentized regression residuals.\n\npar(mfrow=c(3,4), mar=c(4,4,2,0.5))   # you might have to make \n                                      # the plot window big to \n                                      # fit everything\nplot(lm1$fitted, rstudent(lm1), col=1,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"I\")\nplot(lm2$fitted, rstudent(lm2), col=2,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"II\")\nplot(lm3$fitted, rstudent(lm3), col=3,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"III\")\nplot(lm4$fitted, rstudent(lm4), col=4,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"IV\")\n\nqqnorm(rstudent(lm1), pch=20, col=1, main=\"\" )\nabline(a=0,b=1,lty=2)\nqqnorm(rstudent(lm2), pch=20, col=2, main=\"\" )\nabline(a=0,b=1,lty=2)\nqqnorm(rstudent(lm3), pch=20, col=3, main=\"\" )\nabline(a=0,b=1,lty=2)\nqqnorm(rstudent(lm4), pch=20, col=4, main=\"\" )\nabline(a=0,b=1,lty=2)\n\nhist(rstudent(lm1), col=1, xlab=\"Studentized Residuals\", \n     main=\"\", border=8)\nhist(rstudent(lm2), col=2, xlab=\"Studentized Residuals\", main=\"\")\nhist(rstudent(lm3), col=3, xlab=\"Studentized Residuals\", main=\"\")\nhist(rstudent(lm4), col=4, xlab=\"Studentized Residuals\", main=\"\")"
  },
  {
    "objectID": "VB_RegDiagTrans_practical_soln.html#using-the-residual-scatterplots-state-how-the-slr-model-assumptions-are-violated.",
    "href": "VB_RegDiagTrans_practical_soln.html#using-the-residual-scatterplots-state-how-the-slr-model-assumptions-are-violated.",
    "title": "VectorByte Methods Training",
    "section": "3. Using the residual scatterplots, state how the SLR model assumptions are violated.",
    "text": "3. Using the residual scatterplots, state how the SLR model assumptions are violated.\nSet 1: Xs are clumpy AND the variance seems non-constant. It looks a lot like the GDP data from class. Since both Xs and Ys are strictly positive, we can try a log-log transform.\nSet 2: Data have non-constant variance – should probably log transform the Ys\nSet 3: Data have an underlying non-linear pattern. Add in an x^2 and x^3 term in this case.\nSet 4: X values are very clumpy and all positive. Try log transform of the Xs"
  },
  {
    "objectID": "VB_RegDiagTrans_practical_soln.html#determine-the-data-transformation-to-correct-the-problems-in-3-fit-the-corresponding-regression-model-and-plot-the-transformed-data-with-new-fitted-line.",
    "href": "VB_RegDiagTrans_practical_soln.html#determine-the-data-transformation-to-correct-the-problems-in-3-fit-the-corresponding-regression-model-and-plot-the-transformed-data-with-new-fitted-line.",
    "title": "VectorByte Methods Training",
    "section": "4. Determine the data transformation to correct the problems in 3, fit the corresponding regression model, and plot the transformed data with new fitted line.",
    "text": "4. Determine the data transformation to correct the problems in 3, fit the corresponding regression model, and plot the transformed data with new fitted line.\n\n### the fixes are as follows:\nlogX1&lt;- log(X1)\nlogY1 &lt;- log(Y1)\nlogY2 &lt;- log(Y2)\nX3sq &lt;- X3^2\nX3cube&lt;-X3^3\nlogX4 &lt;- log(X4)\n\n\n### re-run the regressions and residual plots to show this worked\nlm1 &lt;- lm(logY1 ~ logX1)\nlm2 &lt;- lm(logY2 ~ X2)\nlm3 &lt;- lm(Y3 ~ X3+ X3sq + X3cube)\nlm4 &lt;- lm(Y4 ~ logX4)\n\n## plot points and lines\npar(mfrow=c(2,2), mar=c(3,2,2,1))\nplot(logX1, logY1, col=1, main=\"I\"); abline(lm1, col=1)\nplot(X2, logY2, col=2, main=\"II\"); abline(lm2, col=2)\nplot(X3, Y3, col=3, main=\"III\")\nxx3 &lt;- seq(min(X3), max(X3), length=1000)\nlines(xx3, lm3$coef[1] + lm3$coef[2]*xx3 + \n        lm3$coef[3]*xx3^2+lm3$coef[3]*xx3^3, col=3)\nplot(logX4, Y4, col=4, main=\"IV\"); abline(lm4, col=4)"
  },
  {
    "objectID": "VB_RegDiagTrans_practical_soln.html#provide-plots-to-show-that-your-transformations-have-mostly-fixed-the-model-violations.",
    "href": "VB_RegDiagTrans_practical_soln.html#provide-plots-to-show-that-your-transformations-have-mostly-fixed-the-model-violations.",
    "title": "VectorByte Methods Training",
    "section": "5. Provide plots to show that your transformations have (mostly) fixed the model violations.",
    "text": "5. Provide plots to show that your transformations have (mostly) fixed the model violations.\n\npar(mfrow=c(3,4), mar=c(4,4,2,0.5))  \nplot(lm1$fitted, rstudent(lm1), col=1,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"I\")\nplot(lm2$fitted, rstudent(lm2), col=2,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"II\")\nplot(lm3$fitted, rstudent(lm3), col=3,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"III\")\nplot(lm4$fitted, rstudent(lm4), col=4,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"IV\")\n\n## Q-Q plots\nqqnorm(rstudent(lm1), pch=20, col=1, main=\"\" )\nabline(a=0,b=1,lty=2)\nqqnorm(rstudent(lm2), pch=20, col=2, main=\"\" )\nabline(a=0,b=1,lty=2)\nqqnorm(rstudent(lm3), pch=20, col=3, main=\"\" )\nabline(a=0,b=1,lty=2)\nqqnorm(rstudent(lm4), pch=20, col=4, main=\"\" )\nabline(a=0,b=1,lty=2)\n\n## histograms of studentized residuals\nhist(rstudent(lm1), col=1, xlab=\"Studentized Residuals\", \n     main=\"\", border=8)\nhist(rstudent(lm2), col=2, xlab=\"Studentized Residuals\", main=\"\")\nhist(rstudent(lm3), col=3, xlab=\"Studentized Residuals\", main=\"\")\nhist(rstudent(lm4), col=4, xlab=\"Studentized Residuals\", main=\"\")"
  },
  {
    "objectID": "GP_Notes.html",
    "href": "GP_Notes.html",
    "title": "VectorByte Methods Training: Introduction to Gaussian Processes for Time Dependent Data (notes)",
    "section": "",
    "text": "This document introduces the conceptual background to Gaussian Process (GP) regression, along with mathematical concepts. We also demonstrate briefly fitting GPs using the laGP(Gramacy 2016) package in R. The material here is intended to give a more verbose introduction to what is covered in the lecture in order to support a student to work through the practical component. This material has been adapted from chapter 5 of the book Surrogates: Gaussian process modeling, design and optimization for the applied sciences by Robert Gramacy."
  },
  {
    "objectID": "GP_Notes.html#gaussian-process-prior",
    "href": "GP_Notes.html#gaussian-process-prior",
    "title": "VectorByte Methods Training: Introduction to Gaussian Processes for Time Dependent Data (notes)",
    "section": "Gaussian Process Prior",
    "text": "Gaussian Process Prior\nIn our setup, we assume that the data follows a Multivariate Normal Distribution. We can think of this as a Prior. Mathematically, we can write it as:\nY_n \\sim N \\ ( \\ \\mu \\ , \\ \\Sigma_n \\ )\nHere, Y and \\mu is an n \\times 1 vector and \\Sigma_n is a positive semi definite matrix. This means that,\nx^T \\Sigma_n x &gt; 0 \\ \\text{ for all } \\ x \\neq 0.\nFor our purposes, we will consider \\mu = 0.\nIn Simple Linear Regression, we assume \\Sigma_n = \\sigma^2 \\mathbb{I}. This means that Y_1 \\ , Y_2 \\ \\ ... \\ \\ Y_n are uncorrelated with each other. However, In a GP, we assume that there is some correlation between the responses. A common covariance function is the squared exponential kernel, which invovles the Euclidean distance i.e. for two inputs x and x', the correlation is defined as,\n\\Sigma(x, x') = \\exp \\left( \\ \\vert \\vert{x - x'} \\vert \\vert^2 \\ \\right)\nThis creates a positive semi-definite correlation kernel. It also uses the proximity of x and x' as a measure of correlation i.e. the closer two points in the input space are, the more highly their corresponding responses are correlated. We will learn the exact form of \\Sigma_n later in the tutorial. First, we need to learn about MVN Distribution and the Posterior Distribution given the data."
  },
  {
    "objectID": "GP_Notes.html#multivariate-normal-distribution",
    "href": "GP_Notes.html#multivariate-normal-distribution",
    "title": "VectorByte Methods Training: Introduction to Gaussian Processes for Time Dependent Data (notes)",
    "section": "Multivariate Normal Distribution",
    "text": "Multivariate Normal Distribution\nSuppose we have X = (X_1, X_2)\n\\begin{equation}\nX =\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\n\\end{bmatrix} \\; , \\;\n\\mu =\n\\begin{bmatrix}\n\\mu_1\\\\\n\\mu_2\\\\\n\\end{bmatrix}\\; , \\;\n\\end{equation} where X_1 and \\mu_1 are q \\times 1 and X_2 and \\mu_2 are (N-q)   \\times 1.\n\\begin{equation}\n\\Sigma =\n\\begin{bmatrix}\n\\Sigma_{11} & \\Sigma_{12}\\\\\n\\Sigma_{21} &  \\Sigma_{22}\\\\\n\\end{bmatrix}\n\\; \\; \\;\n\\\\[5pt]\n\\text{with dimensions } \\; \\;\n\\begin{bmatrix}\nq \\times q & q \\times (N-q) \\\\\n(N -q) \\times q &  (N-q) \\times (N-q)\\\\\n\\end{bmatrix}\n\\;\n\\end{equation}\nThen, we have\n\\begin{equation}\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\n\\end{bmatrix}\n\\ \\sim \\ \\mathcal{N}\n\\left(\n\\;\n\\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\end{bmatrix}\\; , \\;\n\\begin{bmatrix}\n\\Sigma_{11} & \\Sigma_{12}\\\\\n\\Sigma_{21} &  \\Sigma_{22}\\\\\n\\end{bmatrix}\n\\;\n\\right)\n\\\\[5pt]\n\\end{equation}\nNow we can derive the conditional distribution of X_1 \\vert X_2 using properties of MVN.\nX_1 \\vert X_2 \\ \\sim \\mathcal{N} (\\mu_{X_1 \\vert X_2}, \\ \\Sigma_{X_1 \\vert X_2})\nwhere,\n\\mu_{X_1 \\vert X_2} = \\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(x_2 - \\mu_2)\n\\Sigma_{X_1 \\vert X_2} = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1} \\Sigma_{21}\nNow, let’s look at this in our context.\nSuppose we have, D_n = (X_n, Y_n) where Y_n \\sim N \\ ( \\ 0 \\ , \\ \\Sigma_n \\ ). Now, for a new location x_p, we need to find the distribution of Y(x_p).\nWe want to find the distribution of Y(x_p) \\ \\vert \\ D_n. Using the information from above, we know this is normally distributed and we need to identify then mean and variance. Thus, we have\n\\begin{equation}\n\\begin{aligned}\nY(x_p) \\vert \\ D_n \\ & \\sim \\mathcal{N} \\left(\\mu(x_p) \\ , \\ \\sigma^2(x_p) \\right) \\; \\; \\text{where, }\\\\[3pt]\n\\mu(x_p) \\ & = \\Sigma(x_p, X_n) \\Sigma_n^{-1}Y_n \\; \\;\\\\[3pt]\n\\sigma^2(x_p) \\ & = \\Sigma(x_p, x_p) - \\Sigma(x_p, X_n) \\Sigma_n^{-1}\\Sigma(X_n, x_p) \\\\[3pt]\n\\end{aligned}\n\\end{equation}"
  },
  {
    "objectID": "GP_Notes.html#example-gp-for-toy-example",
    "href": "GP_Notes.html#example-gp-for-toy-example",
    "title": "VectorByte Methods Training: Introduction to Gaussian Processes for Time Dependent Data (notes)",
    "section": "Example: GP for Toy Example",
    "text": "Example: GP for Toy Example\nSuppose we have data from the following function, Y(x) = 5 \\ \\sin (x)\nNow we use the above, and try and obtain Y(x_p) \\vert Y. Here is a visual of what a GP prediction for this function looks like. Each gray line is a draw from our predicted normal distribution, the blue line is the truth and the black line is the mean prediction from our GP. The red lines are confidence bounds. Pretty good? Let’s learn how we do that."
  },
  {
    "objectID": "GP_Notes.html#covariance-function",
    "href": "GP_Notes.html#covariance-function",
    "title": "VectorByte Methods Training: Introduction to Gaussian Processes for Time Dependent Data (notes)",
    "section": "Covariance Function",
    "text": "Covariance Function\nAs mentioned before, the main action is in the specification of \\Sigma_n. Let’s express \\Sigma_n = \\tau^2 C_n where C_n is the correlation function. We will be using the squared exponential distance based correlation function. The kernel can be written down mathematically as,\nC_n = \\exp{ \\left( -\\frac{\\vert\\vert x - x' \\vert \\vert ^2}{\\theta} \\right ) + g \\mathbb{I_n}} \nHere, if x and x' are closer in distance, the responses are more highly correlated. Along with our input space we also notice three other paramters; which in this context are referred to as hyper-parameters as they are used for fine-tuning our predictions as opposed to directly affecting them.\nWe have three main hyper-parameters here:\n\n\\tau^2: Scale\nThis parameter dictates the amplitude of our function. A MVN Distribution with scale = 1 will usually have data between -2 to 2. As the scale increases, this range expands. Here is a plo that shows two different scales for the same function, with all other parameters fixed.\n\nlibrary(mvtnorm)\nlibrary(laGP)\n\nset.seed(24)\nn &lt;- 100\nX &lt;- as.matrix(seq(0, 20, length.out = n))\nDx &lt;- laGP::distance(X)\n\ng &lt;- sqrt(.Machine$double.eps)\nCn &lt;- (exp(-Dx) + diag(g, n))\n\nY &lt;- rmvnorm(1, sigma = Cn)\n\nset.seed(28)\ntau2 &lt;- 25\nY_scaled &lt;- rmvnorm(1, sigma = tau2 * Cn)\n\npar(mfrow = c(1, 2), mar = c(5, 5, 4, 2), cex.axis = 2, cex.lab = 2, cex.main = 3, font.lab = 2)\n\n# Plot 1\nmatplot(X, t(Y), type = 'l', main = expression(paste(tau^2, \" = 1\")), \n        ylab = \"Y\", xlab = \"X\", lwd = 2, col = \"blue\")\n\n# Plot 2\nmatplot(X, t(Y_scaled), type = 'l', main = expression(paste(tau^2, \" = 25\")), \n        ylab = \"Y\", xlab = \"X\", lwd = 2, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\\theta: Length-scale\nThe length-scale controls the wiggliness of the function. It is also referred to as the rate of decay of correlation. The smaller it’s value, the more wiggly the function gets. This is because we expect the change in directionalilty of the function to be rather quick. We will once again demonstrate how a difference in magnitude of \\theta affects the function, keeping everything else constant.\n\nset.seed(1)\nn &lt;- 100\nX &lt;- as.matrix(seq(0, 10, length.out = n))\nDx &lt;- laGP::distance(X)\n\ng &lt;- sqrt(.Machine$double.eps)\ntheta1 &lt;- 0.5\nCn &lt;- (exp(-Dx/theta1) + diag(g, n))\n\nY &lt;- rmvnorm(1, sigma = Cn)\n\ntheta2 &lt;- 5\nCn &lt;- (exp(-Dx/theta2) + diag(g, n))\n\nY2 &lt;- rmvnorm(1, sigma = Cn)\n\npar(mfrow = c(1, 2), mar = c(5, 5, 4, 2), cex.axis = 2, cex.lab = 2, cex.main = 3, font.lab = 2)\nmatplot(X, t(Y), type= 'l', main = expression(paste(theta, \" = 0.5\")),\n     ylab = \"Y\", ylim = c(-2.2, 2.2), lwd = 2, col = \"blue\")\nmatplot(X, t(Y2), type= 'l',  main = expression(paste(theta, \" = 5\")),\n     ylab = \"Y\", ylim = c(-2.2, 2.2), lwd = 2, col = \"red\")\n\n\n\n\n\n\n\n\nAn extenstion: Anisotropic GP\nIn a multi-dimensional input setup, where X_{n \\times m} = (\\underline{X}_1, ... \\underline{X}_m). Here, the input space is m-dimensional and we have n observations. We can adjust the kernel so that each dimension has it’s own \\theta i.e. the rate of decay of correlation is different from one dimension to another. This can be done by simply modifying the correlation function, and writing it as,\nC_n = \\exp{ \\left( - \\sum_{k=1}^{m}  \\frac{\\vert\\vert x - x' \\vert \\vert ^2}{\\theta_k} \\right ) + g \\mathbb{I_n}} \nThis type of modelling is also referred to a seperable GP since we can take the sum outside the exponent and it will be a product of m independent dimensions. If we set all \\theta_k= \\theta, it would be an isotropic GP.\n\n\ng: Nugget\nThis parameter adds some noise into the function. For g &gt; 0, it determines the magnitude of discontinuity as x' tends to x. It is also called the nugget effect. For g=0, there would be no noise and the function would interpolate between the points. This effect is only added to the diagonals of the correlation matrix. We never add g to the off-diagonal elements. This also allows for numeric stability. Below is a snippet of what different magnitudes of g look like.\n\nlibrary(mvtnorm)\nlibrary(laGP)\n\nn &lt;- 100\nX &lt;- as.matrix(seq(0, 10, length.out = n))\nDx &lt;- laGP::distance(X)\n\ng &lt;- sqrt(.Machine$double.eps)\nCn &lt;- (exp(-Dx) + diag(g, n))\nY &lt;- rmvnorm(1, sigma = Cn)\n\nCn &lt;- (exp(-Dx) + diag(1e-2, n))\n\nL &lt;- rmvnorm(1, sigma = diag(1e-2, n))\nY2 &lt;- Y + L\n\npar(mfrow = c(1, 2), mar = c(5, 5, 4, 2), cex.axis = 2, cex.lab = 2, cex.main = 3, font.lab = 2)\nplot(X, t(Y), main = expression(paste(g, \" &lt; 1e-8\")),\n     ylab = \"Y\", xlab = \"X\", pch = 19, cex = 1.5, col = 1)\nlines(X, t(Y), col = \"blue\", lwd = 3) \n\nplot(X, t(Y2), main = expression(paste(g, \" = 0.01\")),\n     ylab = \"Y\", xlab = \"X\", pch = 19, cex = 1.5, col = 1)\nlines(X, t(Y), col = \"blue\", lwd = 3)\n\n\n\n\n\n\n\n\nAn extension: Heteroskedastic GP\nWe will study this in some detail later, but here instead of using one nugget g for the entire model, we use a vector of nuggets \\Lambda; one unique nugget for each unique input i.e. simply put, a different value gets added to each diagonal element.\nBack to GP fitting\nFor now, let’s get back to GP and fitting and learn how to use it. We have already seen a small example of the laGP package in action. However, we had not used any of the hyper-parameters in that case. We assumed to know all the information. However, that is not always the case. Without getting into the nitty-gritty details, here is how we obtain our parameters when we have some real data D_n = (X_n, Y_n).\n\ng and \\theta: An estimate can be obtained using MLE method by maximizing the likelihood. This is done using numerical algorithms.\n\\tau^2: An estimate is obtained as a closed form solution once we plug in g."
  },
  {
    "objectID": "VB_RegDiagTrans.html#learning-objectives",
    "href": "VB_RegDiagTrans.html#learning-objectives",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nReview assumptions of SLR/MLR models\nReview using diagnostic plots to assess whether assumptions are met\nReview the idea of basic transformations to use when assumptions aren’t met"
  },
  {
    "objectID": "VB_RegDiagTrans.html#slr-model-assumptions",
    "href": "VB_RegDiagTrans.html#slr-model-assumptions",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "SLR model assumptions",
    "text": "SLR model assumptions\n\\[\nY_i |X_i \\stackrel{ind}{\\sim} \\mathcal{N}(\\beta_0 + \\beta_1 X_i, \\sigma^2)\n\\]\nRecall the key assumptions of the Simple Linear Regression model:\n\nThe conditional mean of \\(Y\\) is linear in \\(X\\).\nThe additive errors (deviations from line)\n\nare normally distributed\nindependent from each other\nidentically distributed (i.e., they have constant variance)"
  },
  {
    "objectID": "VB_RegDiagTrans.html#example-model-violations",
    "href": "VB_RegDiagTrans.html#example-model-violations",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Example model violations",
    "text": "Example model violations\nAnscombe’s quartet comprises four datasets that have similar statistical properties …\n\n\n\n\nXmean\nYmean\nXsd\nYsd\nXYcor\n\n\n\n\n1\n9.000\n7.501\n3.317\n2.032\n0.816\n\n\n2\n9.000\n7.501\n3.317\n2.032\n0.816\n\n\n3\n9.000\n7.500\n3.317\n2.030\n0.816\n\n\n4\n9.000\n7.501\n3.317\n2.031\n0.817"
  },
  {
    "objectID": "VB_RegDiagTrans.html#residuals-and-the-model-assumptions",
    "href": "VB_RegDiagTrans.html#residuals-and-the-model-assumptions",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Residuals and the model assumptions",
    "text": "Residuals and the model assumptions\nRecall that the linear regression model assumes \\[\nY_i =\\beta_0 + \\beta_1 X_i + \\varepsilon_i,~~\\mbox{where}~~\n\\varepsilon_i \\stackrel{iid}{\\sim} \\mathcal{N}(0,\\sigma^2).\n\\]\nOur goal is to determine if the “true” residuals are iid normal and unrelated to \\(X\\). If the SLR model assumptions are true, then the residuals must be just “white noise”:\n\nEach \\(\\varepsilon_i\\) has the same variance (\\(\\sigma^2\\)).\nEach \\(\\varepsilon_i\\) has the same mean (0).\nAll of the \\(\\varepsilon_i\\) have the same normal distribution."
  },
  {
    "objectID": "VB_RegDiagTrans.html#understanding-leverage",
    "href": "VB_RegDiagTrans.html#understanding-leverage",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Understanding Leverage",
    "text": "Understanding Leverage\nThe \\(h_i\\) leverage term measures sensitivity of the estimated least squares regression line to changes in \\(Y_i\\).\nThe term “leverage” provides a mechanical intuition:\n\nThe farther you are from a pivot joint, the more torque you have pulling on a lever.\n\nHere is a nice online (interactive) illustration of leverage:\n\nhttps://omaymas.shinyapps.io/Influence_Analysis/\n\n\nOutliers do more damage if they have high leverage!"
  },
  {
    "objectID": "VB_RegDiagTrans.html#standardized-residuals",
    "href": "VB_RegDiagTrans.html#standardized-residuals",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Standardized residuals",
    "text": "Standardized residuals\nSince \\(e_i \\sim N(0, \\sigma^2 [1-h_i])\\), we know that \\[\n\\color{red}{\\frac{e_i}{\\sigma \\sqrt{1-h_i} }\\sim N(0, 1)}.\n\\]\nThese transformed \\(e_i\\)’s are called the standardized residuals.\n\nThey all have the same distribution if the SLR model assumptions are true.\nThey are almost (close enough) independent (\\(\\stackrel{iid}{\\sim}N(0,1)\\)).\nEstimate \\(\\sigma^2 \\approx s^2 = \\frac{1}{n-p}\\sum_{j=1}^n e_j^2\\). (\\(p=2\\) for SLR)"
  },
  {
    "objectID": "VB_RegDiagTrans.html#studentized-residuals",
    "href": "VB_RegDiagTrans.html#studentized-residuals",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Studentized residuals",
    "text": "Studentized residuals\nWe thus define a standard Studentized residual as \\[\nr_i = \\frac{e_i}{s_{-i} \\sqrt{1-h_i} }\\sim t_{n-p-1}(0, 1)\n\\] where \\(s_{-i}^2 = \\frac{1}{n-p-1}\\sum_{j \\neq i} e_j^2\\) is \\(\\hat{\\sigma~}^2\\) calculated without \\(e_i\\).\n\nThese are easy to get in R with the rstudent() function:\n\nas.numeric(rstudent(reg3))\n#&gt;  [1]   -0.43905545   -0.18550224 1203.53946383   -0.31384418   -0.57429485\n#&gt;  [6]   -1.15598185    0.06640743    0.36185145   -0.73567703   -0.06576806\n#&gt; [11]    0.20026336"
  },
  {
    "objectID": "VB_RegDiagTrans.html#outliers-and-studentized-residuals",
    "href": "VB_RegDiagTrans.html#outliers-and-studentized-residuals",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Outliers and Studentized residuals",
    "text": "Outliers and Studentized residuals\nSince the studentized residuals are distributed \\(t_{n-p-1}(0,1)\\), we should be concerned about any \\(r_i\\) outside of about \\([-2.5, 2.5]\\).\n\n (Note: As \\(n\\) gets much bigger, we will expect to see some very rare events (big \\(\\color{red}{\\varepsilon_i}\\) ) and not get worried unless \\(|r_i| &gt; 3\\) or \\(4\\).)"
  },
  {
    "objectID": "VB_RegDiagTrans.html#how-to-deal-with-outliers",
    "href": "VB_RegDiagTrans.html#how-to-deal-with-outliers",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "How to deal with outliers",
    "text": "How to deal with outliers\n\n\n\n\nfrom Research Wahlberg"
  },
  {
    "objectID": "VB_RegDiagTrans.html#how-to-deal-with-outliers-1",
    "href": "VB_RegDiagTrans.html#how-to-deal-with-outliers-1",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "How to deal with outliers",
    "text": "How to deal with outliers\nWhen should you delete outliers?\n\nOnly when you have a really good reason!\n\nThere is nothing wrong with running a regression with and without potential outliers to see whether results are significantly impacted.\nAny time outliers are dropped, the reasons for doing so should be clearly noted.\n\nI maintain that both a statistical and a non-statistical reason are required."
  },
  {
    "objectID": "VB_RegDiagTrans.html#outliers-leverage-and-residuals",
    "href": "VB_RegDiagTrans.html#outliers-leverage-and-residuals",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Outliers, leverage, and residuals",
    "text": "Outliers, leverage, and residuals\nWarning: Unfortunately, outliers with high leverage are hard to catch through \\(\\color{dodgerblue}{r_i}\\) (since the line is pulled towards them).\nMeans get distracted by outliers…"
  },
  {
    "objectID": "VB_RegDiagTrans.html#outliers-leverage-and-residuals-1",
    "href": "VB_RegDiagTrans.html#outliers-leverage-and-residuals-1",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Outliers, leverage, and residuals",
    "text": "Outliers, leverage, and residuals\nWarning: Unfortunately, outliers with high leverage are hard to catch through \\(\\color{dodgerblue}{r_i}\\) (since the line is pulled towards them).\nConsider data on house Rents vs SqFt:\n\nPlots of \\(r_i\\) or \\(e_i\\) against \\(\\hat{Y~}_i\\) or \\(X_i\\) are still your best diagnostic!"
  },
  {
    "objectID": "VB_RegDiagTrans.html#normality-and-studentized-residuals",
    "href": "VB_RegDiagTrans.html#normality-and-studentized-residuals",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Normality and studentized residuals",
    "text": "Normality and studentized residuals\nA more subtle issue is the normality of the distribution on \\(\\varepsilon\\).\n\nWe can look at the residuals to judge normality if \\(n\\) is big enough (say \\(&gt;20~~ \\rightarrow\\) less than that makes it too hard to call).\n\nIn particular, if we have decent size \\(\\color{red}{n}\\), we want the shape of the studentized residual distribution to “look” like \\(\\color{red}{N(0,1)}\\).\n The most obvious tactic is to look at a histogram of \\(r_i\\)."
  },
  {
    "objectID": "VB_RegDiagTrans.html#assessing-normality-via-q-q-plots",
    "href": "VB_RegDiagTrans.html#assessing-normality-via-q-q-plots",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Assessing normality via Q-Q plots",
    "text": "Assessing normality via Q-Q plots\nHigher fidelity diagnostics are provided by normal quantile-quantile (Q-Q) plots that:\n\nplot the sample quantiles (e.g. \\(10^{th}\\) percentile, etc.)\nagainst true percentiles from a \\(N(0,1)\\) distribution (e.g. \\(-1.96\\) is the true 2.5% quantile).\n\nIf \\(r_i \\sim N(0,1)\\) these quantiles should be equal\n\nlie on a line through 0 with slope 1"
  },
  {
    "objectID": "VB_RegDiagTrans.html#go-to-diagnostic-plots",
    "href": "VB_RegDiagTrans.html#go-to-diagnostic-plots",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "3 Go-To Diagnostic Plots",
    "text": "3 Go-To Diagnostic Plots"
  },
  {
    "objectID": "VB_RegDiagTrans.html#violations-of-slr-model-assumptions",
    "href": "VB_RegDiagTrans.html#violations-of-slr-model-assumptions",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Violations of SLR Model Assumptions",
    "text": "Violations of SLR Model Assumptions\n\\[\\color{dodgerblue}{Y_i |X_i \\stackrel{ind}{\\sim} \\mathcal{N}(\\beta_0 + \\beta_1 X_i, \\sigma^2)}\\]\n\nThe conditional mean of \\(Y\\) is linear in \\(X\\).\nThe additive errors (deviations from line)\n\nare normally distributed\nindependent from each other\nidentically distributed (i.e., they have constant variance)\n\n\nAll of these can be violated! Let’s see what violations look like and how we can deal with them within the SLR framework."
  },
  {
    "objectID": "VB_RegDiagTrans.html#violation-1-non-constant-variance",
    "href": "VB_RegDiagTrans.html#violation-1-non-constant-variance",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Violation 1: Non-constant variance",
    "text": "Violation 1: Non-constant variance\nIf you get a trumpet shape (bunching of the \\(Y\\)s), you have nonconstant variance.\n\nThis violates our assumption that all \\(\\varepsilon_i\\) have the same \\(\\sigma^2\\)."
  },
  {
    "objectID": "VB_RegDiagTrans.html#solution-1-variance-stabilizing-transformations",
    "href": "VB_RegDiagTrans.html#solution-1-variance-stabilizing-transformations",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Solution 1: Variance stabilizing transformations",
    "text": "Solution 1: Variance stabilizing transformations\nThis is one of the most common model violations; luckily, it is usually fixable by transforming the response (\\(Y\\)) variable.\n\\(\\color{dodgerblue}{\\log(Y)}\\) is the most common variance stabilizing transform.\n\nIf \\(Y\\) has only positive values (e.g. sales) or is a count (e.g. # of customers), take \\(\\log(Y)\\) (always natural log).\n\n\\(\\color{dodgerblue}{\\sqrt{Y}}\\) is sometimes used, especially if the data have zeros.\n\nIn general, think what you expect to be linear for your data."
  },
  {
    "objectID": "VB_RegDiagTrans.html#violation-2-nonlinear-residual-patterns",
    "href": "VB_RegDiagTrans.html#violation-2-nonlinear-residual-patterns",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Violation 2: Nonlinear residual patterns",
    "text": "Violation 2: Nonlinear residual patterns\nConsider regression residuals for the 2nd Anscombe dataset:\n\nThings are not good! It appears that we do not have a linear mean function; that is \\(\\color{dodgerblue}{\\mathbb{E}[Y] \\neq \\beta_0 + \\beta_1 X}\\)."
  },
  {
    "objectID": "VB_RegDiagTrans.html#solution-2-polynomial-regression",
    "href": "VB_RegDiagTrans.html#solution-2-polynomial-regression",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Solution 2: Polynomial regression",
    "text": "Solution 2: Polynomial regression\nEven though we are limited to a linear mean, it is possible to get nonlinear regression by transforming the \\(X\\) variable.\n\nIn general, we can add powers of \\(\\color{dodgerblue}X\\) to get polynomial regression: \\(\\color{red}{\\mathbb{E}[Y] = \\beta_0 + \\beta_1X + \\beta_2 X^2 + \\cdots + \\beta_m X^m}\\)\n\nYou can fit any mean function if \\(m\\) is big enough.\n\nUsually stick to m=2 unless you have a good reason."
  },
  {
    "objectID": "VB_RegDiagTrans.html#testing-for-nonlinearity",
    "href": "VB_RegDiagTrans.html#testing-for-nonlinearity",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Testing for nonlinearity",
    "text": "Testing for nonlinearity\nTo see if you need more nonlinearity, try the regression which includes the next polynomial term, and see if it is significant.\nFor example, to see if you need a quadratic term,\n\nfit the model then run the regression \\(\\mathbb{E}[Y] = \\beta_0 + \\beta_1 X + \\beta_2 X^2\\).\nIf your test implies \\(\\color{dodgerblue}{\\beta_2 \\neq 0}\\), you need \\(\\color{dodgerblue}{X^2}\\) in your model.\n\nNote: \\(p\\)-values are calculated “given the other \\(\\beta\\)’s are nonzero”; i.e., conditional on \\(X\\) being in the model."
  },
  {
    "objectID": "VB_RegDiagTrans.html#closing-comments-on-polynomials",
    "href": "VB_RegDiagTrans.html#closing-comments-on-polynomials",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Closing comments on polynomials",
    "text": "Closing comments on polynomials\n\nWe can always add higher powers (cubic, etc.) if necessary.\n\nIf you add a higher order term, the lower order term is kept in the model regardless of its individual \\(t\\)-stat.\n\nBe very careful about predicting outside the data range as the curve may do unintended things beyond the data.\nWatch out for over-fitting.\n\nYou can get a “perfect” fit with enough polynomial terms,\nbut that doesn’t mean it will be any good for prediction or understanding."
  },
  {
    "objectID": "VB_RegDiagTrans.html#other-problems",
    "href": "VB_RegDiagTrans.html#other-problems",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Other problems",
    "text": "Other problems\nSometimes we have other strange things going on in our data sets\n\ndata are “clumped” up in \\(X\\) – high leverage points\nresiduals still aren’t normally distributed after taking transforms from earlier\nresponses take discrete values instead of continuous\n\n\nThe latter 2 we can deal with using MLR and GLMs. What about the first?"
  },
  {
    "objectID": "VB_RegDiagTrans.html#the-log-log-model",
    "href": "VB_RegDiagTrans.html#the-log-log-model",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "The log-log model",
    "text": "The log-log model\nThe other common covariate transform is \\(\\log(X)\\).\n\nWhen \\(X\\)-values are bunched up, \\(\\log(X)\\) helps spread them out and reduces the leverage of extreme values.\nRecall that both reduce \\(s_{b_1}\\).\n\nIn practice, this is often used in conjunction with a \\(\\log(Y)\\) response transformation. The log-log model is \\[\n    \\color{red}{\\log(Y) = \\beta_0 + \\beta_1 \\log(X) + \\varepsilon}.\n    \\]\nIt is super useful, and has some special properties …"
  },
  {
    "objectID": "VB_RegDiagTrans.html#elasticity-and-the-log-log-model",
    "href": "VB_RegDiagTrans.html#elasticity-and-the-log-log-model",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Elasticity and the log-log model",
    "text": "Elasticity and the log-log model\nIn a log-log model, the slope \\(\\beta_1\\) is sometimes called elasticity.\nThe elasticity is (roughly) % change in \\(Y\\) per 1% change in \\(X\\). \\[\\color{dodgerblue}{\n\\beta_1 \\approx \\frac{d\\%Y}{d\\%X}}\\] For example, economists often assume that GDP has import elasticity of 1. Indeed:\n\nGDPlm&lt;-lm(log(GDP) ~ log(IMPORTS))\ncoef(GDPlm)\n#&gt;  (Intercept) log(IMPORTS) \n#&gt;     1.891516     0.969337\n\n\n(Can we test for 1%?)"
  },
  {
    "objectID": "VB_RegDiagTrans.html#practical",
    "href": "VB_RegDiagTrans.html#practical",
    "title": "Review of Diagnostics and Transformations for Regression Models",
    "section": "Practical",
    "text": "Practical\nNext we’ll do a short practical to practice:\n\nFitting linear models in R\nChecking diagnostics\nChoosing transformations\nPlotting predictions"
  },
  {
    "objectID": "VB_RegDiagTrans_practical.html#fit-the-linear-regression-model.-plot-the-data-and-fitted-line.",
    "href": "VB_RegDiagTrans_practical.html#fit-the-linear-regression-model.-plot-the-data-and-fitted-line.",
    "title": "VectorByte Methods Training: Regression Diagnostics and Transformations (practical)",
    "section": "1. Fit the linear regression model. Plot the data and fitted line.",
    "text": "1. Fit the linear regression model. Plot the data and fitted line.\n\n## fit models\nlm1 &lt;- lm(Y1 ~ X1)\n\n## plot points and fitted lines\nplot(X1, Y1, col=1, main=\"I\"); abline(lm1, col=2)"
  },
  {
    "objectID": "VB_RegDiagTrans_practical.html#provide-a-scatterplot-normal-q-q-plot-and-histogram-for-the-studentized-regression-residuals.",
    "href": "VB_RegDiagTrans_practical.html#provide-a-scatterplot-normal-q-q-plot-and-histogram-for-the-studentized-regression-residuals.",
    "title": "VectorByte Methods Training: Regression Diagnostics and Transformations (practical)",
    "section": "2. Provide a scatterplot, normal Q-Q plot, and histogram for the studentized regression residuals.",
    "text": "2. Provide a scatterplot, normal Q-Q plot, and histogram for the studentized regression residuals.\n\npar(mfrow=c(1,3), mar=c(4,4,2,0.5))   \n\n## studentized residuals vs fitted\nplot(lm1$fitted, rstudent(lm1), col=1,\n     xlab=\"Fitted Values\", \n     ylab=\"Studentized Residuals\", \n     pch=20, main=\"I\")\n\n## qq plot of studentized residuals\nqqnorm(rstudent(lm1), pch=20, col=1, main=\"\" )\nabline(a=0,b=1,lty=2, col=2)\n\n## histogram of studentized residuals\nhist(rstudent(lm1), col=1, \n     xlab=\"Studentized Residuals\", \n     main=\"\", border=8)"
  },
  {
    "objectID": "VB_RegDiagTrans_practical.html#using-the-residual-scatterplots-state-how-the-slr-model-assumptions-are-violated.",
    "href": "VB_RegDiagTrans_practical.html#using-the-residual-scatterplots-state-how-the-slr-model-assumptions-are-violated.",
    "title": "VectorByte Methods Training: Regression Diagnostics and Transformations (practical)",
    "section": "3. Using the residual scatterplots, state how the SLR model assumptions are violated.",
    "text": "3. Using the residual scatterplots, state how the SLR model assumptions are violated.\nXs are clumpy AND the variance seems non-constant. It looks a lot like the GDP data from class. Since both Xs and Ys are strictly positive, we can try a log-log transform."
  },
  {
    "objectID": "VB_RegDiagTrans_practical.html#determine-the-data-transformation-to-correct-the-problems-in-3-fit-the-corresponding-regression-model-and-plot-the-transformed-data-with-new-fitted-line.",
    "href": "VB_RegDiagTrans_practical.html#determine-the-data-transformation-to-correct-the-problems-in-3-fit-the-corresponding-regression-model-and-plot-the-transformed-data-with-new-fitted-line.",
    "title": "VectorByte Methods Training: Regression Diagnostics and Transformations (practical)",
    "section": "4. Determine the data transformation to correct the problems in 3, fit the corresponding regression model, and plot the transformed data with new fitted line.",
    "text": "4. Determine the data transformation to correct the problems in 3, fit the corresponding regression model, and plot the transformed data with new fitted line.\n\n### the fix is as follows:\nlogX1&lt;- log(X1)\nlogY1 &lt;- log(Y1)\n\n### re-run the regressions and residual plots to show this worked\nlm1 &lt;- lm(logY1 ~ logX1)\n\n## plot points and lines\nplot(logX1, logY1, col=1, main=\"I\"); abline(lm1, col=2)"
  },
  {
    "objectID": "VB_RegDiagTrans_practical.html#provide-plots-to-show-that-your-transformations-have-mostly-fixed-the-model-violations.",
    "href": "VB_RegDiagTrans_practical.html#provide-plots-to-show-that-your-transformations-have-mostly-fixed-the-model-violations.",
    "title": "VectorByte Methods Training: Regression Diagnostics and Transformations (practical)",
    "section": "5. Provide plots to show that your transformations have (mostly) fixed the model violations.",
    "text": "5. Provide plots to show that your transformations have (mostly) fixed the model violations.\n\n## studentized residuals vs fitted\n\npar(mfrow=c(1,3), mar=c(4,4,2,0.5))  \nplot(lm1$fitted, rstudent(lm1), col=1,\n     xlab=\"Fitted Values\", \n     ylab=\"Studentized Residuals\", \n     pch=20, main=\"I\")\n\n## Q-Q plots\nqqnorm(rstudent(lm1), pch=20, col=1, main=\"\" )\nabline(a=0,b=1,lty=2, col=2)\n\n## histograms of studentized residuals\nhist(rstudent(lm1), col=1, \n     xlab=\"Studentized Residuals\", \n     main=\"\", border=8)\n\n\n\n\n\n\n\n\nThis is much better! The histogram still maybe looks a little funny, but given that the qq-plot looks pretty good, I think we’ve made a good transformation."
  },
  {
    "objectID": "VB_RegDiagTrans_practical.html#your-turn",
    "href": "VB_RegDiagTrans_practical.html#your-turn",
    "title": "VectorByte Methods Training: Regression Diagnostics and Transformations (practical)",
    "section": "Your Turn!",
    "text": "Your Turn!\nRepeat this process with the other 3 datasets, and see if you can figure out a appropriate transformations for each dataset."
  },
  {
    "objectID": "VB_TimeDepData.html#learning-objectives",
    "href": "VB_TimeDepData.html#learning-objectives",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand a bit about the types of time-dependent data\nBe able to conduct simple regression based analyses for time-dependent data.\nBe able to check model assumptions for time dependent data."
  },
  {
    "objectID": "VB_TimeDepData.html#time-dependent-data",
    "href": "VB_TimeDepData.html#time-dependent-data",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Time Dependent Data",
    "text": "Time Dependent Data\nMuch of the data we collect in VBD applications depends on time such as\n\nobserved cases in a city or country\nnumber of mosquitoes over time\n\nAdditionally, we often assume that the reasons these change over time may be because covariates (e.g., temperature, precipitation, insecticide spraying) change over time.\nHow do we incorporate these time varying factors into our regression models?"
  },
  {
    "objectID": "VB_TimeDepData.html#types-of-time-dependent-data",
    "href": "VB_TimeDepData.html#types-of-time-dependent-data",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Types of time dependent data",
    "text": "Types of time dependent data\nThe most common type of time-dependent data that statisticians talk about is time series data. These are data where observations are evenly spaced with no (or very little) missing observations.\n\nAlthough evenly spaced data are ideal (and the most common methods are designed for them), in VBD survey data we often don’t have evenly spaced observations. These data don’t have a specific name, and most time-series methods can’t be directly used with them."
  },
  {
    "objectID": "VB_TimeDepData.html#time-series-data-and-dependence",
    "href": "VB_TimeDepData.html#time-series-data-and-dependence",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Time series data and dependence",
    "text": "Time series data and dependence\nTime-series data are simply a collection of observations gathered over time. For example, suppose \\(y_1, \\ldots, y_T\\) are\n\ndaily temperature,\nsolar activity,\nCO\\(_2\\) levels,\nyearly population size.\n\nIn each case, we might expect what happens at time \\(t\\) to be correlated with time \\(t-1\\)."
  },
  {
    "objectID": "VB_TimeDepData.html#checking-for-dependence",
    "href": "VB_TimeDepData.html#checking-for-dependence",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Checking for dependence",
    "text": "Checking for dependence\nTo see if \\(Y_{t-1}\\) would be useful for predicting \\(Y_t\\), just plot them together and see if there is a relationship.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation between \\(Y_t\\) and \\(Y_{t-1}\\) is called autocorrelation."
  },
  {
    "objectID": "VB_TimeDepData.html#autocorrelation-for-time-series-data",
    "href": "VB_TimeDepData.html#autocorrelation-for-time-series-data",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Autocorrelation (for time series data)",
    "text": "Autocorrelation (for time series data)\nTo summarize the time-varying dependence, compute lag-\\(\\ell\\) correlations for \\(\\ell=1,2,3,\\ldots\\)\nIn general, the autocorrelation function (ACF) for \\(Y\\) is \\[\\color{red}{r(\\ell) = \\mathrm{cor}(Y_t, Y_{t-\\ell})}\\]\nFor our Roanoke temperature data:\n\nprint(acf(weather$temp))\n\n     0      1      2      3      4      5      6      7      8    \n 1.000  0.658  0.298  0.263  0.297  0.177  0.111  0.008 -0.099   \n    9     10    11     12     13     14     15     16     17 \n-0.045 0.071 -0.020 -0.157 -0.156 -0.146 -0.278 -0.346 -0.314"
  },
  {
    "objectID": "VB_TimeDepData.html#autoregression",
    "href": "VB_TimeDepData.html#autoregression",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Autoregression",
    "text": "Autoregression\nHow do we model data that exhibits autocorrelation?\nSuppose \\(Y_1 = \\varepsilon_1\\), \\(Y_2 = \\varepsilon_{1} + \\varepsilon_{2}\\), \\(Y_3 = \\varepsilon_{1} + \\varepsilon_{2} + \\varepsilon_{3}\\), etc.\n\nThen \\(\\color{dodgerblue}{Y_t =  \\sum_{i=1}^{t}\\varepsilon_i = Y_{t-1} + \\varepsilon_t}\\) and \\(\\color{dodgerblue}{ \\mathbb{E}[Y_t] = Y_{t-1}}\\).\n\nThis is called a random walk model for \\(Y_t\\):\n\nthe expectation of what will happen is always what happened most recently."
  },
  {
    "objectID": "VB_TimeDepData.html#random-walk",
    "href": "VB_TimeDepData.html#random-walk",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Random walk",
    "text": "Random walk\nIn a random walk, the series just wanders around.\n\n\\(\\beta_1 = 1\\)"
  },
  {
    "objectID": "VB_TimeDepData.html#exploding-series",
    "href": "VB_TimeDepData.html#exploding-series",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Exploding series",
    "text": "Exploding series\nFor AR term \\(&gt;1\\), the \\(Y_t\\)’s move exponentially far from \\(Y_1\\).\n\n\\(\\beta_1 = 1.02\\)\n\n\n\nUseless for modeling and prediction."
  },
  {
    "objectID": "VB_TimeDepData.html#stationary-series",
    "href": "VB_TimeDepData.html#stationary-series",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Stationary series",
    "text": "Stationary series\nFor \\(\\beta_1&lt;1\\), \\(Y_t\\) is always pulled back towards the mean.\n\n\\(\\beta_1 = 0.8\\)\n\n\n\nThese are the most common and useful type of AR series."
  },
  {
    "objectID": "VB_TimeDepData.html#mean-reversion",
    "href": "VB_TimeDepData.html#mean-reversion",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Mean reversion",
    "text": "Mean reversion\nAn important property of stationary series is mean reversion.\nThink about shifting both \\(Y_t\\) and \\(Y_{t-1}\\) by their mean \\(\\mu\\). \\[\n\\color{dodgerblue}{Y_t - \\mu = \\beta_1 (Y_{t-1} - \\mu) +\\varepsilon_t}\n\\] Since \\(|\\beta_1| &lt; 1\\), \\(Y_t\\) is expected to be closer to \\(\\mu\\) than \\(Y_{t-1}\\).\nMean reversion is all over, and helps predict future behavior:\n\nweekly sales numbers,\ndaily temperature."
  },
  {
    "objectID": "VB_TimeDepData.html#negative-correlation",
    "href": "VB_TimeDepData.html#negative-correlation",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Negative correlation",
    "text": "Negative correlation\nIt is also possible to have negatively correlated AR(1) series.\n\n\\(\\beta_1 = -0.8\\)\n\n\n\nBut you see these far less often in practice."
  },
  {
    "objectID": "VB_TimeDepData.html#summary-of-ar1-behavior",
    "href": "VB_TimeDepData.html#summary-of-ar1-behavior",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Summary of AR(1) behavior",
    "text": "Summary of AR(1) behavior\n\n\\(\\color{dodgerblue}{|\\beta_1|&lt;1|}\\): The series has a mean level to which it reverts over time (stationary). For \\(+\\beta_1\\), the series tends to wander above or below the mean level for a while. For \\(-\\beta_1\\), the series tends to flip back and forth around the mean.\n\\(\\color{dodgerblue}{|\\beta_1|=1|}\\): A random walk series. The series has no mean level and, thus, is called nonstationary. The drift parameter \\(\\beta_0\\) is the direction in which the series wanders.\n\\(\\color{dodgerblue}{|\\beta_1|&gt;1|}\\): The series explodes, is nonstationary, and pretty much useless for prediction."
  },
  {
    "objectID": "VB_TimeDepData.html#arp-models",
    "href": "VB_TimeDepData.html#arp-models",
    "title": "Regression Methods for Time Dependent Data",
    "section": "AR(\\(p\\)) models",
    "text": "AR(\\(p\\)) models\nIt is possible to expand the AR idea to higher lags \\[\n\\color{red}{AR(p): Y_t = \\beta_0 + \\beta_1Y_{t-1} + \\cdots + \\beta_pY_{t-p} + \\varepsilon}.\n\\]\nHowever, it is seldom necessary to fit AR lags for \\(p&gt;1\\).\n\nLike having polynomial terms higher than 2, this just isn’t usually required in practice.\nYou lose all of the stationary/nonstationary intuition.\nOften, the need for higher lags is symptomatic of (missing) a more persistent trend or periodicity in the data, or needing predictors ..."
  },
  {
    "objectID": "VB_TimeDepData.html#trending-series",
    "href": "VB_TimeDepData.html#trending-series",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Trending series",
    "text": "Trending series\nOften, you’ll have a linear trend in your time series.\n\\(\\Rightarrow\\) AR structure, sloping up or down in time."
  },
  {
    "objectID": "VB_TimeDepData.html#periodic-models",
    "href": "VB_TimeDepData.html#periodic-models",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Periodic models",
    "text": "Periodic models\nIt is very common to see seasonality or periodicity in series.\n\nTemperature goes up in Summer and down in Winter.\nGas consumption in Blacksburg would do the opposite.\n\nRecall the monthly lung infection data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppears to oscillate on a 12-month cycle."
  },
  {
    "objectID": "VB_TimeDepData.html#alternative-periodicity",
    "href": "VB_TimeDepData.html#alternative-periodicity",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Alternative Periodicity",
    "text": "Alternative Periodicity\nAn alternative way to add periodicity would be to simply add a dummy variable for each month (feb, mar, apr, ...).\n\nThis achieves basically the same fit as above, without requiring you to add sine or cosine.\nHowever, this takes 11 periodic parameters while we use only 2."
  },
  {
    "objectID": "VB_TimeDepData.html#non-time-series-data",
    "href": "VB_TimeDepData.html#non-time-series-data",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Non-time series data",
    "text": "Non-time series data\n\nWhat happens if data aren’t evenly sampled?\n\n\nAll of the models/tools we explored that incorporate auto-correlation are not valid if data are not evenly spaced.\n\nYou can’t calculate an auto-correlation if the gap between data points and the earlier points aren’t all the same because we don’t expect all lags to have the same correlation.\n\n\nSo what can we do?"
  },
  {
    "objectID": "VB_TimeDepData.html#time-dependent-predictors",
    "href": "VB_TimeDepData.html#time-dependent-predictors",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Time Dependent Predictors",
    "text": "Time Dependent Predictors\nOften we have additional measurements of possible covariates that might impact the time-dependent responses that we want to model. E.g. in VBD systems:\n\nweather variables: temperature, rainfall, humidity\nhabitat/climate variables: greenness, ENSO, land use, container densities\nsocio-economic variables: bed net coverage, insecticide spraying\n\nThese may all depend on time, and can be incorporated into a model for all time dependent data (including time series!)."
  },
  {
    "objectID": "VB_TimeDepData.html#time-lagged-predictors",
    "href": "VB_TimeDepData.html#time-lagged-predictors",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Time-Lagged Predictors",
    "text": "Time-Lagged Predictors\nAdditionally, sometimes there may be a lag between an observed covariate and the response.\n\nExample: The number of people being hospitalized for dengue on a particular day reflect the number of people infected days before, and potentially mosquitoes infected days before that!\nThus, proxies of mosquito abundance, like temperature or humidity, weeks earlier may be appropriate predictors.\n\nHow can we determine an appropriate lag for a predictor?"
  },
  {
    "objectID": "VB_TimeDepData.html#two-strategies",
    "href": "VB_TimeDepData.html#two-strategies",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Two Strategies",
    "text": "Two Strategies\nThe first is what we might call a scientific approach:\n\nUsing our system knowledge, we can define what might be feasible time lags to include in a model, given evenly sampled predictor data. We decide and include just those a priori lags, and maybe do model/feature selection to narrow down.\n\nThis approach may miss a best lag for time series data, but is often the main way we can try to find appropriate lags for unevenly sampled data.\n(Note, we almost always assume a lag of at least 1.)"
  },
  {
    "objectID": "VB_TimeDepData.html#coming-up",
    "href": "VB_TimeDepData.html#coming-up",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Coming up!",
    "text": "Coming up!\nThe tools here are good, but not the best:\n\nIn many situations you want to allow for \\(\\beta\\) or \\(\\sigma\\) parameters that can change in time.\nThis can leave us with some left-over autocorrelation.\nWe’ll talk more about more sophisticated models over the next couple of days."
  },
  {
    "objectID": "VB_TimeDepData.html#practice",
    "href": "VB_TimeDepData.html#practice",
    "title": "Regression Methods for Time Dependent Data",
    "section": "Practice",
    "text": "Practice\nNow we’ll practice combining our regression tools with these additional techniques for time-dependent data.\n\nRemember:\n\nAlso ways check your residual plots to ensure that your assumptions have been met\nTransformations are your friend!\nThink carefully about how to line up your lagged predictors"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the VectorByte Training Materials 2024",
    "section": "",
    "text": "As the VectorByte team has developed these materials, we’ve aimed to provide resources for both guided (during the workshop) and self-led learning. We assume basic familiarity with:\n\nThe R Programming Language\nBasic calculus (especially the mathematical idea of functions)\nBasic probability and statistics (e.g., what is a probability distribution, normal and binomial distributions, means, variances)\nBasics of regression\n\nWe’ve divided the materials into subject matter modules or units. Each module is designed to build on the previous one, and expects at least knowledge of all of the preceding modules in the sequence in addition to the background material.\nEach module consists of four kinds of materials:\n\nslides with presentation of materials\nlabs/hands-on materials to allow you to practice material in a practical way\nsolutions to exercises, when necessary\n\nWe also include links to additional resources/materials/references.\nFor more information about the goals and approach of VectorByte are available at vectorbyte.org."
  },
  {
    "objectID": "VB_RegRev.html",
    "href": "VB_RegRev.html",
    "title": "VectorByte Methods Training: Regression Review",
    "section": "",
    "text": "Main materials"
  },
  {
    "objectID": "VB_RegRev.html#mc-simulation-of-simple-system",
    "href": "VB_RegRev.html#mc-simulation-of-simple-system",
    "title": "VectorByte Methods Training: Regression Review",
    "section": "MC simulation of simple system",
    "text": "MC simulation of simple system"
  },
  {
    "objectID": "VB_RegRev.html#changes-in-population-size",
    "href": "VB_RegRev.html#changes-in-population-size",
    "title": "VectorByte Methods Training: Regression Review",
    "section": "Changes in population size",
    "text": "Changes in population size"
  },
  {
    "objectID": "VB_RegRev.html#sequential-observations",
    "href": "VB_RegRev.html#sequential-observations",
    "title": "VectorByte Methods Training: Regression Review",
    "section": "Sequential observations",
    "text": "Sequential observations\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we had just observed these data, how might we try to estimate parameters?"
  },
  {
    "objectID": "VB_RegRev.html#sampling-distribution-of-ls-line",
    "href": "VB_RegRev.html#sampling-distribution-of-ls-line",
    "title": "VectorByte Methods Training: Regression Review",
    "section": "Sampling distribution of LS line",
    "text": "Sampling distribution of LS line\nWhat did we just do?\n\nWe “imagined” through simulation the sampling distribution of a LS line.\n\nIn real life we get just one data set, and we don’t know the true generating model. But we can still imagine.\n\nWe first find the sampling distribution of our LS coefficients, b_0 and b_1…\n… which requires some review.\n\nIn the online reading and review materials you should have come across some useful probability/stats facts, including:\n\n\\mathbb{E}(X_1+X_2) = \\mathbb{E}(X_1)+ \\mathbb{E}(X_2)\n\\mathbb{E}(cX_1) = c \\mathbb{E}(X_1)\n\\text{var}(c X_1) = c^2\\text{var}(X_1)\n\\text{var}(X_1+X_2) = \\text{var}(X_1)+\\text{var}(X_2) + 2\\text{cov}(X_1 X_2).\n\n\nRecall: distribution of the sample mean\n\nStep back for a moment and consider the mean for an iid sample of n observations of a random variable \\{X_1,\\ldots,X_n\\}.\n\nSuppose that \\mathbb{E}(X_i) = \\mu and \\text{var}(X_i) = \\sigma^2, then\n\n\\mathbb{E}(\\bar{X}) = \\frac{1}{n} \\sum\\mathbb{E}(X_i) = \\mu\n\\text{var}(\\bar{X}) = \\text{var}\\left( \\frac{1}{n} \\sum X_i \\right) = \\frac{1}{n^2} \\sum \\text{var}\\left( X_i \\right) = \\displaystyle \\frac{\\sigma^2}{n}."
  },
  {
    "objectID": "VB_RegRev.html#central-limit-theorem",
    "href": "VB_RegRev.html#central-limit-theorem",
    "title": "VectorByte Methods Training: Regression Review",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe CLT states that for iid random variables, X, with mean \\mu and variance \\sigma^2, the distribution of the sample mean becomes normal as the number of observations, n, gets large.\n\nThat is, \\displaystyle \\bar{X} \\rightarrow_{n} \\mathcal{N}(\\mu, \\sigma^2/n) , and sample averages tend to be normally distributed in large samples.\n\nWe are now ready to describe the sampling distribution of the least squares line …\n… in terms of its effect on the sampling distributions of the coefficients\n\nb_1 = \\hat{\\beta~}_1, the slope of the line\nb_0 = \\hat{\\beta~}_0, the intercept,\nand how they covary together,\n\ngiven a particular (fixed) set of X-values."
  },
  {
    "objectID": "VB_RegRev.html#sampling-distribution-of-b_1",
    "href": "VB_RegRev.html#sampling-distribution-of-b_1",
    "title": "VectorByte Methods Training: Regression Review",
    "section": "Sampling distribution of b_1",
    "text": "Sampling distribution of b_1\nIt turns out that b_1 is normally distributed: b_1 \\sim \\mathcal{N}(\\beta_1, \\sigma^2_{b_1}).\n\nb_1 is unbiased: \\mathbb{E}[b_1] = \\beta_1.\nThe sampling sd \\sigma_{b_1} determines precision of b_1: \n\\sigma_{b_1}^2\n= \\text{var}(b_1) = \\frac{\\sigma^2}{\\sum (X_i - \\bar{X})^2} = \\frac{\\sigma^2}{(n-1)s_x^2}.\n It depends on three factors: 1) sample size (n); 2) error variance (\\sigma^2 = \\sigma_\\varepsilon^2); and 3)X-spread (s_x)."
  },
  {
    "objectID": "VB_RegRev.html#sampling-distribution-of-b_0",
    "href": "VB_RegRev.html#sampling-distribution-of-b_0",
    "title": "VectorByte Methods Training: Regression Review",
    "section": "Sampling Distribution of b_0",
    "text": "Sampling Distribution of b_0\nThe intercept is also normal and unbiased: b_0 \\sim \\mathcal{N}(\\beta_0, \\sigma^2_{b_0}), where \n\\sigma^2_{b_0} = \\text{var}(b_0)  = \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{(n-1)\n    s_x^2} \\right).\n\nWhat is the intuition here? \n\\text{var}(\\bar{Y} - \\bar{X} b_1)\n= \\text{var}(\\bar{Y}) + \\bar{X}^2\\text{var}(b_1) {~-~ 2\\mathrm{cov}(\\bar{Y},b_1) }\n\n\n\\bar{Y} and b_1 are uncorrelated because the slope (b_1) is invariant if you shift the data up or down (\\bar{Y}).\n\n\nOptional Practice Exercise\n\nShow that:\n\n\\mathbb{E}[b_1] = \\beta_1\n\\mathbb{E}[b_0] = \\beta_0\n\\text{var}(b_0) = \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{(n-1)  s_x^2} \\right)\n\nWhy is it that b_0 and b_1 are normally distributed?"
  },
  {
    "objectID": "VB_RegRev.html#joint-distribution-of-b_0-and-b_1",
    "href": "VB_RegRev.html#joint-distribution-of-b_0-and-b_1",
    "title": "VectorByte Methods Training: Regression Review",
    "section": "Joint Distribution of b_0 and b_1",
    "text": "Joint Distribution of b_0 and b_1\nWe know that b_0 and b_1 can be dependent, i.e., \n\\mathbb{E}[(b_0 -\\beta_0)(b_1 - \\beta_1)] \\ne 0.\n This means that estimation error in the slope is correlated with the estimation error in the intercept. \n\\mathrm{cov}(b_0,b_1) = -\\sigma^2 \\left(\\frac{\\bar{X}}{(n-1)s_x^2}\\right).\n\n\nInterpretation:\n\nUsually, if the slope estimate is too high, the intercept estimate is too low (negative correlation).\nThe correlation decreases with more X spread (s^2_x)."
  },
  {
    "objectID": "VB_RegRev.html#estimated-variance",
    "href": "VB_RegRev.html#estimated-variance",
    "title": "VectorByte Methods Training: Regression Review",
    "section": "Estimated variance",
    "text": "Estimated variance\nHowever, these formulas aren’t especially practical since they involve an unknown quantity: \\sigma.\n\nSolution: use s, the residual sample standard deviation estimator for \\sigma = \\sigma_\\varepsilon. \ns_{b_1} = \\sqrt{\\frac{s^2}{(n-1)s_x^2}} ~~~\ns_{b_0} = \\sqrt{s^2 \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{(n-1)s^2_x}\\right)}\n\ns_{b_1} = \\hat{\\sigma~}_{b_1} and s_{b_0} = \\hat{\\sigma~}_{b_0} are estimated coefficient sd’s.\n\n\nInterpretation:\n\nWe now have a notion of standard error for the LS estimates of the slope and intercept.\n\n\nSmall s_b values mean high info/precision/accuracy."
  },
  {
    "objectID": "VB_RegRev.html#normal-and-student-t",
    "href": "VB_RegRev.html#normal-and-student-t",
    "title": "VectorByte Methods Training: Regression Review",
    "section": "Normal and Student-t",
    "text": "Normal and Student-t\nAgain, recall what Student discovered:\nIf \\theta \\sim \\mathcal{N}(\\mu,\\sigma^2), but you estimate \\sigma^2 \\approx s^2 based on n-p degrees of freedom, then \\theta \\sim t_{n-p}(\\mu, s^2).\n\nFor SLR, for example:\n\n\\bar{Y} \\sim t_{n-1}(\\mu, s_y^2/n).\nb_0 \\sim t_{n-2}\\left(\\beta_0, s^2_{b_0}\\right) and b_1 \\sim t_{n-2}\\left(\\beta_1, s^2_{b_1}\\right)\n\n\nWe can use these distributions for drawing conclusions about the parameters via:\n\nConfidence intervals\nHypothesis tests"
  },
  {
    "objectID": "Stats_review.html",
    "href": "Stats_review.html",
    "title": "VectorByte Methods Training: Probability and Statistics Fundamentals",
    "section": "",
    "text": "Main materials\nSolutions to exercises"
  },
  {
    "objectID": "Stats_review.html#some-probability-notation",
    "href": "Stats_review.html#some-probability-notation",
    "title": "VectorByte Methods Training: Probability and Statistics Fundamentals",
    "section": "Some probability notation",
    "text": "Some probability notation\nWe have a set, S of all possible events. Let \\text{Pr}(A) (or alternatively \\text{Prob}(A)) be the probability of event A. Then:\n\nA^c is the complement to A (all events that are not A).\nA \\cup B is the union of events A and B (“A or B”).\nA \\cap B is the intersection of events A and B (“A and B”).\n\\text{Pr}(A|B) is the conditional probability of A given that B occurs."
  },
  {
    "objectID": "Stats_review.html#axioms-of-probability",
    "href": "Stats_review.html#axioms-of-probability",
    "title": "VectorByte Methods Training: Probability and Statistics Fundamentals",
    "section": "Axioms of Probability",
    "text": "Axioms of Probability\nThese are the basic definitions that we use when we talk about probabilities. You’ve probably seen these before, but maybe not in mathematical notation. If the notation is new to you, I suggest that you use the notation above to translate these statements into words and confirm that you understand what they mean. I give you an example for the first statement.\n\n\\sum_{i \\in S} \\text{Pr}(A_i)=1, where 0 \\leq \\text{Pr}(A_i) \\leq 1 (the probabilities of all the events that can happen must sum to one, and all of the individual probabilities must be less than one)\n\\text{Pr}(A)=1-\\text{Pr}(A^c)\n\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B) -\\text{Pr}(A \\cap B)\n\\text{Pr}(A \\cap B) = \\text{Pr}(A|B)\\text{Pr}(B)\nIf A and B are independent, then \\text{Pr}(A|B) = \\text{Pr}(A)"
  },
  {
    "objectID": "Stats_review.html#bayes-theorem",
    "href": "Stats_review.html#bayes-theorem",
    "title": "VectorByte Methods Training: Probability and Statistics Fundamentals",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nBayes Theorem allows us to related the conditional probabilities of two events A and B:\n\\begin{align*}\n\\text{Pr}(A|B) & = \\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B)}\\\\\n&\\\\\n& =  \\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B|A)\\text{Pr}(A) + \\text{Pr}(B|A^c)\\text{Pr}(A^c)}\n\\end{align*}"
  },
  {
    "objectID": "Stats_review.html#discrete-rvs-and-their-probability-distributions",
    "href": "Stats_review.html#discrete-rvs-and-their-probability-distributions",
    "title": "VectorByte Methods Training: Probability and Statistics Fundamentals",
    "section": "Discrete RVs and their Probability Distributions",
    "text": "Discrete RVs and their Probability Distributions\nMany things that we observe are naturally discrete. For instance, whole numbers of chairs or win/loss outcomes for games. Discrete probability distributions are used to describe these kinds of events.\nFor discrete RVs, the distribution of probabilities is described by the probability mass function (pmf), f_k such that:\n\\begin{align*}\nf_k  \\equiv \\text{Pr}(X & = k) \\\\\n\\text{where } 0\\leq f_k \\leq 1 & \\text{ and } \\sum_k f_k = 1\n\\end{align*}\nFor example, for a fair 6-sided die:\nf_k = 1/6 for k= \\{1,2,3,4,5,6\\}.\n\\star Question 1: For the six-sided fair die, what is f_k if k=7? k=1.5?\nRelated to the pmf is the cumulative distribution function (cdf), F(x). F(x) \\equiv \\text{Pr}(X \\leq x)\nFor the 6-sided die F(x)= \\displaystyle\\sum_{k=1}^{x} f_k\nwhere x \\in 1\\dots 6.\n\\star Question 2: For the fair 6-sided die, what is F(3)? F(7)? F(1.5)?\n\nVisualizing distributions of discrete RVs in R\nExample: Imagine a RV can take values 1 through 10, each with probability 0.1:\n \n\nvals&lt;-seq(1,10, by=1)\npmf&lt;-rep(0.1, 10)\ncdf&lt;-pmf[1]\nfor(i in 2:10) cdf&lt;-c(cdf, cdf[i-1]+pmf[i])\npar(mfrow=c(1,2), bty=\"n\")\nbarplot(height=pmf, names.arg=vals, ylim=c(0, 1), main=\"pmf\", col=\"blue\")\nbarplot(height=cdf, names.arg=vals, ylim=c(0, 1), main=\"cdf\", col=\"red\")"
  },
  {
    "objectID": "Stats_review.html#continuous-rvs-and-their-probability-distributions",
    "href": "Stats_review.html#continuous-rvs-and-their-probability-distributions",
    "title": "VectorByte Methods Training: Probability and Statistics Fundamentals",
    "section": "Continuous RVs and their Probability Distributions",
    "text": "Continuous RVs and their Probability Distributions\nThings are just a little different for continuous RVs. Instead we use the probability density function (pdf) of the RV, and denote it by f(x). It still describes how relatively likely are alternative values of an RV – that is, if the pdf his higher around one value than around another, then the first is more likely to happen. However, the pdf does not return a probability, it is a function that describes the probability density.\nAn analogy:\nProbabilities are like weights of objects. The PMF tells you how much weight each possible value or outcome contributes to a whole. The PDF tells you how dense it is around a value. To calculate the weight of a real object, you need to also know the size of the area that you’re interested in and the density there The probability that your RV takes exactly any value is zero, just like the probability that any atom in a very thin wire is lined up at exactly that position is zero (and to the amount of mass at that location is zero). However, you can take a very thin slice around that location to see how much material is there.\nRelated to the pdf is the cumulative distribution function (cdf), F(x). \nF(x) \\equiv \\text{Pr}(X \\leq x)\n For a continuous distribution: \nF(x)= \\int_{-\\infty}^x f(x')dx'\n\n \n For a normal distribution with mean 0, what is F(0)?\n \n\nVisualizing distributions of continuous RVs in R\nExample: exponential RV, where f(x) = re^{-rx}:\n\n\nvals&lt;-seq(0,10, length=1000)\nr&lt;-0.5\npar(mfrow=c(1,2), bty=\"n\")\nplot(vals, dexp(vals, rate=r), main=\"pdf\", col=\"blue\", type=\"l\", lwd=3, ylab=\"\", xlab=\"\")\nplot(vals, pexp(vals, rate=r), main=\"cdf\", ylim=c(0,1), col=\"red\",\n     type=\"l\", lwd=3, ylab=\"\", xlab=\"\")"
  },
  {
    "objectID": "Stats_review.html#confidence-intervals",
    "href": "Stats_review.html#confidence-intervals",
    "title": "VectorByte Methods Training: Probability and Statistics Fundamentals",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nSuppose Z_{n-p} \\sim t_{n-p}(0,1). A centered interval is on this t distribution can be written as: \\text{Pr}(-t_{n-p,\\alpha/2} \\&lt; Z\\_{n-p} \\&lt; t_{n-p,\\alpha/2}) = 1-\\alpha. That is, between these values of the t distribution (1-\\alpha)\\times 100 percent of the probability is contained in that symmetric interval. We can visually indicate these location on a plot of the t distribution (here with df=5 and \\alpha=0.05):\n\nx&lt;-seq(-4.5, 4.5, length=1000)\nalpha=0.05\n\n## draw a line showing the normal pdf on the histogram\nplot(x, dt(x, df=5), col=\"black\", lwd=2, type=\"l\", xlab=\"x\", ylab=\"\")\nabline(v=qt(alpha/2, df=5), col=3, lty=2, lwd=2)\nabline(v=qt(1-alpha/2, df=5), col=2, lty=2, lwd=2)\n\nlegend(\"topright\", \n       legend=c(\"t, df=5\", \"lower a/2\", \"upper a/2\"),\n       col=c(1,3,2), lwd=2, lty=c(1, 2,2))\n\n\n\n\n\n\n\n\nIn the R code here, {\\tt qt} is the Student-t “quantile function”. The function {\\tt qt(alpha, df)} returns a value z such that \\alpha = P(Z_{\\mathrm{df}} &lt; z), i.e., t_{\\mathrm{df},\\alpha}.\nHow can we use this to determine the confidence interval for \\theta? Since \\theta \\sim t_{n-p}(\\mu, s^2), we can replace the Z_{n-p} in the interval above with the definition in terms of \\theta, \\mu and s and rearrange: \\begin{align*}\n1-\\alpha& = \\text{Pr}\\left(-t_{n-p,\\alpha/2} &lt; \\frac{\\mu - \\bar{\\theta}}{s} &lt;\nt_{n-p,\\alpha/2}\\right) \\\\\n&=\n\\text{Pr}(\\bar{\\theta}-t_{n-p,\\alpha/2}s &lt; \\mu &lt;\n\\bar{\\theta} + t_{n-p,\\alpha/2}s)\n\\end{align*}\nThus (1-\\alpha)*100% of the time, \\mu is within the confidence interval (written in two equivalent ways):\n\\bar{\\theta} \\pm t_{n-p,\\alpha/2} \\times s \\;\\;\\; \\Leftrightarrow \\;\\;\\; \\bar{\\theta}-t_{n-p,\\alpha/2} \\times s, \\bar{\\theta} + t_{n-p,\\alpha/2}\\times s\nWhy should we care about confidence intervals?\n\nThe confidence interval captures the amount of information in the data about the parameter.\nThe center of the interval tells you what your estimate is.\nThe length of the interval tells you how sure you are about your estimate."
  },
  {
    "objectID": "Stats_review.html#p-values",
    "href": "Stats_review.html#p-values",
    "title": "VectorByte Methods Training: Probability and Statistics Fundamentals",
    "section": "p-Values",
    "text": "p-Values\nWhat is a p-value? The American Statistical Association issued a statement where they defined it in the following way:\n“Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.” (ASA Statement on Statistical Significance and P-Values.)\nMore formally, we formulate a p-value in terms of a null hypothesis/model and test whether or not our observed data are more extreme than we would expect under that specific null model. In your previous courses you’ve probably seen very specific null models, corresponding to, for instance the null hypothesis that the mean of your data is normally distributed with mean m (often m=0). We often denote the null model as H_0 and the alternative as H_a or H_1. For instance, for our example above with \\theta we might want to test the following:\nH_0: \\bar{\\theta}=0 \\;\\;\\; \\text{vs.} \\;\\;\\; H_a: \\bar{\\theta}\\neq 0\nTo perform the hypothesis test we would FIRST choose our rejection level, \\alpha. Although convention is to use \\alpha =0.05 corresponding to a 95% confidence region, one could choose based on how sure one needs to be for a particular application. Next we build our test statistic. There are two cases, first if we know \\sigma and second if we don’t.\nIf we knew the variance \\sigma^2, our test statistic would be Z=\\frac{\\bar{\\theta}-0}{\\sigma}, and we expect that this should have a standard normal distribution, i.e., Z\\sim\\mathcal{N}(0,1). If we don’t know \\sigma and instead estimate is as s (which is most of the time), our test statistic would be Z_{df}=\\frac{\\bar{\\theta}-0}{s} (i.e., it would have a t-distribution).\nWe calculate the value of the appropriate statistic (either Z or Z_{df}) for our data, and then we compare it to the values of the standard distribution (normal or t, respectively) corresponding to the \\alpha level that we chose, i.e., we see if the number that we got for our statistic is inside the horizontal lines that we drew on the standard distribution above. If it is, then the data are consistent with the null hypothesis and we cannot reject the null. If the statistic is outside the region the data are NOT consistent with the null, and instead we reject the null and use the alternative as our new working hypothesis.\nNotice that this process is focused on the null hypothesis. We cannot tell if the alternative hypothesis is true, or, really, if it’s actually better than the null. We can only say that the null is not consistent with our data (i.e., we can falsify the null) at a given level of certainty.\nAlso, the hypothesis testing process is the same as building a confidence interval, as above, and then seeing if the null hypothesis is within your confidence interval. If the null is outside of your confidence interval then you can reject your null at the level of certainty corresponding to the \\alpha that you used to build your CI. If the value for the null is within your CI, you cannot reject at that level."
  },
  {
    "objectID": "Stats_review.html#the-sampling-distribution-1",
    "href": "Stats_review.html#the-sampling-distribution-1",
    "title": "VectorByte Methods Training: Probability and Statistics Fundamentals",
    "section": "The Sampling Distribution",
    "text": "The Sampling Distribution\nSuppose we have a random sample \\{Y_i, i=1,\\dots,N \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,9) for i=1,\\ldots,N.\n\nWhat is the variance of the sample mean?\nWhat is the expectation of the sample mean?\nWhat is the variance for another i.i.d. realization Y_{ N+1}?\nWhat is the standard error of \\bar{Y}?"
  },
  {
    "objectID": "Stats_review.html#hypothesis-testing-and-confidence-intervals",
    "href": "Stats_review.html#hypothesis-testing-and-confidence-intervals",
    "title": "VectorByte Methods Training: Probability and Statistics Fundamentals",
    "section": "Hypothesis Testing and Confidence Intervals",
    "text": "Hypothesis Testing and Confidence Intervals\nSuppose we sample some data \\{Y_i, i=1,\\dots,n \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,\\sigma^2) for i=1,\\ldots,n, and that you want to test the null hypothesis H_0: ~\\mu=12 vs. the alternative H_a: \\mu \\neq 12, at the 0.05 significance level.\n\nWhat test statistic would you use? How do you estimate \\sigma?\nWhat is the distribution for this test statistic if the null is true?\nWhat is the distribution for the test statistic if the null is true and n \\rightarrow \\infty?\nDefine the test rejection region. (I.e., for what values of the test statistic would you reject the null?)\nHow would compute the p-value associated with a particular sample?\nWhat is the 95% confidence interval for \\mu? How should one interpret this interval?\nIf \\bar{Y} = 11, s_y = 1, and n=9, what is the test result? What is the 95% CI for \\mu?"
  },
  {
    "objectID": "VB_TimeSeriesForecastingPractical.html#series-decomposition-by-loess-arima-or-exponential-smoothing",
    "href": "VB_TimeSeriesForecastingPractical.html#series-decomposition-by-loess-arima-or-exponential-smoothing",
    "title": "VectorByte Methods Training: Introduction to Time Series Forecasting (practical)",
    "section": "Series Decomposition by Loess + ARIMA or Exponential Smoothing",
    "text": "Series Decomposition by Loess + ARIMA or Exponential Smoothing\n\nSlide 22-27 & 40-43\nIn this approach the time series is first de-trended and de-seasonalized using loess smoothing. Then the remaining stationary series is modeled using ARIMA (or another method if we select a different one). The ARIMA part is fitted via an automated process in the background so we do not have to manually specify p, d, and q. For more information about this process, see the documentation for the auto.arima() function.\nIf you want to try out exponential smoothing, you will use method = \"ETS\" and you can specify different model types using etsmodel = \"ANN\" for example. Other potential models can be found in the lecture slides (e.g. “MNN”, “AAA”, etc.). Note that because we have de-trended and de-seasonalized the series, we do not really need the latter two letters to change from “N”.\nFirst, we have to turn our training data into a time series object like we did with the whole series for plotting.\n\nlibrary(forecast)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# STL requires a UNIVARIATE time series object\nts.train &lt;- ts(train$observation, frequency = 365)\n\nBelow you can see the output for the STL + ARIMA model. The ETS version is commented out here, but you can try it out on your machine.\n\n## There are several options that we can customize in the stl() function, but the one we have to specify is the s.window. Check out the documentation and play around with some of the other options if you would like.\nstl.fit &lt;- stlm(ts.train, s.window = \"periodic\", \n                method = \"arima\")\n\n##stl.fit &lt;- stlm(ts.train, s.window = \"periodic\", \n##                method = \"ets\",\n##                etsmodel = \"ANN\")\n\nsummary(stl.fit$model)\n\nSeries: x \nARIMA(0,1,3) \n\nCoefficients:\n          ma1      ma2     ma3\n      -0.3263  -0.1179  0.0410\ns.e.   0.0257   0.0259  0.0263\n\nsigma^2 = 3.873e-05:  log likelihood = 5590.98\nAIC=-11173.95   AICc=-11173.92   BIC=-11152.63\n\nTraining set error measures:\n                       ME        RMSE         MAE         MPE     MAPE\nTraining set 1.262983e-05 0.006215367 0.004032053 -0.01602335 1.078008\n                  MASE        ACF1\nTraining set 0.3462004 0.001065878\n\n\nLet’s look at the residuals to see if there is anything left unaccounted for.\n\n## Check for leftover autocorrelation in the residuals\nacf(stl.fit$residuals)\n\n\n\n\n\n\n\n## These models still do assume normal residuals - these look good!\nhist(stl.fit$residuals)\n\n\n\n\n\n\n\n\nNow that we are comfortable with the model set up, we can generate forecasts.\n\n## We can generate forecasts with the forecast() function in the forecast package\n\nstl.forecasts &lt;- forecast(stl.fit, h = 30)\n\n## The forecast function gives us point forecasts, as well as prediction intervals\nstl.forecasts\n\n         Point Forecast     Lo 80     Hi 80     Lo 95     Hi 95\n5.186301      0.4472946 0.4393188 0.4552704 0.4350967 0.4594925\n5.189041      0.4531159 0.4434988 0.4627330 0.4384079 0.4678240\n5.191781      0.4476411 0.4370512 0.4582309 0.4314453 0.4638368\n5.194521      0.4406500 0.4290394 0.4522606 0.4228931 0.4584069\n5.197260      0.4452514 0.4327028 0.4578000 0.4260600 0.4644428\n5.200000      0.4423128 0.4288916 0.4557340 0.4217868 0.4628388\n5.202740      0.4429717 0.4287313 0.4572122 0.4211928 0.4647506\n5.205479      0.4375731 0.4225581 0.4525882 0.4146096 0.4605367\n5.208219      0.4404230 0.4246714 0.4561746 0.4163330 0.4645130\n5.210959      0.4468553 0.4304001 0.4633106 0.4216892 0.4720214\n5.213699      0.4412252 0.4240952 0.4583552 0.4150272 0.4674232\n5.216438      0.4444451 0.4266659 0.4622242 0.4172542 0.4716359\n5.219178      0.4360549 0.4176495 0.4544603 0.4079063 0.4642035\n5.221918      0.4397223 0.4207112 0.4587333 0.4106474 0.4687972\n5.224658      0.4513696 0.4317716 0.4709676 0.4213971 0.4813422\n5.227397      0.4438545 0.4236866 0.4640224 0.4130104 0.4746986\n5.230137      0.4433468 0.4226248 0.4640689 0.4116552 0.4750385\n5.232877      0.4429392 0.4216774 0.4642011 0.4104220 0.4754564\n5.235616      0.4404891 0.4187008 0.4622773 0.4071668 0.4738113\n5.238356      0.4491964 0.4268942 0.4714986 0.4150881 0.4833047\n5.241096      0.4469288 0.4241242 0.4697334 0.4120521 0.4818054\n5.243836      0.4396761 0.4163800 0.4629723 0.4040477 0.4753046\n5.246575      0.4386110 0.4148334 0.4623886 0.4022463 0.4749757\n5.249315      0.4388684 0.4146189 0.4631178 0.4017821 0.4759547\n5.252055      0.4363757 0.4116635 0.4610880 0.3985816 0.4741699\n5.254795      0.4401331 0.4149665 0.4652997 0.4016441 0.4786221\n5.257534      0.4372529 0.4116401 0.4628658 0.3980814 0.4764244\n5.260274      0.4355078 0.4094563 0.4615593 0.3956655 0.4753501\n5.263014      0.4369713 0.4104885 0.4634542 0.3964693 0.4774733\n5.265753      0.4391473 0.4122400 0.4660546 0.3979962 0.4802985\n\n## Notice that it is not straightforward to get the point forecasts out - we have to convert it to a data frame first.\nstr(stl.forecasts)\n\nList of 10\n $ method   : chr \"STL +  ARIMA(0,1,3)\"\n $ model    :List of 18\n  ..$ coef     : Named num [1:3] -0.326 -0.118 0.041\n  .. ..- attr(*, \"names\")= chr [1:3] \"ma1\" \"ma2\" \"ma3\"\n  ..$ sigma2   : num 3.87e-05\n  ..$ var.coef : num [1:3, 1:3] 0.000661 -0.000198 -0.000116 -0.000198 0.00067 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:3] \"ma1\" \"ma2\" \"ma3\"\n  .. .. ..$ : chr [1:3] \"ma1\" \"ma2\" \"ma3\"\n  ..$ mask     : logi [1:3] TRUE TRUE TRUE\n  ..$ loglik   : num 5591\n  ..$ aic      : num -11174\n  ..$ arma     : int [1:7] 0 3 0 0 365 1 0\n  ..$ residuals: Time-Series [1:1528] from 1 to 5.18: 3.66e-04 -7.76e-05 1.23e-03 4.86e-04 -2.20e-04 ...\n  ..$ call     : language auto.arima(y = x, seasonal = FALSE, xreg = xreg, x = list(x = c(0.365520220303196,  0.36543793829309, 0.366717656| __truncated__ ...\n  ..$ series   : chr \"x\"\n  ..$ code     : int 0\n  ..$ n.cond   : int 0\n  ..$ nobs     : int 1527\n  ..$ model    :List of 10\n  .. ..$ phi  : num(0) \n  .. ..$ theta: num [1:3] -0.326 -0.118 0.041\n  .. ..$ Delta: num 1\n  .. ..$ Z    : num [1:5] 1 0 0 0 1\n  .. ..$ a    : num [1:5] 4.80e-03 2.00e-03 -9.60e-04 9.87e-05 3.71e-01\n  .. ..$ P    : num [1:5, 1:5] 0.00 0.00 0.00 0.00 1.86e-21 ...\n  .. ..$ T    : num [1:5, 1:5] 0 0 0 0 1 1 0 0 0 0 ...\n  .. ..$ V    : num [1:5, 1:5] 1 -0.326 -0.118 0.041 0 ...\n  .. ..$ h    : num 0\n  .. ..$ Pn   : num [1:5, 1:5] 1.00 -3.26e-01 -1.18e-01 4.10e-02 2.67e-22 ...\n  ..$ bic      : num -11153\n  ..$ aicc     : num -11174\n  ..$ x        : Time-Series [1:1528] from 1 to 5.18: 0.366 0.365 0.367 0.367 0.366 ...\n  ..$ fitted   : Time-Series [1:1528] from 1 to 5.18: 0.365 0.366 0.365 0.366 0.367 ...\n  ..- attr(*, \"class\")= chr [1:3] \"forecast_ARIMA\" \"ARIMA\" \"Arima\"\n $ level    : num [1:2] 80 95\n $ mean     : Time-Series [1:30] from 5.19 to 5.27: 0.447 0.453 0.448 0.441 0.445 ...\n $ lower    : Time-Series [1:30, 1:2] from 5.19 to 5.27: 0.439 0.443 0.437 0.429 0.433 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"80%\" \"95%\"\n $ upper    : Time-Series [1:30, 1:2] from 5.19 to 5.27: 0.455 0.463 0.458 0.452 0.458 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"80%\" \"95%\"\n $ x        : Time-Series [1:1528] from 1 to 5.18: 0.328 0.328 0.329 0.329 0.328 ...\n $ series   : chr \"ts.train\"\n $ fitted   : Time-Series [1:1528] from 1 to 5.18: 0.328 0.328 0.327 0.328 0.328 ...\n $ residuals: Time-Series [1:1528] from 1 to 5.18: 3.66e-04 -7.76e-05 1.23e-03 4.86e-04 -2.20e-04 ...\n - attr(*, \"class\")= chr \"forecast\"\n\nstl.df &lt;- as.data.frame(stl.forecasts)\n\nLet’s visualize the forecasts! For the basic models available in the forecast package, we can use the plot() function to see a quick time series plot.\n\n# Let's look at the forecasts!\nplot(stl.forecasts)\n\n\n\n\n\n\n\n\nLet’s compare them to the observed test series values.\n\n# First make a data frame with both in there\ncompare &lt;- data.frame(time = seq(1:30),\n                      observed = test$observation,\n                      forecast = stl.df$`Point Forecast`)\n\n# What do you think??\nggplot(data = compare, aes(x = time, y = observed))+\n  geom_line(color = \"blue\")+\n  geom_point(color = \"blue\")+\n  geom_line(aes(y = forecast), color = \"red\")+\n  geom_point(aes(y = forecast), color = \"red\")\n\n\n\n\n\n\n\n\nFinally, we can generate some accuracy metrics and save them in a data frame for later use.\n\n# Last - let's get the RMSE and MAE out. We can use the Metrics package for this. While we're at it, let's save them to a data set for later.\n\nlibrary(Metrics)\n\n\nAttaching package: 'Metrics'\n\n\nThe following object is masked from 'package:forecast':\n\n    accuracy\n\nstl.rmse &lt;- rmse(compare$observed, compare$forecast)\nstl.mae &lt;- mae(compare$observed, compare$forecast)\n\n(comparisonDF &lt;- data.frame(model = \"stl\",\n                           RMSE = stl.rmse,\n                           MAE = stl.mae))\n\n  model       RMSE         MAE\n1   stl 0.01162321 0.009048619\n\n\nAn additional argument in stlm() exists that allows for the inclusion of extra predictors into an ARIMA-based model post-decomposition. This function requires the predictors to exist in a separate matrix rather than inside of a data frame, which makes it somewhat challenging to use if you are unfamiliar with using matrices in R."
  },
  {
    "objectID": "VB_TimeSeriesForecastingPractical.html#linear-regression",
    "href": "VB_TimeSeriesForecastingPractical.html#linear-regression",
    "title": "VectorByte Methods Training: Introduction to Time Series Forecasting (practical)",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nSlide 29-32\nFortunately, there are several kinds of time series models that accommodate extra predictors in a more intuitive way. The most obvious starting place is linear regression! We have a lot of predictors in our data set to choose from. Feel free to play around with them! For time’s sake, I am going to use mean temperature and its one day lag only. This part is probably not too different from what you are used to doing for non-time series data. The only major difference is in what predictors we choose to include to account for autocorrelation, trend, and seasonality.\n\n# We will start with just our predictors\nlm.fit &lt;- lm(observation ~ meanTemp + meanTempLag1, \n             data = train)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = observation ~ meanTemp + meanTempLag1, data = train)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.108835 -0.017111  0.001229  0.016760  0.090118 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.2626627  0.0022072 119.002   &lt;2e-16 ***\nmeanTemp     0.0011590  0.0001145  10.124   &lt;2e-16 ***\nmeanTempLag1 0.0011180  0.0001145   9.764   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02741 on 1525 degrees of freedom\nMultiple R-squared:  0.6546,    Adjusted R-squared:  0.6541 \nF-statistic:  1445 on 2 and 1525 DF,  p-value: &lt; 2.2e-16\n\nacf(lm.fit$residuals, 365) # Still a lot of seasonality left\n\n\n\n\n\n\n\n\nLet’s add the sine and cosine terms we discussed into the data set to include them in the model. We will need to do this for train and test. Note that if you were to be forecasting beyond test, you would need to generate these terms for those time points as well.\n\n# Add to the training set\ntrain$sinSeason &lt;- sin((2*pi*train$X)/365)\ntrain$cosSeason &lt;- cos((2*pi*train$X)/365)\n\n# Add to the testing set\ntest$sinSeason &lt;- sin((2*pi*test$X)/365)\ntest$cosSeason &lt;- cos((2*pi*test$X)/365)\n\nBack to business! Let’s add in the sine and cosine terms for seasonality.\n\n# Add seasonality via sine and cosine terms\nlm.fit &lt;- lm(observation ~ sinSeason + cosSeason + meanTemp +\n               meanTempLag1, \n             data = train)\n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = observation ~ sinSeason + cosSeason + meanTemp + \n    meanTempLag1, data = train)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.081050 -0.013551  0.003378  0.016710  0.072831 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.3531644  0.0049347  71.568   &lt;2e-16 ***\nsinSeason     0.0447465  0.0022696  19.715   &lt;2e-16 ***\ncosSeason    -0.0154489  0.0011013 -14.027   &lt;2e-16 ***\nmeanTemp      0.0002431  0.0001119   2.172   0.0300 *  \nmeanTempLag1  0.0001918  0.0001119   1.714   0.0868 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0244 on 1523 degrees of freedom\nMultiple R-squared:  0.7266,    Adjusted R-squared:  0.7259 \nF-statistic:  1012 on 4 and 1523 DF,  p-value: &lt; 2.2e-16\n\nacf(lm.fit$residuals, 365)\n\n\n\n\n\n\n\n\nFinally, we can add in the trend.\n\n# Add trend\nlm.fit &lt;- lm(observation ~ X + sinSeason + cosSeason + meanTemp +\n               meanTempLag1, \n             data = train)\n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = observation ~ X + sinSeason + cosSeason + meanTemp + \n    meanTempLag1, data = train)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.082566 -0.013092  0.003663  0.016387  0.063609 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.465e-01  4.895e-03  70.789  &lt; 2e-16 ***\nX             1.164e-05  1.403e-06   8.297 2.33e-16 ***\nsinSeason     4.662e-02  2.232e-03  20.884  &lt; 2e-16 ***\ncosSeason    -1.637e-02  1.083e-03 -15.112  &lt; 2e-16 ***\nmeanTemp      2.202e-04  1.096e-04   2.010   0.0446 *  \nmeanTempLag1  1.694e-04  1.096e-04   1.546   0.1222    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02388 on 1522 degrees of freedom\nMultiple R-squared:  0.7384,    Adjusted R-squared:  0.7376 \nF-statistic: 859.3 on 5 and 1522 DF,  p-value: &lt; 2.2e-16\n\n# Check for leftover autocorrelation in the residuals - still a lot.\n\n#   Stay tuned for how to deal with this! For now we will move forward.\nacf(lm.fit$residuals)\n\n\n\n\n\n\n\n# These models still do assume normal residuals - these look good!\nhist(lm.fit$residuals)\n\n\n\n\n\n\n\n\nNext we’ll generate forecasts for this model.\n\n# We can generate forecasts with the forecast() function in the forecast package\nlm.forecasts &lt;- forecast(lm.fit, h = 30, newdata = test)\n\n# The forecast function gives us point forecasts, as well as prediction intervals\nlm.forecasts\n\n   Point Forecast     Lo 80     Hi 80     Lo 95     Hi 95\n1       0.4265763 0.3958960 0.4572565 0.3796379 0.4735146\n2       0.4286415 0.3979637 0.4593193 0.3817068 0.4755761\n3       0.4298292 0.3991522 0.4605061 0.3828959 0.4767625\n4       0.4295394 0.3988597 0.4602191 0.3826018 0.4764769\n5       0.4285087 0.3978253 0.4591920 0.3815655 0.4754518\n6       0.4300729 0.3993812 0.4607646 0.3831171 0.4770288\n7       0.4337247 0.4030231 0.4644262 0.3867538 0.4806956\n8       0.4348493 0.4041528 0.4655458 0.3878861 0.4818125\n9       0.4350253 0.4043367 0.4657138 0.3880742 0.4819763\n10      0.4348475 0.4041593 0.4655356 0.3878971 0.4817979\n11      0.4333377 0.4026513 0.4640242 0.3863899 0.4802856\n12      0.4336375 0.4029566 0.4643184 0.3866982 0.4805769\n13      0.4348544 0.4041768 0.4655320 0.3879201 0.4817887\n14      0.4359595 0.4052789 0.4666402 0.3890205 0.4828985\n15      0.4376019 0.4069138 0.4682900 0.3906515 0.4845523\n16      0.4370947 0.4064009 0.4677886 0.3901356 0.4840539\n17      0.4350371 0.4043475 0.4657267 0.3880844 0.4819898\n18      0.4350350 0.4043494 0.4657207 0.3880884 0.4819817\n19      0.4377471 0.4070499 0.4684442 0.3907829 0.4847112\n20      0.4379329 0.4072393 0.4686266 0.3909741 0.4848917\n21      0.4371022 0.4064218 0.4677826 0.3901636 0.4840408\n22      0.4379258 0.4072451 0.4686064 0.3909868 0.4848647\n23      0.4389425 0.4082622 0.4696228 0.3920040 0.4858809\n24      0.4403174 0.4096319 0.4710030 0.3933710 0.4872639\n25      0.4414457 0.4107581 0.4721334 0.3944961 0.4883954\n26      0.4413800 0.4106927 0.4720672 0.3944309 0.4883290\n27      0.4412094 0.4105261 0.4718927 0.3942664 0.4881524\n28      0.4416591 0.4109746 0.4723435 0.3947143 0.4886039\n29      0.4422830 0.4115963 0.4729697 0.3953348 0.4892312\n30      0.4427546 0.4120662 0.4734429 0.3958038 0.4897053\n\n# Notice that it is not straightforward to get the point forecasts out - we have to convert it to a data frame first.\nstr(lm.forecasts)\n\nList of 11\n $ model    :List of 14\n  ..$ coefficients : Named num [1:6] 3.46e-01 1.16e-05 4.66e-02 -1.64e-02 2.20e-04 ...\n  .. ..- attr(*, \"names\")= chr [1:6] \"(Intercept)\" \"X\" \"sinSeason\" \"cosSeason\" ...\n  ..$ residuals    : Named num [1:1528] -0.0201 -0.0215 -0.0227 -0.022 -0.0225 ...\n  .. ..- attr(*, \"names\")= chr [1:1528] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ effects      : Named num [1:1528] -14.6653 -0.0223 1.471 0.525 0.0881 ...\n  .. ..- attr(*, \"names\")= chr [1:1528] \"(Intercept)\" \"X\" \"sinSeason\" \"cosSeason\" ...\n  ..$ rank         : int 6\n  ..$ fitted.values: Named num [1:1528] 0.348 0.349 0.351 0.351 0.35 ...\n  .. ..- attr(*, \"names\")= chr [1:1528] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ assign       : int [1:6] 0 1 2 3 4 5\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:1528, 1:6] -39.0896 0.0256 0.0256 0.0256 0.0256 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:1528] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. .. ..$ : chr [1:6] \"(Intercept)\" \"X\" \"sinSeason\" \"cosSeason\" ...\n  .. .. ..- attr(*, \"assign\")= int [1:6] 0 1 2 3 4 5\n  .. ..$ qraux: num [1:6] 1.03 1.04 1 1.04 1.01 ...\n  .. ..$ pivot: int [1:6] 1 2 3 4 5 6\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 6\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 1522\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = observation ~ X + sinSeason + cosSeason + meanTemp + meanTempLag1,      data = train)\n  ..$ terms        :Classes 'terms', 'formula'  language observation ~ X + sinSeason + cosSeason + meanTemp + meanTempLag1\n  .. .. ..- attr(*, \"variables\")= language list(observation, X, sinSeason, cosSeason, meanTemp, meanTempLag1)\n  .. .. ..- attr(*, \"factors\")= int [1:6, 1:5] 0 1 0 0 0 0 0 0 1 0 ...\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:6] \"observation\" \"X\" \"sinSeason\" \"cosSeason\" ...\n  .. .. .. .. ..$ : chr [1:5] \"X\" \"sinSeason\" \"cosSeason\" \"meanTemp\" ...\n  .. .. ..- attr(*, \"term.labels\")= chr [1:5] \"X\" \"sinSeason\" \"cosSeason\" \"meanTemp\" ...\n  .. .. ..- attr(*, \"order\")= int [1:5] 1 1 1 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(observation, X, sinSeason, cosSeason, meanTemp, meanTempLag1)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:6] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. .. ..- attr(*, \"names\")= chr [1:6] \"observation\" \"X\" \"sinSeason\" \"cosSeason\" ...\n  ..$ model        :'data.frame':   1528 obs. of  6 variables:\n  .. ..$ observation : num [1:1528] 0.328 0.328 0.329 0.329 0.328 ...\n  .. ..$ X           : int [1:1528] 1 2 3 4 5 6 7 8 9 10 ...\n  .. ..$ sinSeason   : num [1:1528] 0.0172 0.0344 0.0516 0.0688 0.086 ...\n  .. ..$ cosSeason   : num [1:1528] 1 0.999 0.999 0.998 0.996 ...\n  .. ..$ meanTemp    : num [1:1528] 41.1 48.5 47.6 42.4 40.5 ...\n  .. ..$ meanTempLag1: num [1:1528] 47.9 41.1 48.5 47.6 42.4 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language observation ~ X + sinSeason + cosSeason + meanTemp + meanTempLag1\n  .. .. .. ..- attr(*, \"variables\")= language list(observation, X, sinSeason, cosSeason, meanTemp, meanTempLag1)\n  .. .. .. ..- attr(*, \"factors\")= int [1:6, 1:5] 0 1 0 0 0 0 0 0 1 0 ...\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:6] \"observation\" \"X\" \"sinSeason\" \"cosSeason\" ...\n  .. .. .. .. .. ..$ : chr [1:5] \"X\" \"sinSeason\" \"cosSeason\" \"meanTemp\" ...\n  .. .. .. ..- attr(*, \"term.labels\")= chr [1:5] \"X\" \"sinSeason\" \"cosSeason\" \"meanTemp\" ...\n  .. .. .. ..- attr(*, \"order\")= int [1:5] 1 1 1 1 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(observation, X, sinSeason, cosSeason, meanTemp, meanTempLag1)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:6] \"numeric\" \"numeric\" \"numeric\" \"numeric\" ...\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:6] \"observation\" \"X\" \"sinSeason\" \"cosSeason\" ...\n  ..$ x            : num [1:1528] 0.328 0.328 0.329 0.329 0.328 ...\n  ..$ series       : chr \"observation\"\n  ..- attr(*, \"class\")= chr \"lm\"\n $ mean     : Named num [1:30] 0.427 0.429 0.43 0.43 0.429 ...\n  ..- attr(*, \"names\")= chr [1:30] \"1529\" \"1530\" \"1531\" \"1532\" ...\n $ lower    : num [1:30, 1:2] 0.396 0.398 0.399 0.399 0.398 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:30] \"1529\" \"1530\" \"1531\" \"1532\" ...\n  .. ..$ : NULL\n $ upper    : num [1:30, 1:2] 0.457 0.459 0.461 0.46 0.459 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:30] \"1529\" \"1530\" \"1531\" \"1532\" ...\n  .. ..$ : NULL\n $ level    : num [1:2] 80 95\n $ x        : num [1:1528] 0.328 0.328 0.329 0.329 0.328 ...\n $ series   : chr \"observation\"\n $ method   : chr \"Linear regression model\"\n $ newdata  :'data.frame':  30 obs. of  30 variables:\n  ..$ X.1                   : int [1:30] 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 ...\n  ..$ DATE                  : chr [1:30] \"2024-06-12\" \"2024-06-13\" \"2024-06-14\" \"2024-06-15\" ...\n  ..$ X                     : int [1:30] 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 ...\n  ..$ project_id            : chr [1:30] \"neon4cast\" \"neon4cast\" \"neon4cast\" \"neon4cast\" ...\n  ..$ site_id               : chr [1:30] \"HARV\" \"HARV\" \"HARV\" \"HARV\" ...\n  ..$ datetime              : chr [1:30] \"2024-06-12\" \"2024-06-13\" \"2024-06-14\" \"2024-06-15\" ...\n  ..$ duration              : chr [1:30] \"P1D\" \"P1D\" \"P1D\" \"P1D\" ...\n  ..$ variable              : chr [1:30] \"gcc_90\" \"gcc_90\" \"gcc_90\" \"gcc_90\" ...\n  ..$ observation           : num [1:30] 0.445 0.429 0.453 0.429 0.434 ...\n  ..$ update                : num [1:30] 0.445 0.429 0.453 0.429 0.434 ...\n  ..$ meanTemp              : num [1:30] 66.5 70 70.1 66.2 62.1 ...\n  ..$ minTemp               : int [1:30] 54 50 65 51 45 55 61 68 67 66 ...\n  ..$ maxTemp               : int [1:30] 77 84 77 75 73 81 91 92 91 90 ...\n  ..$ meanRH                : num [1:30] 68.5 59.7 82.3 56.5 58 ...\n  ..$ minRH                 : int [1:30] 40 35 73 29 26 51 45 52 54 48 ...\n  ..$ maxRH                 : int [1:30] 100 93 97 93 93 90 93 100 100 100 ...\n  ..$ totalPrecipitation    : num [1:30] 0 0 0.2 0 0 0 0 0.97 1.72 2.63 ...\n  ..$ meanWindSpeed         : num [1:30] 3.96 6.04 6.27 8.83 3.33 ...\n  ..$ maxWindSpeed          : int [1:30] 10 14 16 18 9 14 15 31 21 9 ...\n  ..$ meanTempLag1          : num [1:30] 62.2 66.5 70 70.1 66.2 ...\n  ..$ minTempLag1           : int [1:30] 50 54 50 65 51 45 55 61 68 67 ...\n  ..$ maxTempLag1           : int [1:30] 70 77 84 77 75 73 81 91 92 91 ...\n  ..$ meanRHLag1            : num [1:30] 80.2 68.5 59.7 82.3 56.5 ...\n  ..$ minRHLag1             : int [1:30] 57 40 35 73 29 26 51 45 52 54 ...\n  ..$ maxRHLag1             : int [1:30] 100 100 93 97 93 93 90 93 100 100 ...\n  ..$ totalPrecipitationLag1: num [1:30] 0 0 0 0.2 0 0 0 0 0.97 1.72 ...\n  ..$ meanWindSpeedLag1     : num [1:30] 2.79 3.96 6.04 6.27 8.83 ...\n  ..$ maxWindSpeedLag1      : int [1:30] 8 10 14 16 18 9 14 15 31 21 ...\n  ..$ sinSeason             : num [1:30] 0.928 0.934 0.94 0.946 0.951 ...\n  ..$ cosSeason             : num [1:30] 0.374 0.358 0.342 0.325 0.309 ...\n $ residuals: Named num [1:1528] -0.0201 -0.0215 -0.0227 -0.022 -0.0225 ...\n  ..- attr(*, \"names\")= chr [1:1528] \"1\" \"2\" \"3\" \"4\" ...\n $ fitted   : Named num [1:1528] 0.348 0.349 0.351 0.351 0.35 ...\n  ..- attr(*, \"names\")= chr [1:1528] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"class\")= chr \"forecast\"\n\nlm.df &lt;- as.data.frame(lm.forecasts)\n\nWe can visualize the forecasts against the observed values if we add them to our test data set.\n\n# Add the forecasts to test\ntest$forecast &lt;- lm.df$`Point Forecast`\n\n# Let's look at the forecasts!\n\n# What do you test??\nggplot(data = compare, aes(x = time, y = observed))+\n  geom_line(color = \"blue\")+\n  geom_point(color = \"blue\")+\n  geom_line(aes(y = forecast), color = \"red\")+\n  geom_point(aes(y = forecast), color = \"red\")\n\n\n\n\n\n\n\n\nFinally we will generate and save the accuracy metrics again.\n\n# Last - let's get the RMSE and MAE out. We can use the Metrics package for this. While we're at it, let's save them to a data set for later.\n\nlm.rmse &lt;- rmse(test$observation, test$forecast)\nlm.mae &lt;- mae(test$observation, test$forecast)\n\nnewrow &lt;- c(\"lm\", lm.rmse, lm.mae)\n\n(comparisonDF &lt;- rbind(comparisonDF, newrow))\n\n  model               RMSE                 MAE\n1   stl 0.0116232134034641 0.00904861924106978\n2    lm 0.0113361721973712 0.00885045372950685\n\n\n\n\nApplying ARIMA to the residual series (ARIMA as a second-layer model)\n\nSlide 34-39\nWe can adjust any model with correlated error terms by modeling its residuals! The process is as follows:\n\nBuild a model\nExtract the residual series\nModel the residuals using ARIMA (or another time series model of your choosing)\nForecast the future residuals\nAdjust your forecasts using your forecasted residuals.\n\nWe are going to use the linear model from above as an example to see what effect it has on our predictions. First, let’s extract the residuals and turn them into a time series object.\n\n# Make a time series of the residuals from the model\nlmResids &lt;- ts(lm.fit$residuals)\n\nNow we can use the auto.arima() function to fit an ARIMA model to the residual series. Remember, auto.arima() will select p, d, and q for us.\n\n# Use the auto.arima() function to easily fit an ARIMA model\nresid.fit &lt;- auto.arima(lmResids)\nsummary(resid.fit)\n\nSeries: lmResids \nARIMA(3,0,4) with zero mean \n\nCoefficients:\n         ar1     ar2      ar3      ma1      ma2     ma3     ma4\n      0.9117  0.9255  -0.8546  -0.1998  -0.9377  0.3255  0.0977\ns.e.  0.0420  0.0259   0.0372   0.0485   0.0356  0.0258  0.0299\n\nsigma^2 = 4.973e-05:  log likelihood = 5404.25\nAIC=-10792.5   AICc=-10792.41   BIC=-10749.85\n\nTraining set error measures:\n                       ME        RMSE         MAE      MPE     MAPE      MASE\nTraining set 1.177324e-05 0.007036099 0.004752026 30.56487 84.39989 0.9678809\n                      ACF1\nTraining set -0.0006014338\n\n\nNow we will make forecasts for the error for the next 30 days and use those forecasted errors to adjust our original forecasts.\n\n# Forecast the error\nresid.forecast &lt;- forecast(resid.fit, h = 30)\n\nresid.df &lt;- as.data.frame(resid.forecast)\n\n# Adjust the original forecasts using the forecasted residuals\ntest$adjForecast &lt;- test$forecast + resid.df$`Point Forecast`\n\nLet’s see what effect that had on our original forecasts - any better?\n\n# Some data cleaning...\ntestLong &lt;- test %&gt;% pivot_longer(cols = c(forecast, observation, adjForecast), names_to = \"forecastType\", values_to = \"prediction\")\n\n# Plot the results! What do you think?\nggplot(data = testLong, aes(x = X, y = prediction))+\n  geom_line(aes(color = forecastType, lty = forecastType))+\n  geom_point(aes(color = forecastType))\n\n\n\n\n\n\n\n\nWe can look at the acf() plots from before and after to see how they compare.\n\n# Did we take care of that extra autocorrelation from before?\ntest$oldResids &lt;- test$observation - test$forecast\nacf(test$oldResids)\n\n\n\n\n\n\n\ntest$newResids &lt;- test$observation - test$adjForecast\nacf(test$newResids)\n\n\n\n\n\n\n\n\nFinally, let’s generate a new set of accuracy metrics for the adjusted model.\n\n# We will add this to our comparisonDF to compare later\n\nlmAdj.rmse &lt;- rmse(test$observation, test$adjForecast)\nlmAdj.mae &lt;- mae(test$observation, test$adjForecast)\n\nnewrow &lt;- c(\"lmAdj\", lmAdj.rmse, lmAdj.mae)\n\n(comparisonDF &lt;- rbind(comparisonDF, newrow))\n\n  model                RMSE                 MAE\n1   stl  0.0116232134034641 0.00904861924106978\n2    lm  0.0113361721973712 0.00885045372950685\n3 lmAdj 0.00975236059762854 0.00759983144121353"
  },
  {
    "objectID": "VB_TimeSeriesForecastingPractical.html#generalized-additive-models-gams",
    "href": "VB_TimeSeriesForecastingPractical.html#generalized-additive-models-gams",
    "title": "VectorByte Methods Training: Introduction to Time Series Forecasting (practical)",
    "section": "(Generalized) Additive Models (GAMs)",
    "text": "(Generalized) Additive Models (GAMs)\n\nNot in lecture :) #Bonus\nWe did not explicitly cover these in the lecture, but Generalized Additive Models (GAMs) are similar to generalized linear models except that they allow us to include spline terms. These can be useful tools for seasonal data as well. On the flip side, splines are complicated beasts and highly susceptible to traps like overfitting. We don’t have time to cover them in depth today so I set a few things in the s() function for you, but I highly encourage you to look at the documentation for both the gam() model fitting function in mgcv and the s() function that produces the spline terms.\n\n# GAM modeling\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:forecast':\n\n    getResponse\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\n# We need a 'day of the year' variable to capture the annual seasonality.\ntrain$DOY &lt;- yday(train$DATE)\ntest$DOY &lt;- yday(test$DATE)\n\ngam.fit &lt;- gam(observation ~ X + meanTemp + meanTempLag1 + \n                 s(DOY, bs = \"cr\", k = 2),\n               data = train)\n\nWarning in smooth.construct.cr.smooth.spec(object, dk$data, dk$knots): basis dimension, k, increased to minimum possible\n\nsummary(gam.fit)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nobservation ~ X + meanTemp + meanTempLag1 + s(DOY, bs = \"cr\", \n    k = 2)\n\nParametric coefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.775e-01  4.485e-03  61.860  &lt; 2e-16 ***\nX            8.699e-06  1.565e-06   5.560 3.19e-08 ***\nmeanTemp     9.191e-04  1.187e-04   7.743 1.76e-14 ***\nmeanTempLag1 9.240e-04  1.173e-04   7.878 6.26e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n         edf Ref.df     F p-value    \ns(DOY) 1.971  1.999 24.67  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.671   Deviance explained = 67.2%\nGCV = 0.00071784  Scale est. = 0.00071503  n = 1528\n\n\nMost of the techniques from the linear model above apply here. We can evaluate the residuals to see if we missed any opportunities.\n\n# Check for leftover autocorrelation in the residuals - it may be wise to apply the residual ARIMA from above.\nacf(gam.fit$residuals)\n\n\n\n\n\n\n\n# These models still do assume normal residuals - these look good!\nhist(gam.fit$residuals)\n\n\n\n\n\n\n\n\nThis time we will generate forecasts using the predict() function because it works better with this kind of model.\n\n# For some reason the forecast() function did not like the GAM... We can just use good old predict to do the same thing since test contains the number of rows we need.\ngam.forecasts &lt;- predict(gam.fit, newdata = test)\n\n# The predict function gives us point forecasts.\ngam.forecasts\n\n     1529      1530      1531      1532      1533      1534      1535      1536 \n0.4194578 0.4266983 0.4301488 0.4267564 0.4194278 0.4229415 0.4378083 0.4422476 \n     1537      1538      1539      1540      1541      1542      1543      1544 \n0.4402423 0.4378353 0.4287927 0.4269180 0.4310494 0.4340437 0.4397890 0.4368044 \n     1545      1546      1547      1548      1549      1550      1551      1552 \n0.4251501 0.4223446 0.4327906 0.4339941 0.4275738 0.4297662 0.4331898 0.4380823 \n     1553      1554      1555      1556      1557      1558 \n0.4424296 0.4412897 0.4390363 0.4399015 0.4418108 0.4431266 \n\n# Add the forecasts to test\ntest$GAMforecast &lt;- gam.forecasts\n\nLet’s see what they look like and stash the accuracy metrics for later.\n\n# Let's look at the forecasts!\n\n# What do you think??\nggplot(data = test, aes(x = X, y = observation))+\n  geom_line(color = \"blue\")+\n  geom_point(color = \"blue\")+\n  geom_line(aes(y = GAMforecast), color = \"red\")+\n  geom_point(aes(y = GAMforecast), color = \"red\")\n\n\n\n\n\n\n\n# Save the results\n\ngam.rmse &lt;- rmse(test$observation, test$GAMforecast)\ngam.mae &lt;- mae(test$observation, test$GAMforecast)\n\nnewrow &lt;- c(\"gam\", gam.rmse, gam.mae)\n\n(comparisonDF &lt;- rbind(comparisonDF, newrow))\n\n  model                RMSE                 MAE\n1   stl  0.0116232134034641 0.00904861924106978\n2    lm  0.0113361721973712 0.00885045372950685\n3 lmAdj 0.00975236059762854 0.00759983144121353\n4   gam   0.012275278533877 0.00938259655210016"
  },
  {
    "objectID": "VB_TimeSeriesForecastingPractical.html#time-varying-coefficient-ar-model",
    "href": "VB_TimeSeriesForecastingPractical.html#time-varying-coefficient-ar-model",
    "title": "VectorByte Methods Training: Introduction to Time Series Forecasting (practical)",
    "section": "Time-Varying Coefficient AR Model",
    "text": "Time-Varying Coefficient AR Model\n\nSlide 50-51\nTime-varying (TV) coefficient models gained popularity after their introduction to the forecasting world and their applications and methodologies are now expansive. There is no way to cover all of the TV models that are now available in software, so we will explore a time-varying auto-regressive (TVAR) model in the tvReg package in R. This package supports several other kinds of TV models as well.\ntvReg requires that we clean up our data some to only include the necessary information. We will do that first.\n\nlibrary(tvReg)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nFunded by the Horizon 2020. Framework Programme of the European Union.\n\n\n\nAttaching package: 'tvReg'\n\n\nThe following object is masked from 'package:forecast':\n\n    forecast\n\ntrainRed &lt;- train %&gt;% select(meanTemp,\n                             meanTempLag1,\n                             sinSeason,\n                             cosSeason)\n\ntestRed &lt;- test %&gt;% select(meanTemp,\n                           meanTempLag1,\n                           sinSeason,\n                           cosSeason)\n\nNow we can fit the model! We are fitting a TV model that accounts for autocorrelation via an AR model. The argument p lets us tell tvAR how many lags to include in the AR part of the model. The exogen argument is where we put our data set of predictor variables that we made above.\n\n# Fit the model - p represents the order of the AR model. \n#     Exogen contains the exogenous variables we want to use.\n#     See the documentation for explanations of the other \n#     available options.\ntvar.fit &lt;- tvAR(train$observation, p = 1, exogen = trainRed)\n\nCalculating regression bandwidth...\n\nsummary(tvar.fit)\n\n\nCall: \ntvAR(y = train$observation, p = 1, exogen = trainRed)\n\nClass:  tvar \n\nSummary of time-varying estimated coefficients: \n================================================ \n        (Intercept)   y.l1  meanTemp meanTempLag1 sinSeason cosSeason\nMin.        0.01343 0.9525 0.0002315   -0.0001426  0.001012 0.0005829\n1st Qu.     0.01344 0.9525 0.0002315   -0.0001426  0.001012 0.0005830\nMedian      0.01344 0.9525 0.0002315   -0.0001426  0.001012 0.0005831\nMean        0.01344 0.9525 0.0002315   -0.0001426  0.001012 0.0005831\n3rd Qu.     0.01344 0.9526 0.0002315   -0.0001426  0.001013 0.0005832\nMax.        0.01345 0.9526 0.0002315   -0.0001426  0.001013 0.0005833\n\nBandwidth:  20\nPseudo R-squared:  0.9751 \n\nacf(tvar.fit$residuals, 365)\n\n\n\n\n\n\n\n\nA neat thing to look at for the TV models is how the coefficients change with time. This is set within the modeling framework, so it is no surprise that these graphs look this way - hopefully these help you see how the model is allowing the betas to change.\n\n# Plot how the coefficients change over time\npar(mfrow=c(3,2), bty=\"n\")\nplot(train$X[2:1528], tvar.fit$coefficients[,1]) # Intercept\nplot(train$X[2:1528], tvar.fit$coefficients[,2]) # coef for AR1\nplot(train$X[2:1528], tvar.fit$coefficients[,3]) # coef meanTemp\nplot(train$X[2:1528], tvar.fit$coefficients[,4]) # coef for meanTempLag1\nplot(train$X[2:1528], tvar.fit$coefficients[,5]) # coef for sin\nplot(train$X[2:1528], tvar.fit$coefficients[,6]) # coef for cos\n\n\n\n\n\n\n\n\nForecast generation time! The forecast function works for this package too! How do they look?\n\n# Generate 30 day out forecasts using the forecast function\ntest$tvarForecast &lt;- forecast(tvar.fit, n.ahead = 30, \n                              newexogen = testRed)\n\n# What do you think?\nggplot(data = test, aes(x = X, y = observation))+\n  geom_line(color = \"black\")+\n  geom_point(color = \"black\")+\n  geom_line(aes(y = tvarForecast), color = \"red\")+\n  geom_point(aes(y = tvarForecast), color = \"red\")\n\n\n\n\n\n\n\n\nWe’ll save the results for later comparison.\n\n# Save the results\n\ntvar.rmse &lt;- rmse(test$observation, test$tvarForecast)\ntvar.mae &lt;- mae(test$observation, test$tvarForecast)\n\nnewrow &lt;- c(\"tvar\", tvar.rmse, tvar.mae)\n\n(comparisonDF &lt;- rbind(comparisonDF, newrow))\n\n  model                RMSE                 MAE\n1   stl  0.0116232134034641 0.00904861924106978\n2    lm  0.0113361721973712 0.00885045372950685\n3 lmAdj 0.00975236059762854 0.00759983144121353\n4   gam   0.012275278533877 0.00938259655210016\n5  tvar    0.01080312146469 0.00828432083262372"
  },
  {
    "objectID": "VB_TimeSeriesForecastingPractical.html#neural-network",
    "href": "VB_TimeSeriesForecastingPractical.html#neural-network",
    "title": "VectorByte Methods Training: Introduction to Time Series Forecasting (practical)",
    "section": "Neural Network",
    "text": "Neural Network\n\nSlide 53-55\nDisclaimer: I do not claim to be an expert of any kind in machine learning methods, but I wanted to provide you with a basic ML option to explore. Additionally, if ML is what you find yourself interested in, I highly suggest you learn how to code in Python because it has way more tools for this kind of modeling :).\nWe need a particular predictor set up to model time series data and capture autocorrelation and seasonality. We can build this kind of model using the nnetar() function in the forecast package in R. This is nice because that means we can use all of the other useful functions in the forecast package too! It is also nice because it does all of the tedious pre-processing for us!\nFirst we need to set up a data frame with relevant predictors like we did for the TV model.\n\n# Make reduced data set of predictors - this is how we have to set up the predictors for the model fitting package.\ntrainRed &lt;- train %&gt;% select(meanTemp,\n                             meanTempLag1)\n\ntestRed &lt;- test %&gt;% select(meanTemp,\n                           meanTempLag1)\n\nNow we can fit the model! Notice we are building a time series object again and specifying a frequency. This tells nnetar() that the data are seasonal annually. We get to tell the model how far back to look using the p and P arguments. The argument scale_inputs = T does some critical centering and scaling for us to save us a lot of coding time.\n\n# Now we are ready to fit the model!\nobsTrain &lt;- ts(train$observation, frequency = 365)\n\nnn.fit &lt;- nnetar(obsTrain,\n                 xreg = trainRed, #include our external predictors\n                 scale.inputs = T, # scale the inputs - critical for ML. The only time this would be set to F is if you already scaled them.\n                 size = 1, # size of the hidden layer\n                 p = 2, # number of regular lags\n                 P = 2) # number of seasonal lags\n\nTime to make some forecasts!\n\n# To get the forecasts, we can use the forecast function. I had to explicitly tell R to look in the forecast package for this one. You may not need the extra forecast:: part.\nnn.results &lt;- as.data.frame(forecast::forecast(nn.fit, 30, xreg = testRed))\n\nresults &lt;- data.frame(time = 1:30,\n                      actual = test$observation, \n                      forecast = nn.results$`Point Forecast`)\n\nLet’s have a look at the forecasts and save some accuracy metrics!\n\n# Let's check out the results!\nggplot(data = results, aes(x = time, y = actual))+\n  geom_line(color = \"blue\")+\n  geom_point(color = \"blue\")+\n  geom_line(aes(y = forecast), color = \"red\")+\n  geom_point(aes(y = forecast), color = \"red\")\n\n\n\n\n\n\n\n# Save the results for comparison\n\nnn.rmse &lt;- rmse(results$actual, results$forecast)\nnn.mae &lt;- mae(results$actual, results$forecast)\n\nnewrow &lt;- c(\"nn\", nn.rmse, nn.mae)\n\n(comparisonDF &lt;- rbind(comparisonDF, newrow))\n\n  model                RMSE                 MAE\n1   stl  0.0116232134034641 0.00904861924106978\n2    lm  0.0113361721973712 0.00885045372950685\n3 lmAdj 0.00975236059762854 0.00759983144121353\n4   gam   0.012275278533877 0.00938259655210016\n5  tvar    0.01080312146469 0.00828432083262372\n6    nn 0.00957754199104779 0.00777989960957334"
  },
  {
    "objectID": "VB_TimeSeriesForecastingPractical.html#an-easy-ensemble",
    "href": "VB_TimeSeriesForecastingPractical.html#an-easy-ensemble",
    "title": "VectorByte Methods Training: Introduction to Time Series Forecasting (practical)",
    "section": "An Easy Ensemble",
    "text": "An Easy Ensemble\n\nSlide 57-58\nThe last modeling approach we will explore today is a basic ensemble modeling approach. Ensemble models incorporate forecasts from multiple other models to produce forecasts that are some kind of average of all the others. There are several averaging methods that exist, but a really simple one is to use a weighted average where the weight is based on each model’s test set RMSE. This way, the best performing models get to contribute the most information to the new forecasts. Amazingly, these ensembles often produce better predictions than any one model alone - even with bad predictions included!\nFirst we will pull all of the forecasts we made into a data set.\n\n# Make a data set of all of the previous forecasts\n\nensembleDF &lt;- data.frame(stl = stl.df$`Point Forecast`,\n                         lm = lm.df$`Point Forecast`,\n                         lmAdj = test$adjForecast,\n                         gam = test$GAMforecast,\n                         tvar = test$tvarForecast,\n                         nn =  results$forecast,\n                         observed = test$observation)\n\nNow we need to build a weighting system for the ensemble predictions. Here we are going to use the system shown in the lecture (slide 58).\n\n# Now we can set up our weighting system based on RMSE\n\ntotalWeight &lt;- (1/stl.rmse) +\n               (1/lm.rmse) +\n               (1/lmAdj.rmse) +\n               (1/gam.rmse) +\n               (1/tvar.rmse) +\n               (1/nn.rmse)\n  \nweightSTL &lt;- (1/stl.rmse)/totalWeight\nweightLM &lt;- (1/lm.rmse)/totalWeight\nweightLMadj &lt;- (1/lmAdj.rmse)/totalWeight\nweightGAM &lt;- (1/gam.rmse)/totalWeight\nweightTVAR &lt;- (1/tvar.rmse)/totalWeight\nweightNN &lt;- (1/nn.rmse)/totalWeight\n\nNow we can use those weights to build a column of ensemble forecasts into the data set we made. All we have to do is multiply the other forecasts by their weights and add them all up.\n\n# Add a column to our data frame with the ensemble forecasts\n\nensembleDF &lt;- ensembleDF %&gt;% mutate(ensembleForecast = \n                                      (stl*weightSTL)+\n                                      (lm*weightLM)+\n                                      (lmAdj*weightLMadj)+\n                                      (gam*weightGAM)+\n                                      (tvar*weightTVAR)+\n                                      (nn*weightNN))\n\nLet’s see how the ensemble predictions look!\n\n# Let's check them out!\nensembleDF$time &lt;- 1:30\n\n# What do you think??\nggplot(ensembleDF, aes(x = time, y = observed))+\n  geom_line(color = \"blue\")+\n  geom_point(color = \"blue\")+\n  geom_line(aes(y = ensembleForecast), color = \"red\")+\n  geom_point(aes(y = ensembleForecast), color = \"red\")\n\n\n\n\n\n\n\n\nLast we will save the accuracy metrics.\n\n# Save the results like before\n\nens.rmse &lt;- rmse(ensembleDF$observed, ensembleDF$ensembleForecast)\nens.mae &lt;- mae(ensembleDF$observed, ensembleDF$ensembleForecast)\n\nnewrow &lt;- c(\"ens\", ens.rmse, ens.mae)\n\n(comparisonDF &lt;- rbind(comparisonDF, newrow))\n\n  model                RMSE                 MAE\n1   stl  0.0116232134034641 0.00904861924106978\n2    lm  0.0113361721973712 0.00885045372950685\n3 lmAdj 0.00975236059762854 0.00759983144121353\n4   gam   0.012275278533877 0.00938259655210016\n5  tvar    0.01080312146469 0.00828432083262372\n6    nn 0.00957754199104779 0.00777989960957334\n7   ens 0.00944071281885272 0.00786566824479918"
  },
  {
    "objectID": "VB_TimeSeriesForecastingPractical.html#challenge-details",
    "href": "VB_TimeSeriesForecastingPractical.html#challenge-details",
    "title": "VectorByte Methods Training: Introduction to Time Series Forecasting (practical)",
    "section": "Challenge Details",
    "text": "Challenge Details\n\nThe Data Set\nYou will use the data set called NEONbeetlesChallenge.csv for this challenge. It contains biweekly (every two weeks) observations of beetle abundances at one NEON site, in addition to several climatological covariates for that site. You also have access to one-week lagged versions of the climate variables to use if you choose.\nThe data set called tinyForecastingChallengeCovs.csv contains all of the covariates for the 5 time-steps that you need to forecast for.\nWhen you have your forecasts, enter them here: Beetle Forecasting Challenge Entry Form\n\n\nHints\n\nNotice that the data set has an indicator variable that tells you whether or not there is an active sampling period going on, because when no sampling occurs we are assuming the number of beetles observed will always be 0. That makes these cases ‘special events’ - how can you take advantage of that information? See slides 46-49"
  }
]