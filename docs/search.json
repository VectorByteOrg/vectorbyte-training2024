[
  {
    "objectID": "VB_IntroTimeDepData.html#learning-objectives",
    "href": "VB_IntroTimeDepData.html#learning-objectives",
    "title": "VectorByte Methods Training",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand basic concepts in how we understand and model time-dependent population data in VBD applications\nReview basic idea of forecasting\nOverview of course"
  },
  {
    "objectID": "VB_IntroTimeDepData.html#population-dynamics-of-disease",
    "href": "VB_IntroTimeDepData.html#population-dynamics-of-disease",
    "title": "VectorByte Methods Training",
    "section": "Population dynamics of disease",
    "text": "Population dynamics of disease\nThe number of hosts, vectors, pathogens, and infected individuals change over time\nWe use models to understand and to forecast/predict"
  },
  {
    "objectID": "VB_IntroTimeDepData.html#what-types-of-models",
    "href": "VB_IntroTimeDepData.html#what-types-of-models",
    "title": "VectorByte Methods Training",
    "section": "What types of models?",
    "text": "What types of models?\ntactical to strategic\nWe’ll focus on the tactical end of things here (i.e., no dif eqs)"
  },
  {
    "objectID": "VB_IntroTimeDepData.html#simple-example-a-first-deterministic-model",
    "href": "VB_IntroTimeDepData.html#simple-example-a-first-deterministic-model",
    "title": "VectorByte Methods Training",
    "section": "Simple example: A first (deterministic) model",
    "text": "Simple example: A first (deterministic) model\n\n\nSuppose we model a population in discrete time as \\[\\begin{align*}\nN(t+1) = s N(t) + b(t).\n\\end{align*}\\]\nHere \\(s\\) is the fraction of individuals surviving each time step and \\(b(t)\\) is the number of new births."
  },
  {
    "objectID": "VB_IntroTimeDepData.html#a-first-stochastic-model",
    "href": "VB_IntroTimeDepData.html#a-first-stochastic-model",
    "title": "VectorByte Methods Training",
    "section": "A first (stochastic) model",
    "text": "A first (stochastic) model\nIn that simplest model the population can’t go extinct!\n\nWhat if the number of births vary? \\[\\begin{align*}\nN(t+1) = s N(t) + b(t) +W(t)\n\\end{align*}\\]\nHere \\(W(t)\\) is process uncertainty/stochasticity: we assume it is drawn from a particular distribution, and each year/time we observe a particular value \\(w(t)\\). E.g., \\(W(t) \\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2)\\).\n\nWhat might we see? When would the population go extinct?"
  },
  {
    "objectID": "VB_IntroTimeDepData.html#observation-models",
    "href": "VB_IntroTimeDepData.html#observation-models",
    "title": "VectorByte Methods Training",
    "section": "Observation Models",
    "text": "Observation Models\nWe also have to go out into the field and take some observations of the populations. Let’s say that we observe \\(N_{\\mathrm{obs}}(t)\\) individuals at time \\(t\\). How does this relate to the true population size? One possibility is: \\[\\begin{align*}\nN_{\\mathrm{obs}}(t) = N(t) + V(t)\n\\end{align*}\\] where \\(V(t)\\) is our “observation uncertainty”, and all together this equation describes our observation model."
  },
  {
    "objectID": "VB_IntroTimeDepData.html#observation-process-matters",
    "href": "VB_IntroTimeDepData.html#observation-process-matters",
    "title": "VectorByte Methods Training",
    "section": "Observation process matters",
    "text": "Observation process matters\nAnalysis approach depends on the sampling – evenly or unevenly spaced, goal of the modeling exercise (understanding vs forecasting/prediction)."
  },
  {
    "objectID": "VB_IntroTimeDepData.html#forecasting-process",
    "href": "VB_IntroTimeDepData.html#forecasting-process",
    "title": "VectorByte Methods Training",
    "section": "Forecasting process",
    "text": "Forecasting process\nsomething about forecasting"
  },
  {
    "objectID": "VB_IntroTimeDepData.html#outline-of-course",
    "href": "VB_IntroTimeDepData.html#outline-of-course",
    "title": "VectorByte Methods Training",
    "section": "Outline of Course",
    "text": "Outline of Course\n\nAbundance data from VecDyn and NEON\nRegression refresher for time dep data – basics plus time dependent predictors, transformations, simple AR\nAnalysis of evenly-spaced data: basic time-series methods\nAdvanced modeling with Gaussian Process Models"
  },
  {
    "objectID": "Stats_review_soln.html",
    "href": "Stats_review_soln.html",
    "title": "VectorByte Methods Training 2024",
    "section": "",
    "text": "Main materials\nBack to stats review\n\nQuestion 1: For the six-sided fair die, what is f_k if k=7? k=1.5?\n\nAnswer: both of these are zero, because the die cannot take these values.\n    \n\n\nQuestion 2: For the fair 6-sided die, what is F(3)? F(7)? F(1.5)?\nAnswer: The CDF total probability of having a value less than or equal to its argument. Thus F(3)= 1/2, F(7)=1, and F(1.5)=1/6\n    \n\n\nQuestion 3: For a normal distribution with mean 0, what is F(0)?\n\nAnswer: The normal distribution is symmetric around its mean, with half of its probability on each side. Thus, F(0)=1/2\n    \n\n\nQuestion 4: Summation Notation Practice\n\n\n\ni\n1\n2\n3\n4\n\n\n\n\nZ_i\n2.0\n-2.0\n3.0\n-3.0\n\n\n\n\nCompute \\sum_{i=1}^{4}{z_i} = 0 \nCompute \\sum_{i=1}^4{(z_i - \\bar{z})^2} = 26 \nWhat is the sample variance? Assume that the z_i are i.i.d.. Note that i.i.d.~stands for “independent and identically distributed”. \n\nSolution: \ns^2= \\frac{\\sum_{i=1}^N(Y_i - \\bar{Y})^2}{N-1} = \\frac{26}{3}\n= 8\\times \\frac{2}{3}\n \n\nFor a general set of N numbers, \\{X_1, X_2, \\dots, X_N \\} and \\{Y_1, Y_2, \\dots, Y_N \\} show that \n\\sum_{i=1}^N{(X_i - \\bar{X})(Y_i - \\bar{Y})} = \\sum_{i=1}^N{(X_i-\\bar{X})Y_i}\n\n\n Solution: First, we multiply through and distribute: \n\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y}) = \\sum_{i=1}^N(X_i-\\bar{X})Y_i\n- \\sum_{i=1}^N(X_i-\\bar{X})\\bar{Y}\n Next note that \\bar{Y} (the mean of the Y_is) doesn’t depend on i so we can pull it out of the summation: \n\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y}) = \\sum_{i=1}^N(X_i-\\bar{X})Y_i\n- \\bar{Y} \\sum_{i=1}^N(X_i-\\bar{X}).\n Finally, the last sum must be zero because \n\\sum_{i=1}^N(X_i-\\bar{X}) = \\sum_{i=1}^N X_i- \\sum_{i=1}^N \\bar{X} = N\\bar{X} - N\\bar{X}=0.\n Thus \\begin{align*}\n\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y}) &= \\sum_{i=1}^N(X_i-\\bar{X})Y_i - \\bar{Y}\\times 0\\\\\n& = \\sum_{i=1}^N(X_i-\\bar{X})Y_i.\n\\end{align*}\n    \n\n\nQuestion 5: Properites of Expected Values\nUsing the definition of an expected value above and with X and Y having the same probability distribution, show that:\n\\begin{align*}\n\\text{E}[X+Y]  & = \\text{E}[X] + \\text{E}[Y]\\\\  \n& \\text{and} \\\\\n\\text{E}[cX]  & = c\\text{E}[X]. \\\\\n\\end{align*}\nGiven these, and the fact that \\mu=\\text{E}[X], show that:\n\\begin{align*}\n\\text{E}[(X-\\mu)^2]  = \\text{E}[X^2] - (\\text{E}[X])^2\n\\end{align*}\nThis gives a formula for calculating variances (since \\text{Var}(X)= \\text{E}[(X-\\mu)^2]).\nSolution: Assuming X and Y are both i.i.d. with distribution f(x). The expectation of X+Y is defined as \\begin{align*}\n\\text{E}[X+Y]  & =  \\int (X+Y) f(x)dx \\\\\n              & =  \\int (X f(x) +Y f(x))dx  \\\\\n              & =  \\int X f(x)dx  +\\int Y f(x)dx  \\\\\n               & = \\text{E}[X] + \\text{E}[Y]  \n\\end{align*} Similarly \\begin{align*}\n\\text{E}[cX]   & =  \\int cXf(x)dx \\\\\n              & =  c \\int Xf(x) dx  \\\\\n              & = c\\text{E}[X]. \\\\\n\\end{align*} Thus we can re-write: \\begin{align*}\n\\text{E}[(X-\\mu)^2]  & = \\text{E}[ X^2 - 2X\\mu + \\mu^2] \\\\\n                        & = \\text{E}[X^2] - 2\\mu\\text{E}[X] + \\mu^2 \\\\\n                        & = \\text{E}[X^2] -2\\mu^2 + \\mu^2 \\\\\n                        & = \\text{E}[X^2] - \\mu^2 \\\\\n& = \\text{E}[X^2] - (\\text{E}[X])^2.\n\\end{align*}\n   \n\n\nQuestion 6: Functions of Random Variables\nSuppose that \\mathrm{E}[X]=\\mathrm{E}[Y]=0, \\mathrm{var}(X)=\\mathrm{var}(Y)=1, and \\mathrm{corr}(X,Y)=0.5.\n\nCompute \\mathrm{E}[3X-2Y]; and\n\\mathrm{var}(3X-2Y).\nCompute \\mathrm{E}[X^2].\n\nSolution:\n\nUsing the properties of expectations, we can re-write this as: \\begin{align*}\n\\mathrm{E}[3X-2Y] & = \\mathrm{E}[3X] + \\mathrm{E}[-2Y]\\\\\n& = 3 \\mathrm{E}[X] -2 \\mathrm{E}[Y]\\\\\n& = 3 \\times 0 -2 \\times 0\\\\\n&=0\n\\end{align*}\n\n\nUsing the properties of variances, we can re-write this as: \\begin{align*}\n\\mathrm{var}(3X-2Y) & = 3^2\\text{Var}(X) + (-2)^2\\text{Var}(Y) + 2(3)(-2)\\text{Cov}(XY)\\\\\n& =  9 \\times 1 + 4 \\times 1 -12 \\text{Corr}(XY)\\sqrt{\\text{Var}(X)\\text{Var}(Y)}\\\\\n& = 9+4 -12 \\times 0.5\\times1\\\\\n&=7\n\\end{align*}\n\n\nRecalling from Question 5 that the variance is \\mathrm{var}(X) = \\text{E}[X^2] - (\\text{E}[X])^2, we can re-arrange to obtain: \\begin{align*}\n\\mathrm{E}[X^2] & = \\mathrm{var}(X) + (\\mathrm{E}[X])^2\\\\\n& = 1+(0)^2 \\\\\n& =1\n\\end{align*}\n\n\n\nThe Sampling Distribution\nSuppose we have a random sample \\{Y_i, i=1,\\dots,N \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,4) for i=1,\\ldots,N.\n\nWhat is the variance of the sample mean?\n\n\\displaystyle \\mathrm{Var}(\\bar{Y}) =\n\\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i=1}^N Y_i\\right) =\n\\frac{N}{N^2}\\mathrm{Var}(Y) =\\frac{4}{N}.\nThis is the derivation for the variance of the sampling distribution.\n \n\nWhat is the expectation of the sample mean?\n\n\\displaystyle\\mathrm{E}[\\bar{Y}] = \\frac{N}{N}\\mathrm{E}(Y) = \\mu. This is the mean of the sampling distribution.\n\n\nWhat is the variance for another i.i.d. realization Y_{ N+1}?\n\n\\displaystyle \\mathrm{Var}(Y) = 4, because this is a sample directly from the population distribution.\n \n\nWhat is the standard error of \\bar{Y}?\n\nHere, again, we are looking at the distribution of the sample mean, so we must consider the sampling distribution, and the standard error (aka the standard distribution) is just the square root of the variance from part i.\n\\displaystyle \\mathrm{se}(\\bar{Y}) = \\sqrt{\\mathrm{Var}(\\bar{Y})} =\\frac{2}{\\sqrt{N}}.\n\n\nHypothesis Testing and Confidence Intervals\nSuppose we sample some data \\{Y_i, i=1,\\dots,n \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,\\sigma^2) for i=1,\\ldots,n, and that you want to test the null hypothesis H_0: ~\\mu=12 vs. the alternative H_a: \\mu \\neq ` r m`, at the 0.05 significance level.\n\nWhat test statistic would you use? How do you estimate \\sigma?\nWhat is the distribution for this test statistic if the null is true?\nWhat is the distribution for the test statistic if the null is true and n \\rightarrow \\infty?\nDefine the test rejection region. (I.e., for what values of the test statistic would you reject the null?)\nHow would compute the p-value associated with a particular sample?\nWhat is the 95% confidence interval for \\mu? How should one interpret this interval?\nIf \\bar{Y} = 11, s_y = 1, and n=9, what is the test result? What is the 95% CI for \\mu?\n\n  \nThis question is asking you think about the hypothesis that the mean of your distribution is equal to 12. I give you the distribution of the data themselves (i.e., that they’re normal). To test the hypothesis, you work with the sampling distribution (i.e., the distribution of the sample mean) which is: \\bar{Y}\\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right).\n\nIf we knew \\sigma, we could use as our test statistic z=\\displaystyle \\frac{\\bar{y} - 12}{\\sigma/\\sqrt{n}}. However, here we need to estimate \\sigma so we use z=\\displaystyle \\frac{\\bar{y} - 12}{s_y/\\sqrt{n}} where \\displaystyle s_{y} = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\bar{y})^2}{n-1}}.\n\n\nIf the null is true, the z \\sim t_{n-1}(0,1). Since we estimate the mean frm the data, the degrees of freedom is n-1.\n\n\nAs n approaches infinity, t_{n-1}(0,1) \\rightarrow N(0,1).\n\n\nYou reject the null for \\{z: |z| &gt; t_{n-1,\\alpha/2}\\}.\n\n\nThe p-value is 2\\Pr(Z_{n-1} &gt;|z|). \n\n\nThe 95% CI is \\bar{Y} \\pm \\frac{s_{y}}{\\sqrt{n}} t_{n-1,\\alpha/2}.\n\nFor 19 out of 20 different samples, an interval constructed in this way will include the true value of the mean, \\mu. \n\nz = (11-12)/(1/3) = -3 and 2\\Pr(Z_{8} &gt;|z|) = .017, so we do reject the null.  The 95% CI for \\mu is 11 \\pm \\frac{1}{3}2.3 = (10.23, 11.77)."
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "VectorByte Training Materials 2024",
    "section": "",
    "text": "We will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 3.6 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and test it before the workshop.\nWe will also be using Slack for additional support during the training. Please have these installed in advance. We will have a channel on Slack dedicated to software/hardware issues and troubleshooting.\n\n\n\nWe are assuming familiarity with R basics as well as at least introductory statistics, including up through simple linear regression. If you would like materials to review, we recommend that you do the following:\nReview of R\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter.\nIn addition / alternatively to pre-work element (1), here are some resources for brushing up on R at the end of the Intro R Chapter you can try. There are many more resources online (e.g., this and this ) – pick something that suits your learning style.\n\nReview of statistics\n\nReview background on introductory probability and statistics (solutions to exercises). You can also use the resources on The Multilingual Quantitative Biologist - Basic Data Analyses and Statistics through into linear models.\nReview the conceptual basics of Simple Linear Regression. We’ll refresh fitting linear models in R as well as transformations and diagnostic plots during the workshop."
  },
  {
    "objectID": "materials.html#hardware-and-software",
    "href": "materials.html#hardware-and-software",
    "title": "VectorByte Training Materials 2024",
    "section": "",
    "text": "We will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 3.6 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and test it before the workshop.\nWe will also be using Slack for additional support during the training. Please have these installed in advance. We will have a channel on Slack dedicated to software/hardware issues and troubleshooting."
  },
  {
    "objectID": "materials.html#pre-requisites",
    "href": "materials.html#pre-requisites",
    "title": "VectorByte Training Materials 2024",
    "section": "",
    "text": "We are assuming familiarity with R basics as well as at least introductory statistics, including up through simple linear regression. If you would like materials to review, we recommend that you do the following:\nReview of R\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter.\nIn addition / alternatively to pre-work element (1), here are some resources for brushing up on R at the end of the Intro R Chapter you can try. There are many more resources online (e.g., this and this ) – pick something that suits your learning style.\n\nReview of statistics\n\nReview background on introductory probability and statistics (solutions to exercises). You can also use the resources on The Multilingual Quantitative Biologist - Basic Data Analyses and Statistics through into linear models.\nReview the conceptual basics of Simple Linear Regression. We’ll refresh fitting linear models in R as well as transformations and diagnostic plots during the workshop."
  },
  {
    "objectID": "materials.html#introduction-to-time-dependent-abundance-data",
    "href": "materials.html#introduction-to-time-dependent-abundance-data",
    "title": "VectorByte Training Materials 2024",
    "section": "Introduction to Time Dependent Abundance Data",
    "text": "Introduction to Time Dependent Abundance Data"
  },
  {
    "objectID": "materials.html#introduction-to-the-vecdyn-database1",
    "href": "materials.html#introduction-to-the-vecdyn-database1",
    "title": "VectorByte Training Materials 2024",
    "section": "Introduction to the VecDyn database1",
    "text": "Introduction to the VecDyn database1\n\nThis component will be delivered live & synchronously. The VecDyn website can be found here. It might be an idea to explore this prior to the workshop."
  },
  {
    "objectID": "materials.html#regression-in-r-refresher",
    "href": "materials.html#regression-in-r-refresher",
    "title": "VectorByte Training Materials 2024",
    "section": "Regression in R Refresher",
    "text": "Regression in R Refresher"
  },
  {
    "objectID": "materials.html#time-dependent-regression-analysis",
    "href": "materials.html#time-dependent-regression-analysis",
    "title": "VectorByte Training Materials 2024",
    "section": "Time dependent regression analysis",
    "text": "Time dependent regression analysis"
  },
  {
    "objectID": "materials.html#basics-of-time-series-using-r",
    "href": "materials.html#basics-of-time-series-using-r",
    "title": "VectorByte Training Materials 2024",
    "section": "Basics of Time Series using R",
    "text": "Basics of Time Series using R"
  },
  {
    "objectID": "materials.html#gaussian-process-models-gps-for-time-dependent-data",
    "href": "materials.html#gaussian-process-models-gps-for-time-dependent-data",
    "title": "VectorByte Training Materials 2024",
    "section": "Gaussian Process models (GPs) for Time Dependent Data",
    "text": "Gaussian Process models (GPs) for Time Dependent Data"
  },
  {
    "objectID": "materials.html#footnotes",
    "href": "materials.html#footnotes",
    "title": "VectorByte Training Materials 2024",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhat is the difference between VectorBiTE and VectorByte? We are glad you asked! VectorBiTE was an RCN or a research coordination network funded by a 5 year grant from the BBSRC. VectorByte is hosting this training which is a newly funded NSF grant to establish a global open access data platform to study disease vectors. All the databases have transitioned to VectorByte but the legacy options will still be available on the VectorBiTE website.↩︎"
  },
  {
    "objectID": "materials_temp.html",
    "href": "materials_temp.html",
    "title": "VectorByte Training Materials 2024",
    "section": "",
    "text": "We will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 3.6 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and test it before the workshop.\nWe will also be using Slack for additional support during the training. Please have these installed in advance. We will have a channel on Slack dedicated to software/hardware issues and troubleshooting.\n\n\n\nWe are assuming familiarity with R basics as well as at least introductory statistics, including up through simple linear regression. If you would like materials to review, we recommend that you do the following:\nReview of R\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter.\nIn addition / alternatively to pre-work element (1), here are some resources for brushing up on R at the end of the Intro R Chapter you can try. There are many more resources online (e.g., this and this ) – pick something that suits your learning style.\n\nReview of statistics\n\nReview background on introductory probability and statistics (solutions to exercises). You can also use the resources on The Multilingual Quantitative Biologist - Basic Data Analyses and Statistics through into linear models.\nReview the conceptual basics of Simple Linear Regression. We’ll refresh fitting linear models in R as well as transformations and diagnostic plots during the workshop."
  },
  {
    "objectID": "materials_temp.html#hardware-and-software",
    "href": "materials_temp.html#hardware-and-software",
    "title": "VectorByte Training Materials 2024",
    "section": "",
    "text": "We will be using R for all data manipulation and analyses/model fitting. Any operating system (Windows, Mac, Linux) will do, as long as you have R (version 3.6 or higher) installed.\nYou may use any IDE/ GUI for R (VScode, RStudio, Emacs, etc). For most people, RStudio is a good option. Whichever one you decide to use, please make sure it is installed and test it before the workshop.\nWe will also be using Slack for additional support during the training. Please have these installed in advance. We will have a channel on Slack dedicated to software/hardware issues and troubleshooting."
  },
  {
    "objectID": "materials_temp.html#pre-requisites",
    "href": "materials_temp.html#pre-requisites",
    "title": "VectorByte Training Materials 2024",
    "section": "",
    "text": "We are assuming familiarity with R basics as well as at least introductory statistics, including up through simple linear regression. If you would like materials to review, we recommend that you do the following:\nReview of R\n\nGo to The Multilingual Quantitative Biologist, and read+work through the Biological Computing in R Chapter.\nIn addition / alternatively to pre-work element (1), here are some resources for brushing up on R at the end of the Intro R Chapter you can try. There are many more resources online (e.g., this and this ) – pick something that suits your learning style.\n\nReview of statistics\n\nReview background on introductory probability and statistics (solutions to exercises). You can also use the resources on The Multilingual Quantitative Biologist - Basic Data Analyses and Statistics through into linear models.\nReview the conceptual basics of Simple Linear Regression. We’ll refresh fitting linear models in R as well as transformations and diagnostic plots during the workshop."
  },
  {
    "objectID": "materials_temp.html#introduction-to-the-workshop-and-time-dependent-abundance-data",
    "href": "materials_temp.html#introduction-to-the-workshop-and-time-dependent-abundance-data",
    "title": "VectorByte Training Materials 2024",
    "section": "Introduction to the Workshop and Time Dependent Abundance Data",
    "text": "Introduction to the Workshop and Time Dependent Abundance Data\n\nLecture (coming soon)"
  },
  {
    "objectID": "materials_temp.html#regression-in-r-refresher-transformations-and-diagnostics",
    "href": "materials_temp.html#regression-in-r-refresher-transformations-and-diagnostics",
    "title": "VectorByte Training Materials 2024",
    "section": "Regression in R Refresher (Transformations and Diagnostics)",
    "text": "Regression in R Refresher (Transformations and Diagnostics)\n\nLecture Slides\nPractical\nDataset: transforms.csv"
  },
  {
    "objectID": "materials_temp.html#time-dependent-regression-analysis",
    "href": "materials_temp.html#time-dependent-regression-analysis",
    "title": "VectorByte Training Materials 2024",
    "section": "Time dependent regression analysis",
    "text": "Time dependent regression analysis\n\nLecture Slides\nPractical\nDataset: Walton Co, FL Mosquito data"
  },
  {
    "objectID": "materials_temp.html#basics-of-time-series-using-r",
    "href": "materials_temp.html#basics-of-time-series-using-r",
    "title": "VectorByte Training Materials 2024",
    "section": "Basics of Time Series using R",
    "text": "Basics of Time Series using R\n\nIntegrated Lecture and Activities (coming soon)\nDataset: coming soon"
  },
  {
    "objectID": "materials_temp.html#introduction-to-the-vecdyn-database",
    "href": "materials_temp.html#introduction-to-the-vecdyn-database",
    "title": "VectorByte Training Materials 2024",
    "section": "Introduction to the VecDyn database",
    "text": "Introduction to the VecDyn database\n\nThe VecDyn website.\nAbout the VecDyn API\nOther Materials Coming Soon."
  },
  {
    "objectID": "materials_temp.html#gaussian-process-models-gps-for-time-dependent-data",
    "href": "materials_temp.html#gaussian-process-models-gps-for-time-dependent-data",
    "title": "VectorByte Training Materials 2024",
    "section": "Gaussian Process models (GPs) for Time Dependent Data",
    "text": "Gaussian Process models (GPs) for Time Dependent Data\n\nLecture Slides (coming soon)\nLecture Note (coming soon)\nPractical (coming soon)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the 2024 VectorByte Training Workshop",
    "section": "",
    "text": "Check out our about and schedule: coming soon! pages to continue.\nInformation about pre-workshop preparation – including software installation, expectations for what you should already be familiar with, and review materials – is available in the pre-work portion of the materials page."
  },
  {
    "objectID": "VB_RegDiagTrans.html#learning-objectives",
    "href": "VB_RegDiagTrans.html#learning-objectives",
    "title": "VectorByte Methods Training",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nReview assumptions of SLR/MLR models\nReview using diagnostic plots to assess whether assumptions are met\nReview the idea of basic transformations to use when assumptions aren’t met"
  },
  {
    "objectID": "VB_RegDiagTrans.html#slr-model-assumptions",
    "href": "VB_RegDiagTrans.html#slr-model-assumptions",
    "title": "VectorByte Methods Training",
    "section": "SLR model assumptions",
    "text": "SLR model assumptions\n\\[\nY_i |X_i \\stackrel{ind}{\\sim} \\mathcal{N}(\\beta_0 + \\beta_1 X_i, \\sigma^2)\n\\]\nRecall the key assumptions of the Simple Linear Regression model:\n\nThe conditional mean of \\(Y\\) is linear in \\(X\\).\nThe additive errors (deviations from line)\n\nare normally distributed\nindependent from each other\nidentically distributed (i.e., they have constant variance)"
  },
  {
    "objectID": "VB_RegDiagTrans.html#example-model-violations",
    "href": "VB_RegDiagTrans.html#example-model-violations",
    "title": "VectorByte Methods Training",
    "section": "Example model violations",
    "text": "Example model violations\nAnscombe’s quartet comprises four datasets that have similar statistical properties …\n\n\n\n\nXmean\nYmean\nXsd\nYsd\nXYcor\n\n\n\n\n1\n9.000\n7.501\n3.317\n2.032\n0.816\n\n\n2\n9.000\n7.501\n3.317\n2.032\n0.816\n\n\n3\n9.000\n7.500\n3.317\n2.030\n0.816\n\n\n4\n9.000\n7.501\n3.317\n2.031\n0.817"
  },
  {
    "objectID": "VB_RegDiagTrans.html#residuals-and-the-model-assumptions",
    "href": "VB_RegDiagTrans.html#residuals-and-the-model-assumptions",
    "title": "VectorByte Methods Training",
    "section": "Residuals and the model assumptions",
    "text": "Residuals and the model assumptions\nRecall that the linear regression model assumes \\[\nY_i =\\beta_0 + \\beta_1 X_i + \\varepsilon_i,~~\\mbox{where}~~\n\\varepsilon_i \\stackrel{iid}{\\sim} \\mathcal{N}(0,\\sigma^2).\n\\]\nOur goal is to determine if the “true” residuals are iid normal and unrelated to \\(X\\). If the SLR model assumptions are true, then the residuals must be just “white noise”:\n\nEach \\(\\varepsilon_i\\) has the same variance (\\(\\sigma^2\\)).\nEach \\(\\varepsilon_i\\) has the same mean (0).\nAll of the \\(\\varepsilon_i\\) have the same normal distribution."
  },
  {
    "objectID": "VB_RegDiagTrans.html#understanding-leverage",
    "href": "VB_RegDiagTrans.html#understanding-leverage",
    "title": "VectorByte Methods Training",
    "section": "Understanding Leverage",
    "text": "Understanding Leverage\nThe \\(h_i\\) leverage term measures sensitivity of the estimated least squares regression line to changes in \\(Y_i\\).\nThe term “leverage” provides a mechanical intuition:\n\nThe farther you are from a pivot joint, the more torque you have pulling on a lever.\n\nHere is a nice online (interactive) illustration of leverage:\n\nhttps://omaymas.shinyapps.io/Influence_Analysis/\n\n\nOutliers do more damage if they have high leverage!"
  },
  {
    "objectID": "VB_RegDiagTrans.html#standardized-residuals",
    "href": "VB_RegDiagTrans.html#standardized-residuals",
    "title": "VectorByte Methods Training",
    "section": "Standardized residuals",
    "text": "Standardized residuals\nSince \\(e_i \\sim N(0, \\sigma^2 [1-h_i])\\), we know that \\[\n\\color{red}{\\frac{e_i}{\\sigma \\sqrt{1-h_i} }\\sim N(0, 1)}.\n\\]\nThese transformed \\(e_i\\)’s are called the standardized residuals.\n\nThey all have the same distribution if the SLR model assumptions are true.\nThey are almost (close enough) independent (\\(\\stackrel{iid}{\\sim}N(0,1)\\)).\nEstimate \\(\\sigma^2 \\approx s^2 = \\frac{1}{n-p}\\sum_{j=1}^n e_j^2\\). (\\(p=2\\) for SLR)"
  },
  {
    "objectID": "VB_RegDiagTrans.html#studentized-residuals",
    "href": "VB_RegDiagTrans.html#studentized-residuals",
    "title": "VectorByte Methods Training",
    "section": "Studentized residuals",
    "text": "Studentized residuals\nWe thus define a standard Studentized residual as \\[\nr_i = \\frac{e_i}{s_{-i} \\sqrt{1-h_i} }\\sim t_{n-p-1}(0, 1)\n\\] where \\(s_{-i}^2 = \\frac{1}{n-p-1}\\sum_{j \\neq i} e_j^2\\) is \\(\\hat{\\sigma~}^2\\) calculated without \\(e_i\\).\n\nThese are easy to get in R with the rstudent() function:\n\nas.numeric(rstudent(reg3))\n#&gt;  [1]   -0.43905545   -0.18550224 1203.53946383   -0.31384418   -0.57429485\n#&gt;  [6]   -1.15598185    0.06640743    0.36185145   -0.73567703   -0.06576806\n#&gt; [11]    0.20026336"
  },
  {
    "objectID": "VB_RegDiagTrans.html#outliers-and-studentized-residuals",
    "href": "VB_RegDiagTrans.html#outliers-and-studentized-residuals",
    "title": "VectorByte Methods Training",
    "section": "Outliers and Studentized residuals",
    "text": "Outliers and Studentized residuals\nSince the studentized residuals are distributed \\(t_{n-p-1}(0,1)\\), we should be concerned about any \\(r_i\\) outside of about \\([-2.5, 2.5]\\).\n\n (Note: As \\(n\\) gets much bigger, we will expect to see some very rare events (big $\u000barepsilon_i$) and not get worried unless \\(|r_i| &gt; 3\\) or \\(4\\).)"
  },
  {
    "objectID": "VB_RegDiagTrans.html#how-to-deal-with-outliers",
    "href": "VB_RegDiagTrans.html#how-to-deal-with-outliers",
    "title": "VectorByte Methods Training",
    "section": "How to deal with outliers",
    "text": "How to deal with outliers\n\n\n\n\nfrom Research Wahlberg"
  },
  {
    "objectID": "VB_RegDiagTrans.html#how-to-deal-with-outliers-1",
    "href": "VB_RegDiagTrans.html#how-to-deal-with-outliers-1",
    "title": "VectorByte Methods Training",
    "section": "How to deal with outliers",
    "text": "How to deal with outliers\nWhen should you delete outliers?\n\nOnly when you have a really good reason!\n\nThere is nothing wrong with running a regression with and without potential outliers to see whether results are significantly impacted.\nAny time outliers are dropped, the reasons for doing so should be clearly noted.\n\nI maintain that both a statistical and a non-statistical reason are required."
  },
  {
    "objectID": "VB_RegDiagTrans.html#outliers-leverage-and-residuals",
    "href": "VB_RegDiagTrans.html#outliers-leverage-and-residuals",
    "title": "VectorByte Methods Training",
    "section": "Outliers, leverage, and residuals",
    "text": "Outliers, leverage, and residuals\nWarning: Unfortunately, outliers with high leverage are hard to catch through \\(\\color{dodgerblue}{r_i}\\) (since the line is pulled towards them).\nMeans get distracted by outliers…"
  },
  {
    "objectID": "VB_RegDiagTrans.html#outliers-leverage-and-residuals-1",
    "href": "VB_RegDiagTrans.html#outliers-leverage-and-residuals-1",
    "title": "VectorByte Methods Training",
    "section": "Outliers, leverage, and residuals",
    "text": "Outliers, leverage, and residuals\nWarning: Unfortunately, outliers with high leverage are hard to catch through \\(\\color{dodgerblue}{r_i}\\) (since the line is pulled towards them).\nConsider data on house Rents vs SqFt:\n\nPlots of \\(r_i\\) or \\(e_i\\) against \\(\\hat{Y~}_i\\) or \\(X_i\\) are still your best diagnostic!"
  },
  {
    "objectID": "VB_RegDiagTrans.html#normality-and-studentized-residuals",
    "href": "VB_RegDiagTrans.html#normality-and-studentized-residuals",
    "title": "VectorByte Methods Training",
    "section": "Normality and studentized residuals",
    "text": "Normality and studentized residuals\nA more subtle issue is the normality of the distribution on \\(\\varepsilon\\).\n\nWe can look at the residuals to judge normality if \\(n\\) is big enough (say \\(&gt;20~~ \\rightarrow\\) less than that makes it too hard to call).\n\nIn particular, if we have decent size \\(\\color{red}{n}\\), we want the shape of the studentized residual distribution to “look” like \\(\\color{red}{N(0,1)}\\).\n The most obvious tactic is to look at a histogram of \\(r_i\\)."
  },
  {
    "objectID": "VB_RegDiagTrans.html#assessing-normality-via-q-q-plots",
    "href": "VB_RegDiagTrans.html#assessing-normality-via-q-q-plots",
    "title": "VectorByte Methods Training",
    "section": "Assessing normality via Q-Q plots",
    "text": "Assessing normality via Q-Q plots\nHigher fidelity diagnostics are provided by normal quantile-quantile (Q-Q) plots that:\n\nplot the sample quantiles (e.g. \\(10^{th}\\) percentile, etc.)\nagainst true percentiles from a \\(N(0,1)\\) distribution (e.g. \\(-1.96\\) is the true 2.5% quantile).\n\nIf \\(r_i \\sim N(0,1)\\) these quantiles should be equal\n\nlie on a line through 0 with slope 1"
  },
  {
    "objectID": "VB_RegDiagTrans.html#go-to-diagnostic-plots",
    "href": "VB_RegDiagTrans.html#go-to-diagnostic-plots",
    "title": "VectorByte Methods Training",
    "section": "3 Go-To Diagnostic Plots",
    "text": "3 Go-To Diagnostic Plots"
  },
  {
    "objectID": "VB_RegDiagTrans.html#violations-of-slr-model-assumptions",
    "href": "VB_RegDiagTrans.html#violations-of-slr-model-assumptions",
    "title": "VectorByte Methods Training",
    "section": "Violations of SLR Model Assumptions",
    "text": "Violations of SLR Model Assumptions\n\\[\\color{dodgerblue}{Y_i |X_i \\stackrel{ind}{\\sim} \\mathcal{N}(\\beta_0 + \\beta_1 X_i, \\sigma^2)}\\]\n\nThe conditional mean of \\(Y\\) is linear in \\(X\\).\nThe additive errors (deviations from line)\n\nare normally distributed\nindependent from each other\nidentically distributed (i.e., they have constant variance)\n\n\nAll of these can be violated! Let’s see what violations look like and how we can deal with them within the SLR framework."
  },
  {
    "objectID": "VB_RegDiagTrans.html#violation-1-non-constant-variance",
    "href": "VB_RegDiagTrans.html#violation-1-non-constant-variance",
    "title": "VectorByte Methods Training",
    "section": "Violation 1: Non-constant variance",
    "text": "Violation 1: Non-constant variance\nIf you get a trumpet shape (bunching of the \\(Y\\)s), you have nonconstant variance.\n\nThis violates our assumption that all \\(\\varepsilon_i\\) have the same \\(\\sigma^2\\)."
  },
  {
    "objectID": "VB_RegDiagTrans.html#solution-1-variance-stabilizing-transformations",
    "href": "VB_RegDiagTrans.html#solution-1-variance-stabilizing-transformations",
    "title": "VectorByte Methods Training",
    "section": "Solution 1: Variance stabilizing transformations",
    "text": "Solution 1: Variance stabilizing transformations\nThis is one of the most common model violations; luckily, it is usually fixable by transforming the response (\\(Y\\)) variable.\n\\(\\color{dodgerblue}{\\log(Y)}\\) is the most common variance stabilizing transform.\n\nIf \\(Y\\) has only positive values (e.g. sales) or is a count (e.g. # of customers), take \\(\\log(Y)\\) (always natural log).\n\n\\(\\color{dodgerblue}{\\sqrt{Y}}\\) is sometimes used, especially if the data have zeros.\n\nIn general, think what you expect to be linear for your data."
  },
  {
    "objectID": "VB_RegDiagTrans.html#violation-2-nonlinear-residual-patterns",
    "href": "VB_RegDiagTrans.html#violation-2-nonlinear-residual-patterns",
    "title": "VectorByte Methods Training",
    "section": "Violation 2: Nonlinear residual patterns",
    "text": "Violation 2: Nonlinear residual patterns\nConsider regression residuals for the 2nd Anscombe dataset:\n\nThings are not good! It appears that we do not have a linear mean function; that is \\(\\color{dodgerblue}{\\mathbb{E}[Y] \\neq \\beta_0 + \\beta_1 X}\\)."
  },
  {
    "objectID": "VB_RegDiagTrans.html#solution-2-polynomial-regression",
    "href": "VB_RegDiagTrans.html#solution-2-polynomial-regression",
    "title": "VectorByte Methods Training",
    "section": "Solution 2: Polynomial regression",
    "text": "Solution 2: Polynomial regression\nEven though we are limited to a linear mean, it is possible to get nonlinear regression by transforming the \\(X\\) variable.\n\nIn general, we can add powers of \\(\\color{dodgerblue}X\\) to get polynomial regression: \\(\\color{red}{\\mathbb{E}[Y] = \\beta_0 + \\beta_1X + \\beta_2 X^2 + \\cdots + \\beta_m X^m}\\)\n\nYou can fit any mean function if \\(m\\) is big enough.\n\nUsually stick to m=2 unless you have a good reason."
  },
  {
    "objectID": "VB_RegDiagTrans.html#testing-for-nonlinearity",
    "href": "VB_RegDiagTrans.html#testing-for-nonlinearity",
    "title": "VectorByte Methods Training",
    "section": "Testing for nonlinearity",
    "text": "Testing for nonlinearity\nTo see if you need more nonlinearity, try the regression which includes the next polynomial term, and see if it is significant.\nFor example, to see if you need a quadratic term,\n\nfit the model then run the regression \\(\\mathbb{E}[Y] = \\beta_0 + \\beta_1 X + \\beta_2 X^2\\).\nIf your test implies \\(\\color{dodgerblue}{\\beta_2 \\neq 0}\\), you need \\(\\color{dodgerblue}{X^2}\\) in your model.\n\nNote: \\(p\\)-values are calculated “given the other \\(\\beta\\)’s are nonzero”; i.e., conditional on \\(X\\) being in the model."
  },
  {
    "objectID": "VB_RegDiagTrans.html#closing-comments-on-polynomials",
    "href": "VB_RegDiagTrans.html#closing-comments-on-polynomials",
    "title": "VectorByte Methods Training",
    "section": "Closing comments on polynomials",
    "text": "Closing comments on polynomials\n\nWe can always add higher powers (cubic, etc.) if necessary.\n\nIf you add a higher order term, the lower order term is kept in the model regardless of its individual \\(t\\)-stat.\n\nBe very careful about predicting outside the data range as the curve may do unintended things beyond the data.\nWatch out for over-fitting.\n\nYou can get a “perfect” fit with enough polynomial terms,\nbut that doesn’t mean it will be any good for prediction or understanding."
  },
  {
    "objectID": "VB_RegDiagTrans.html#other-problems",
    "href": "VB_RegDiagTrans.html#other-problems",
    "title": "VectorByte Methods Training",
    "section": "Other problems",
    "text": "Other problems\nSometimes we have other strange things going on in our data sets\n\ndata are “clumped” up in \\(X\\) – high leverage points\nresiduals still aren’t normally distributed after taking transforms from earlier\nresponses take discrete values instead of continuous\n\n\nThe latter 2 we can deal with using MLR and GLMs. What about the first?"
  },
  {
    "objectID": "VB_RegDiagTrans.html#the-log-log-model",
    "href": "VB_RegDiagTrans.html#the-log-log-model",
    "title": "VectorByte Methods Training",
    "section": "The log-log model",
    "text": "The log-log model\nThe other common covariate transform is \\(\\log(X)\\).\n\nWhen \\(X\\)-values are bunched up, \\(\\log(X)\\) helps spread them out and reduces the leverage of extreme values.\nRecall that both reduce \\(s_{b_1}\\).\n\nIn practice, this is often used in conjunction with a \\(\\log(Y)\\) response transformation. The log-log model is \\[\n    \\color{red}{\\log(Y) = \\beta_0 + \\beta_1 \\log(X) + \\varepsilon}.\n    \\]\nIt is super useful, and has some special properties …"
  },
  {
    "objectID": "VB_RegDiagTrans.html#elasticity-and-the-log-log-model",
    "href": "VB_RegDiagTrans.html#elasticity-and-the-log-log-model",
    "title": "VectorByte Methods Training",
    "section": "Elasticity and the log-log model",
    "text": "Elasticity and the log-log model\nIn a log-log model, the slope \\(\\beta_1\\) is sometimes called elasticity.\nThe elasticity is (roughly) % change in \\(Y\\) per 1% change in \\(X\\). \\[\\color{dodgerblue}{\n\\beta_1 \\approx \\frac{d\\%Y}{d\\%X}}\\] For example, economists often assume that GDP has import elasticity of 1. Indeed:\n\nGDPlm&lt;-lm(log(GDP) ~ log(IMPORTS))\ncoef(GDPlm)\n#&gt;  (Intercept) log(IMPORTS) \n#&gt;     1.891516     0.969337\n\n\n(Can we test for 1%?)"
  },
  {
    "objectID": "VB_RegDiagTrans.html#practical",
    "href": "VB_RegDiagTrans.html#practical",
    "title": "VectorByte Methods Training",
    "section": "Practical",
    "text": "Practical\nNext we’ll do a short practical to practice:\n\nFitting linear models in R\nChecking diagnostics\nChoosing transformations\nPlotting predictions"
  },
  {
    "objectID": "VB_RegDiagTrans_practical_soln.html#fit-the-linear-regression-model.-plot-the-data-and-fitted-line.",
    "href": "VB_RegDiagTrans_practical_soln.html#fit-the-linear-regression-model.-plot-the-data-and-fitted-line.",
    "title": "VectorByte Methods Training",
    "section": "1. Fit the linear regression model. Plot the data and fitted line.",
    "text": "1. Fit the linear regression model. Plot the data and fitted line.\n\n## fit models\nattach(D &lt;- read.csv(\"data/transforms.csv\"))\nlm1 &lt;- lm(Y1 ~ X1)\nlm2 &lt;- lm(Y2 ~ X2)\nlm3 &lt;- lm(Y3 ~ X3)\nlm4 &lt;- lm(Y4 ~ X4)\n\n## plot points and lines\npar(mfrow=c(2,2), mar=c(3,2,2,1))\nplot(X1, Y1, col=1, main=\"I\"); abline(lm1, col=1)\nplot(X2, Y2, col=2, main=\"II\"); abline(lm2, col=2)\nplot(X3, Y3, col=3, main=\"III\"); abline(lm3, col=3)\nplot(X4, Y4, col=4, main=\"IV\"); abline(lm4, col=4)"
  },
  {
    "objectID": "VB_RegDiagTrans_practical_soln.html#provide-a-scatterplot-normal-q-q-plot-and-histogram-for-the-studentized-regression-residuals.",
    "href": "VB_RegDiagTrans_practical_soln.html#provide-a-scatterplot-normal-q-q-plot-and-histogram-for-the-studentized-regression-residuals.",
    "title": "VectorByte Methods Training",
    "section": "2. Provide a scatterplot, normal Q-Q plot, and histogram for the studentized regression residuals.",
    "text": "2. Provide a scatterplot, normal Q-Q plot, and histogram for the studentized regression residuals.\n\npar(mfrow=c(3,4), mar=c(4,4,2,0.5))   # you might have to make \n                                      # the plot window big to \n                                      # fit everything\nplot(lm1$fitted, rstudent(lm1), col=1,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"I\")\nplot(lm2$fitted, rstudent(lm2), col=2,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"II\")\nplot(lm3$fitted, rstudent(lm3), col=3,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"III\")\nplot(lm4$fitted, rstudent(lm4), col=4,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"IV\")\n\nqqnorm(rstudent(lm1), pch=20, col=1, main=\"\" )\nabline(a=0,b=1,lty=2)\nqqnorm(rstudent(lm2), pch=20, col=2, main=\"\" )\nabline(a=0,b=1,lty=2)\nqqnorm(rstudent(lm3), pch=20, col=3, main=\"\" )\nabline(a=0,b=1,lty=2)\nqqnorm(rstudent(lm4), pch=20, col=4, main=\"\" )\nabline(a=0,b=1,lty=2)\n\nhist(rstudent(lm1), col=1, xlab=\"Studentized Residuals\", \n     main=\"\", border=8)\nhist(rstudent(lm2), col=2, xlab=\"Studentized Residuals\", main=\"\")\nhist(rstudent(lm3), col=3, xlab=\"Studentized Residuals\", main=\"\")\nhist(rstudent(lm4), col=4, xlab=\"Studentized Residuals\", main=\"\")"
  },
  {
    "objectID": "VB_RegDiagTrans_practical_soln.html#using-the-residual-scatterplots-state-how-the-slr-model-assumptions-are-violated.",
    "href": "VB_RegDiagTrans_practical_soln.html#using-the-residual-scatterplots-state-how-the-slr-model-assumptions-are-violated.",
    "title": "VectorByte Methods Training",
    "section": "3. Using the residual scatterplots, state how the SLR model assumptions are violated.",
    "text": "3. Using the residual scatterplots, state how the SLR model assumptions are violated.\nSet 1: Xs are clumpy AND the variance seems non-constant. It looks a lot like the GDP data from class. Since both Xs and Ys are strictly positive, we can try a log-log transform.\nSet 2: Data have non-constant variance – should probably log transform the Ys\nSet 3: Data have an underlying non-linear pattern. Add in an x^2 and x^3 term in this case.\nSet 4: X values are very clumpy and all positive. Try log transform of the Xs"
  },
  {
    "objectID": "VB_RegDiagTrans_practical_soln.html#determine-the-data-transformation-to-correct-the-problems-in-3-fit-the-corresponding-regression-model-and-plot-the-transformed-data-with-new-fitted-line.",
    "href": "VB_RegDiagTrans_practical_soln.html#determine-the-data-transformation-to-correct-the-problems-in-3-fit-the-corresponding-regression-model-and-plot-the-transformed-data-with-new-fitted-line.",
    "title": "VectorByte Methods Training",
    "section": "4. Determine the data transformation to correct the problems in 3, fit the corresponding regression model, and plot the transformed data with new fitted line.",
    "text": "4. Determine the data transformation to correct the problems in 3, fit the corresponding regression model, and plot the transformed data with new fitted line.\n\n### the fixes are as follows:\nlogX1&lt;- log(X1)\nlogY1 &lt;- log(Y1)\nlogY2 &lt;- log(Y2)\nX3sq &lt;- X3^2\nX3cube&lt;-X3^3\nlogX4 &lt;- log(X4)\n\n\n### re-run the regressions and residual plots to show this worked\nlm1 &lt;- lm(logY1 ~ logX1)\nlm2 &lt;- lm(logY2 ~ X2)\nlm3 &lt;- lm(Y3 ~ X3+ X3sq + X3cube)\nlm4 &lt;- lm(Y4 ~ logX4)\n\n## plot points and lines\npar(mfrow=c(2,2), mar=c(3,2,2,1))\nplot(logX1, logY1, col=1, main=\"I\"); abline(lm1, col=1)\nplot(X2, logY2, col=2, main=\"II\"); abline(lm2, col=2)\nplot(X3, Y3, col=3, main=\"III\")\nxx3 &lt;- seq(min(X3), max(X3), length=1000)\nlines(xx3, lm3$coef[1] + lm3$coef[2]*xx3 + \n        lm3$coef[3]*xx3^2+lm3$coef[3]*xx3^3, col=3)\nplot(logX4, Y4, col=4, main=\"IV\"); abline(lm4, col=4)"
  },
  {
    "objectID": "VB_RegDiagTrans_practical_soln.html#provide-plots-to-show-that-your-transformations-have-mostly-fixed-the-model-violations.",
    "href": "VB_RegDiagTrans_practical_soln.html#provide-plots-to-show-that-your-transformations-have-mostly-fixed-the-model-violations.",
    "title": "VectorByte Methods Training",
    "section": "5. Provide plots to show that your transformations have (mostly) fixed the model violations.",
    "text": "5. Provide plots to show that your transformations have (mostly) fixed the model violations.\n\npar(mfrow=c(3,4), mar=c(4,4,2,0.5))  \nplot(lm1$fitted, rstudent(lm1), col=1,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"I\")\nplot(lm2$fitted, rstudent(lm2), col=2,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"II\")\nplot(lm3$fitted, rstudent(lm3), col=3,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"III\")\nplot(lm4$fitted, rstudent(lm4), col=4,\n     xlab=\"Fitted Values\", ylab=\"Studentized Residuals\", \n     pch=20, main=\"IV\")\n\n## Q-Q plots\nqqnorm(rstudent(lm1), pch=20, col=1, main=\"\" )\nabline(a=0,b=1,lty=2)\nqqnorm(rstudent(lm2), pch=20, col=2, main=\"\" )\nabline(a=0,b=1,lty=2)\nqqnorm(rstudent(lm3), pch=20, col=3, main=\"\" )\nabline(a=0,b=1,lty=2)\nqqnorm(rstudent(lm4), pch=20, col=4, main=\"\" )\nabline(a=0,b=1,lty=2)\n\n## histograms of studentized residuals\nhist(rstudent(lm1), col=1, xlab=\"Studentized Residuals\", \n     main=\"\", border=8)\nhist(rstudent(lm2), col=2, xlab=\"Studentized Residuals\", main=\"\")\nhist(rstudent(lm3), col=3, xlab=\"Studentized Residuals\", main=\"\")\nhist(rstudent(lm4), col=4, xlab=\"Studentized Residuals\", main=\"\")"
  },
  {
    "objectID": "VB_IntroTimeDepData_practical.html#exploring-the-data",
    "href": "VB_IntroTimeDepData_practical.html#exploring-the-data",
    "title": "VectorByte Methods Training",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nAs always, we first want to take a look at the data, to make sure we understand it, and that we don’t have missing or weird values.\n\nmozData&lt;-read.csv(\"data/Culex_erraticus_walton_covariates_aggregated.csv\")\nsummary(mozData)\n\n   Month_Yr          sample_value        MaxTemp          Precip      \n Length:36          Min.   :0.00000   Min.   :16.02   Min.   : 0.000  \n Class :character   1st Qu.:0.04318   1st Qu.:22.99   1st Qu.: 2.162  \n Mode  :character   Median :0.73001   Median :26.69   Median : 4.606  \n                    Mean   :0.80798   Mean   :26.23   Mean   : 5.595  \n                    3rd Qu.:1.22443   3rd Qu.:30.70   3rd Qu.: 7.864  \n                    Max.   :3.00595   Max.   :33.31   Max.   :18.307  \n\n\nWe can see that the minimum observed average number of mosquitoes it zero, and max is only 3 (there are likely many zeros averaged over many days in the month). There don’t appear to be any NAs in the data. In this case the dataset itself is small enough that we can print the whole thing to ensure it’s complete:\n\nmozData\n\n   Month_Yr sample_value  MaxTemp       Precip\n1   2015-01  0.000000000 17.74602  3.303991888\n2   2015-02  0.018181818 17.87269 16.544265802\n3   2015-03  0.468085106 23.81767  2.405651215\n4   2015-04  1.619047619 26.03559  8.974406168\n5   2015-05  0.821428571 30.01602  0.567960943\n6   2015-06  3.005952381 31.12094  4.841342729\n7   2015-07  2.380952381 32.81130  3.849010353\n8   2015-08  1.826347305 32.56245  5.562845324\n9   2015-09  0.648809524 30.55155 10.409724627\n10  2015-10  0.988023952 27.22605  0.337750269\n11  2015-11  0.737804878 24.86768 18.306749680\n12  2015-12  0.142857143 22.46588  5.621475377\n13  2016-01  0.000000000 16.02406  3.550622029\n14  2016-02  0.020202020 19.42057 11.254680803\n15  2016-03  0.015151515 23.13610  4.785664728\n16  2016-04  0.026143791 24.98082  4.580424519\n17  2016-05  0.025252525 28.72884  0.053057634\n18  2016-06  0.833333333 30.96990  6.155417473\n19  2016-07  1.261363636 33.30509  4.496368193\n20  2016-08  1.685279188 32.09633 11.338749182\n21  2016-09  2.617142857 31.60575  2.868288451\n22  2016-10  1.212121212 29.14275  0.000000000\n23  2016-11  1.539772727 24.48482  0.005462681\n24  2016-12  0.771573604 20.46054 11.615521725\n25  2017-01  0.045454545 18.35473  0.000000000\n26  2017-02  0.036363636 23.65584  3.150710053\n27  2017-03  0.194285714 22.53573  1.430094952\n28  2017-04  0.436548223 26.15299  0.499381616\n29  2017-05  1.202020202 28.00173  6.580562663\n30  2017-06  0.834196891 29.48951 13.333939858\n31  2017-07  1.765363128 32.25135  7.493927035\n32  2017-08  0.744791667 31.86476  6.082113434\n33  2017-09  0.722222222 30.60566  4.631037395\n34  2017-10  0.142131980 27.73453 11.567112214\n35  2017-11  0.289772727 23.23140  1.195760473\n36  2017-12  0.009174312 18.93603  4.018254442"
  },
  {
    "objectID": "VB_IntroTimeDepData_practical.html#plotting-the-data",
    "href": "VB_IntroTimeDepData_practical.html#plotting-the-data",
    "title": "VectorByte Methods Training",
    "section": "Plotting the data",
    "text": "Plotting the data\nFirst we’ll examine the data itself, including the predictors:\n\nmonths&lt;-dim(mozData)[1]\nt&lt;-1:months ## counter for months in the data set\npar(mfrow=c(3,1))\nplot(t, mozData$sample_value, type=\"l\", lwd=2, \n     main=\"Average Monthly Abundance\", \n     xlab =\"Time (months)\", \n     ylab = \"Average Count\")\nplot(t, mozData$MaxTemp, type=\"l\",\n     col = 2, lwd=2, \n     main=\"Average Maximum Temp\", \n     xlab =\"Time (months)\", \n     ylab = \"Temperature (C)\")\nplot(t, mozData$Precip, type=\"l\",\n     col=\"dodgerblue\", lwd=2,\n     main=\"Average Monthly Precip\", \n     xlab =\"Time (months)\", \n     ylab = \"Precipitation (in)\")\n\n\n\n\n\n\n\n\nVisually we noticed that there may be a bit of clumping in the values for abundance (this is subtle) – in particular, since we have a lot of very small/nearly zero counts, a transform, such as a square root, may spread things out for the abundances. It also looks like both the abundance and temperature data are more cyclical than the precipitation, and thus more likely to be related to each other. There’s also not visually a lot of indication of a trend, but it’s usually worthwhile to consider it anyway. Replotting the abundance data with a transformation:\n\nmonths&lt;-dim(mozData)[1]\nt&lt;-1:months ## counter for months in the data set\nplot(t, sqrt(mozData$sample_value), type=\"l\", lwd=2, \n     main=\"Sqrt Average Monthly Abundance\", \n     xlab =\"Time (months)\", \n     ylab = \"Average Count\")\n\n\n\n\n\n\n\n\nThat looks a little bit better. I suggest we go with this for our response."
  },
  {
    "objectID": "VB_IntroTimeDepData_practical.html#building-a-data-frame",
    "href": "VB_IntroTimeDepData_practical.html#building-a-data-frame",
    "title": "VectorByte Methods Training",
    "section": "Building a data frame",
    "text": "Building a data frame\nBefore we get into model building, we always want to build a data frame to contain all of the predictors that we want to consider, at the potential lags that we’re interested in. In the lecture we saw building the AR, sine/cosine, and trend predictors:\n\nt &lt;- 2:months ## to make building the AR1 predictors easier\n\nmozTS &lt;- data.frame(\n  Y=sqrt(mozData$sample_value[t]), # transformed response\n  Yl1=sqrt(mozData$sample_value[t-1]), # AR1 predictor\n  t=t, # trend predictor\n  sin12=sin(2*pi*t/12), \n  cos12=cos(2*pi*t/12) # periodic predictors\n  )\n\nWe will also put in the temperature and precipitation predictors. But we need to think about what might be an appropriate lag. If this were daily or weekly data, we’d probably want to have a fairly sizable lag – mosquitoes take a while to develop, so the number we see today is not likely related to the temperature today. However, since these data are agregated across a whole month, as is the temperature/precipitaion, the current month values are likely to be useful. However, it’s even possible that last month’s values may be so we’ll add those in as well:\n\nmozTS$MaxTemp&lt;-mozData$MaxTemp[t] ## current temps\nmozTS$MaxTempl1&lt;-mozData$MaxTemp[t-1] ## previous temps\nmozTS$Precip&lt;-mozData$Precip[t] ## current precip\nmozTS$Precipl1&lt;-mozData$Precip[t-1] ## previous precip\n\nThus our full dataframe:\n\nsummary(mozTS)\n\n       Y               Yl1               t            sin12         \n Min.   :0.0000   Min.   :0.0000   Min.   : 2.0   Min.   :-1.00000  \n 1st Qu.:0.2951   1st Qu.:0.2951   1st Qu.:10.5   1st Qu.:-0.68301  \n Median :0.8590   Median :0.8590   Median :19.0   Median : 0.00000  \n Mean   :0.7711   Mean   :0.7684   Mean   :19.0   Mean   :-0.01429  \n 3rd Qu.:1.1120   3rd Qu.:1.1120   3rd Qu.:27.5   3rd Qu.: 0.68301  \n Max.   :1.7338   Max.   :1.7338   Max.   :36.0   Max.   : 1.00000  \n     cos12             MaxTemp        MaxTempl1         Precip      \n Min.   :-1.00000   Min.   :16.02   Min.   :16.02   Min.   : 0.000  \n 1st Qu.:-0.68301   1st Qu.:23.18   1st Qu.:23.18   1st Qu.: 1.918  \n Median : 0.00000   Median :27.23   Median :27.23   Median : 4.631  \n Mean   :-0.02474   Mean   :26.47   Mean   :26.44   Mean   : 5.660  \n 3rd Qu.: 0.50000   3rd Qu.:30.79   3rd Qu.:30.79   3rd Qu.: 8.234  \n Max.   : 1.00000   Max.   :33.31   Max.   :33.31   Max.   :18.307  \n    Precipl1     \n Min.   : 0.000  \n 1st Qu.: 1.918  \n Median : 4.631  \n Mean   : 5.640  \n 3rd Qu.: 8.234  \n Max.   :18.307  \n\n\n\nhead(mozTS)\n\n          Y       Yl1 t         sin12         cos12  MaxTemp MaxTempl1\n1 0.1348400 0.0000000 2  8.660254e-01  5.000000e-01 17.87269  17.74602\n2 0.6841675 0.1348400 3  1.000000e+00  6.123234e-17 23.81767  17.87269\n3 1.2724180 0.6841675 4  8.660254e-01 -5.000000e-01 26.03559  23.81767\n4 0.9063270 1.2724180 5  5.000000e-01 -8.660254e-01 30.01602  26.03559\n5 1.7337683 0.9063270 6  1.224647e-16 -1.000000e+00 31.12094  30.01602\n6 1.5430335 1.7337683 7 -5.000000e-01 -8.660254e-01 32.81130  31.12094\n      Precip   Precipl1\n1 16.5442658  3.3039919\n2  2.4056512 16.5442658\n3  8.9744062  2.4056512\n4  0.5679609  8.9744062\n5  4.8413427  0.5679609\n6  3.8490104  4.8413427"
  },
  {
    "objectID": "VB_IntroTimeDepData_practical.html#building-a-first-model",
    "href": "VB_IntroTimeDepData_practical.html#building-a-first-model",
    "title": "VectorByte Methods Training",
    "section": "Building a first model",
    "text": "Building a first model\nWe will first build a very simple model – just a trend – to practice building the model, checking diagnostics, and plotting predictions.\n\nmod1&lt;-lm(Y ~ t, data=mozTS)\nsummary(mod1)\n\n\nCall:\nlm(formula = Y ~ t, data = mozTS)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.81332 -0.47902  0.03671  0.37384  0.87119 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.904809   0.178421   5.071  1.5e-05 ***\nt           -0.007038   0.008292  -0.849    0.402    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4954 on 33 degrees of freedom\nMultiple R-squared:  0.02136,   Adjusted R-squared:  -0.008291 \nF-statistic: 0.7204 on 1 and 33 DF,  p-value: 0.4021\n\n\nThe model output indicates that this model is not useful – the trend is not significant and it only explains about 2% of the variability. Let’s plot the predictions:\n\n## plot points and fitted lines\nplot(Y~t, data=mozTS, col=1, type=\"l\")\nlines(t, mod1$fitted, col=\"dodgerblue\", lwd=2)\n\n\n\n\n\n\n\n\nNot good – we’ll definitely need to try something else! Remember that since we’re using a linear model for this, that we should check our residual plots as usual, and then also plot the acf of the residuals:\n\npar(mfrow=c(1,3), mar=c(4,4,2,0.5))   \n\n## studentized residuals vs fitted\nplot(mod1$fitted, rstudent(mod1), col=1,\n     xlab=\"Fitted Values\", \n     ylab=\"Studentized Residuals\", \n     pch=20, main=\"AR 1 only model\")\n\n## qq plot of studentized residuals\nqqnorm(rstudent(mod1), pch=20, col=1, main=\"\" )\nabline(a=0,b=1,lty=2, col=2)\n\n## histogram of studentized residuals\nhist(rstudent(mod1), col=1, \n     xlab=\"Studentized Residuals\", \n     main=\"\", border=8)\n\n\n\n\n\n\n\n\nThis doesn’t look really bad, although the histogram might be a bit weird. Finally the acf\n\nacf(mod1$residuals)\n\n\n\n\n\n\n\n\nThis is where we can see that we definitely aren’t able to capture the pattern. There’s substantial autocorrelation left at a 1 month lag, and around 6 months.\nFinally, for moving forward, we can extract the BIC for this model so that we can compare with other models that you’ll build next.\n\nn&lt;-length(t)\nextractAIC(mod1, k=log(n))[2]\n\n[1] -44.11057"
  },
  {
    "objectID": "schedule2024.html",
    "href": "schedule2024.html",
    "title": "2024 Training Schedule",
    "section": "",
    "text": "Main materials\n\nPre-workshop\nInformation about pre-workshop preparation – including software installation, expectations for what you should already be familiar with, and review materials – is available in the pre-work portion of the materials page.\n  \n\n\n22 July 2024\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n\nArrival\n\n\n\n\n  \n\n\n23 July 2024 (08:30 - 17:00)\n\n\n\n\n\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n\nComing Soon!\n\n\n\n12:00\nLunch\n\n\n\n\n  \n\n\n24 July 2024 (08:30 - 17:00)\n\n\n\n\n\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n\nComing Soon!\n\n\n\n\n\n\n\n\n12:00\nLunch\n\n\n\n\n\n\n\n\n\n\n\n25 July 2024 (08:30 - 17:00)\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n\nComing Soon!\n\n\n\n\n\n\n26 July 2024\n\n\n\nTime\nActivity\nMaterials\n\n\n\n\n\nTravel\n\n\n\n\n\n\nPost-workshop\nEnjoy using these new techniques and databases!"
  },
  {
    "objectID": "VB_RegDiagTrans_practical.html#fit-the-linear-regression-model.-plot-the-data-and-fitted-line.",
    "href": "VB_RegDiagTrans_practical.html#fit-the-linear-regression-model.-plot-the-data-and-fitted-line.",
    "title": "VectorByte Methods Training",
    "section": "1. Fit the linear regression model. Plot the data and fitted line.",
    "text": "1. Fit the linear regression model. Plot the data and fitted line.\n\n## fit models\nlm1 &lt;- lm(Y1 ~ X1)\n\n## plot points and fitted lines\nplot(X1, Y1, col=1, main=\"I\"); abline(lm1, col=2)"
  },
  {
    "objectID": "VB_RegDiagTrans_practical.html#provide-a-scatterplot-normal-q-q-plot-and-histogram-for-the-studentized-regression-residuals.",
    "href": "VB_RegDiagTrans_practical.html#provide-a-scatterplot-normal-q-q-plot-and-histogram-for-the-studentized-regression-residuals.",
    "title": "VectorByte Methods Training",
    "section": "2. Provide a scatterplot, normal Q-Q plot, and histogram for the studentized regression residuals.",
    "text": "2. Provide a scatterplot, normal Q-Q plot, and histogram for the studentized regression residuals.\n\npar(mfrow=c(1,3), mar=c(4,4,2,0.5))   \n\n## studentized residuals vs fitted\nplot(lm1$fitted, rstudent(lm1), col=1,\n     xlab=\"Fitted Values\", \n     ylab=\"Studentized Residuals\", \n     pch=20, main=\"I\")\n\n## qq plot of studentized residuals\nqqnorm(rstudent(lm1), pch=20, col=1, main=\"\" )\nabline(a=0,b=1,lty=2, col=2)\n\n## histogram of studentized residuals\nhist(rstudent(lm1), col=1, \n     xlab=\"Studentized Residuals\", \n     main=\"\", border=8)"
  },
  {
    "objectID": "VB_RegDiagTrans_practical.html#using-the-residual-scatterplots-state-how-the-slr-model-assumptions-are-violated.",
    "href": "VB_RegDiagTrans_practical.html#using-the-residual-scatterplots-state-how-the-slr-model-assumptions-are-violated.",
    "title": "VectorByte Methods Training",
    "section": "3. Using the residual scatterplots, state how the SLR model assumptions are violated.",
    "text": "3. Using the residual scatterplots, state how the SLR model assumptions are violated.\nXs are clumpy AND the variance seems non-constant. It looks a lot like the GDP data from class. Since both Xs and Ys are strictly positive, we can try a log-log transform."
  },
  {
    "objectID": "VB_RegDiagTrans_practical.html#determine-the-data-transformation-to-correct-the-problems-in-3-fit-the-corresponding-regression-model-and-plot-the-transformed-data-with-new-fitted-line.",
    "href": "VB_RegDiagTrans_practical.html#determine-the-data-transformation-to-correct-the-problems-in-3-fit-the-corresponding-regression-model-and-plot-the-transformed-data-with-new-fitted-line.",
    "title": "VectorByte Methods Training",
    "section": "4. Determine the data transformation to correct the problems in 3, fit the corresponding regression model, and plot the transformed data with new fitted line.",
    "text": "4. Determine the data transformation to correct the problems in 3, fit the corresponding regression model, and plot the transformed data with new fitted line.\n\n### the fix is as follows:\nlogX1&lt;- log(X1)\nlogY1 &lt;- log(Y1)\n\n### re-run the regressions and residual plots to show this worked\nlm1 &lt;- lm(logY1 ~ logX1)\n\n## plot points and lines\nplot(logX1, logY1, col=1, main=\"I\"); abline(lm1, col=2)"
  },
  {
    "objectID": "VB_RegDiagTrans_practical.html#provide-plots-to-show-that-your-transformations-have-mostly-fixed-the-model-violations.",
    "href": "VB_RegDiagTrans_practical.html#provide-plots-to-show-that-your-transformations-have-mostly-fixed-the-model-violations.",
    "title": "VectorByte Methods Training",
    "section": "5. Provide plots to show that your transformations have (mostly) fixed the model violations.",
    "text": "5. Provide plots to show that your transformations have (mostly) fixed the model violations.\n\n## studentized residuals vs fitted\n\npar(mfrow=c(1,3), mar=c(4,4,2,0.5))  \nplot(lm1$fitted, rstudent(lm1), col=1,\n     xlab=\"Fitted Values\", \n     ylab=\"Studentized Residuals\", \n     pch=20, main=\"I\")\n\n## Q-Q plots\nqqnorm(rstudent(lm1), pch=20, col=1, main=\"\" )\nabline(a=0,b=1,lty=2, col=2)\n\n## histograms of studentized residuals\nhist(rstudent(lm1), col=1, \n     xlab=\"Studentized Residuals\", \n     main=\"\", border=8)\n\n\n\n\n\n\n\n\nThis is much better! The histogram still maybe looks a little funny, but given that the qq-plot looks pretty good, I think we’ve made a good transformation."
  },
  {
    "objectID": "VB_RegDiagTrans_practical.html#your-turn",
    "href": "VB_RegDiagTrans_practical.html#your-turn",
    "title": "VectorByte Methods Training",
    "section": "Your Turn!",
    "text": "Your Turn!\nRepeat this process with the other 3 datasets, and see if you can figure out a appropriate transformations for each dataset."
  },
  {
    "objectID": "VB_TimeDepData.html#learning-objectives",
    "href": "VB_TimeDepData.html#learning-objectives",
    "title": "VectorByte Methods Training",
    "section": "Learning Objectives",
    "text": "Learning Objectives"
  },
  {
    "objectID": "VB_TimeDepData.html#time-dependent-data",
    "href": "VB_TimeDepData.html#time-dependent-data",
    "title": "VectorByte Methods Training",
    "section": "Time Dependent Data",
    "text": "Time Dependent Data\nMuch of the data we collect in VBD applications depends on time such as\n\nobserved cases in a city or country\nnumber of mosquitoes over time\n\nAdditionally, we often assume that the reasons these change over time may be because covariates (e.g., temperature, precipitation, insecticide spraying) change over time.\nHow do we incorporate these time varying factors into our regression models?"
  },
  {
    "objectID": "VB_TimeDepData.html#types-of-time-dependent-data",
    "href": "VB_TimeDepData.html#types-of-time-dependent-data",
    "title": "VectorByte Methods Training",
    "section": "Types of time dependent data",
    "text": "Types of time dependent data\nThe most common type of time-dependent data that statisticians talk about is time series data. These are data where observations are evenly spaced with no (or very little) missing observations.\n\nAlthough evenly spaced data are ideal (and the most common methods are designed for them), in VBD survey data we often don’t have evenly spaced observations. These data don’t have a specific name, and most time-series methods can’t be directly used with them."
  },
  {
    "objectID": "VB_TimeDepData.html#time-series-data-and-dependence",
    "href": "VB_TimeDepData.html#time-series-data-and-dependence",
    "title": "VectorByte Methods Training",
    "section": "Time series data and dependence",
    "text": "Time series data and dependence\nTime-series data are simply a collection of observations gathered over time. For example, suppose \\(y_1, \\ldots, y_T\\) are\n\ndaily temperature,\nsolar activity,\nCO\\(_2\\) levels,\nyearly population size.\n\nIn each case, we might expect what happens at time \\(t\\) to be correlated with time \\(t-1\\)."
  },
  {
    "objectID": "VB_TimeDepData.html#checking-for-dependence",
    "href": "VB_TimeDepData.html#checking-for-dependence",
    "title": "VectorByte Methods Training",
    "section": "Checking for dependence",
    "text": "Checking for dependence\nTo see if \\(Y_{t-1}\\) would be useful for predicting \\(Y_t\\), just plot them together and see if there is a relationship.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation between \\(Y_t\\) and \\(Y_{t-1}\\) is called autocorrelation."
  },
  {
    "objectID": "VB_TimeDepData.html#autocorrelation-for-time-series-data",
    "href": "VB_TimeDepData.html#autocorrelation-for-time-series-data",
    "title": "VectorByte Methods Training",
    "section": "Autocorrelation (for time series data)",
    "text": "Autocorrelation (for time series data)\nTo summarize the time-varying dependence, compute lag-\\(\\ell\\) correlations for \\(\\ell=1,2,3,\\ldots\\)\nIn general, the autocorrelation function (ACF) for \\(Y\\) is \\[\\color{red}{r(\\ell) = \\mathrm{cor}(Y_t, Y_{t-\\ell})}\\]\nFor our Roanoke temperature data:\n\nprint(acf(weather$temp))\n\n     0      1      2      3      4      5      6      7      8    \n 1.000  0.658  0.298  0.263  0.297  0.177  0.111  0.008 -0.099   \n    9     10    11     12     13     14     15     16     17 \n-0.045 0.071 -0.020 -0.157 -0.156 -0.146 -0.278 -0.346 -0.314"
  },
  {
    "objectID": "VB_TimeDepData.html#autoregression",
    "href": "VB_TimeDepData.html#autoregression",
    "title": "VectorByte Methods Training",
    "section": "Autoregression",
    "text": "Autoregression\nHow do we model data that exhibits autocorrelation?\nSuppose \\(Y_1 = \\varepsilon_1\\), \\(Y_2 = \\varepsilon_{1} + \\varepsilon_{2}\\), \\(Y_3 = \\varepsilon_{1} + \\varepsilon_{2} + \\varepsilon_{3}\\), etc.\n\nThen \\(\\color{dodgerblue}{Y_t =  \\sum_{i=1}^{t}\\varepsilon_i = Y_{t-1} + \\varepsilon_t}\\) and \\(\\color{dodgerblue}{ \\mathbb{E}[Y_t] = Y_{t-1}}\\).\n\nThis is called a random walk model for \\(Y_t\\):\n\nthe expectation of what will happen is always what happened most recently."
  },
  {
    "objectID": "VB_TimeDepData.html#random-walk",
    "href": "VB_TimeDepData.html#random-walk",
    "title": "VectorByte Methods Training",
    "section": "Random walk",
    "text": "Random walk\nIn a random walk, the series just wanders around.\n\n\\(\\beta_1 = 1\\)"
  },
  {
    "objectID": "VB_TimeDepData.html#exploding-series",
    "href": "VB_TimeDepData.html#exploding-series",
    "title": "VectorByte Methods Training",
    "section": "Exploding series",
    "text": "Exploding series\nFor AR term \\(&gt;1\\), the \\(Y_t\\)’s move exponentially far from \\(Y_1\\).\n\n\\(\\beta_1 = 1.02\\)\n\n\n\nUseless for modeling and prediction."
  },
  {
    "objectID": "VB_TimeDepData.html#stationary-series",
    "href": "VB_TimeDepData.html#stationary-series",
    "title": "VectorByte Methods Training",
    "section": "Stationary series",
    "text": "Stationary series\nFor \\(\\beta_1&lt;1\\), \\(Y_t\\) is always pulled back towards the mean.\n\n\\(\\beta_1 = 0.8\\)\n\n\n\nThese are the most common and useful type of AR series."
  },
  {
    "objectID": "VB_TimeDepData.html#mean-reversion",
    "href": "VB_TimeDepData.html#mean-reversion",
    "title": "VectorByte Methods Training",
    "section": "Mean reversion",
    "text": "Mean reversion\nAn important property of stationary series is mean reversion.\nThink about shifting both \\(Y_t\\) and \\(Y_{t-1}\\) by their mean \\(\\mu\\). \\[\n\\color{dodgerblue}{Y_t - \\mu = \\beta_1 (Y_{t-1} - \\mu) +\\varepsilon_t}\n\\] Since \\(|\\beta_1| &lt; 1\\), \\(Y_t\\) is expected to be closer to \\(\\mu\\) than \\(Y_{t-1}\\).\nMean reversion is all over, and helps predict future behavior:\n\nweekly sales numbers,\ndaily temperature."
  },
  {
    "objectID": "VB_TimeDepData.html#negative-correlation",
    "href": "VB_TimeDepData.html#negative-correlation",
    "title": "VectorByte Methods Training",
    "section": "Negative correlation",
    "text": "Negative correlation\nIt is also possible to have negatively correlated AR(1) series.\n\n\\(\\beta_1 = -0.8\\)\n\n\n\nBut you see these far less often in practice."
  },
  {
    "objectID": "VB_TimeDepData.html#summary-of-ar1-behavior",
    "href": "VB_TimeDepData.html#summary-of-ar1-behavior",
    "title": "VectorByte Methods Training",
    "section": "Summary of AR(1) behavior",
    "text": "Summary of AR(1) behavior\n\n\\(\\color{dodgerblue}{|\\beta_1|&lt;1|}\\): The series has a mean level to which it reverts over time (stationary). For \\(+\\beta_1\\), the series tends to wander above or below the mean level for a while. For \\(-\\beta_1\\), the series tends to flip back and forth around the mean.\n\\(\\color{dodgerblue}{|\\beta_1|=1|}\\): A random walk series. The series has no mean level and, thus, is called nonstationary. The drift parameter \\(\\beta_0\\) is the direction in which the series wanders.\n\\(\\color{dodgerblue}{|\\beta_1|&gt;1|}\\): The series explodes, is nonstationary, and pretty much useless for prediction."
  },
  {
    "objectID": "VB_TimeDepData.html#arp-models",
    "href": "VB_TimeDepData.html#arp-models",
    "title": "VectorByte Methods Training",
    "section": "AR(\\(p\\)) models",
    "text": "AR(\\(p\\)) models\nIt is possible to expand the AR idea to higher lags \\[\n\\color{red}{AR(p): Y_t = \\beta_0 + \\beta_1Y_{t-1} + \\cdots + \\beta_pY_{t-p} + \\varepsilon}.\n\\]\nHowever, it is seldom necessary to fit AR lags for \\(p&gt;1\\).\n\nLike having polynomial terms higher than 2, this just isn’t usually required in practice.\nYou lose all of the stationary/nonstationary intuition.\nOften, the need for higher lags is symptomatic of (missing) a more persistent trend or periodicity in the data, or needing predictors ..."
  },
  {
    "objectID": "VB_TimeDepData.html#trending-series",
    "href": "VB_TimeDepData.html#trending-series",
    "title": "VectorByte Methods Training",
    "section": "Trending series",
    "text": "Trending series\nOften, you’ll have a linear trend in your time series.\n\\(\\Rightarrow\\) AR structure, sloping up or down in time."
  },
  {
    "objectID": "VB_TimeDepData.html#periodic-models",
    "href": "VB_TimeDepData.html#periodic-models",
    "title": "VectorByte Methods Training",
    "section": "Periodic models",
    "text": "Periodic models\nIt is very common to see seasonality or periodicity in series.\n\nTemperature goes up in Summer and down in Winter.\nGas consumption in Blacksburg would do the opposite.\n\nRecall the monthly lung infection data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppears to oscillate on a 12-month cycle."
  },
  {
    "objectID": "VB_TimeDepData.html#alternative-periodicity",
    "href": "VB_TimeDepData.html#alternative-periodicity",
    "title": "VectorByte Methods Training",
    "section": "Alternative Periodicity",
    "text": "Alternative Periodicity\nAn alternative way to add periodicity would be to simply add a dummy variable for each month (feb, mar, apr, ...).\n\nThis achieves basically the same fit as above, without requiring you to add sine or cosine.\nHowever, this takes 11 periodic parameters while we use only 2."
  },
  {
    "objectID": "VB_TimeDepData.html#non-time-series-data",
    "href": "VB_TimeDepData.html#non-time-series-data",
    "title": "VectorByte Methods Training",
    "section": "Non-time series data",
    "text": "Non-time series data\n\nWhat happens if data aren’t evenly sampled?\n\n\nAll of the models/tools we explored that incorporate auto-correlation are not valid if data are not evenly spaced.\n\nYou can’t calculate an auto-correlation if the gap between data points and the earlier points aren’t all the same because we don’t expect all lags to have the same correlation.\n\n\nSo what can we do?"
  },
  {
    "objectID": "VB_TimeDepData.html#time-dependent-predictors",
    "href": "VB_TimeDepData.html#time-dependent-predictors",
    "title": "VectorByte Methods Training",
    "section": "Time Dependent Predictors",
    "text": "Time Dependent Predictors\nOften we have additional measurements of possible covariates that might impact the time-dependent responses that we want to model. E.g. in VBD systems:\n\nweather variables: temperature, rainfall, humidity\nhabitat/climate variables: greenness, ENSO, land use, container densities\nsocio-economic variables: bed net coverage, insecticide spraying\n\nThese may all depend on time, and can be incorporated into a model for all time dependent data (including time series!)."
  },
  {
    "objectID": "VB_TimeDepData.html#time-lagged-predictors",
    "href": "VB_TimeDepData.html#time-lagged-predictors",
    "title": "VectorByte Methods Training",
    "section": "Time-Lagged Predictors",
    "text": "Time-Lagged Predictors\nAdditionally, sometimes there may be a lag between an observed covariate and the response.\n\nExample: The number of people being hospitalized for dengue on a particular day reflect the number of people infected days before, and potentially mosquitoes infected days before that!\nThus, proxies of mosquito abundance, like temperature or humidity, weeks earlier may be appropriate predictors.\n\nHow can we determine an appropriate lag for a predictor?"
  },
  {
    "objectID": "VB_TimeDepData.html#two-strategies",
    "href": "VB_TimeDepData.html#two-strategies",
    "title": "VectorByte Methods Training",
    "section": "Two Strategies",
    "text": "Two Strategies\nThe first is what we might call a scientific approach:\n\nUsing our system knowledge, we can define what might be feasible time lags to include in a model, given evenly sampled predictor data. We decide and include just those a priori lags, and maybe do model/feature selection to narrow down.\n\nThis approach may miss a best lag for time series data, but is often the main way we can try to find appropriate lags for unevenly sampled data.\n(Note, we almost always assume a lag of at least 1.)"
  },
  {
    "objectID": "VB_TimeDepData.html#coming-up",
    "href": "VB_TimeDepData.html#coming-up",
    "title": "VectorByte Methods Training",
    "section": "Coming up!",
    "text": "Coming up!\nThe tools here are good, but not the best:\n\nIn many situations you want to allow for \\(\\beta\\) or \\(\\sigma\\) parameters that can change in time.\nThis can leave us with some left-over autocorrelation.\nWe’ll talk more about more sophisticated models over the next couple of days."
  },
  {
    "objectID": "VB_TimeDepData.html#practice",
    "href": "VB_TimeDepData.html#practice",
    "title": "VectorByte Methods Training",
    "section": "Practice",
    "text": "Practice\nNow we’ll practice combining our regression tools with these additional techniques for time-dependent data.\n\nRemember:\n\nAlso ways check your residual plots to ensure that your assumptions have been met\nTransformations are your friend!\nThink carefully about how to line up your lagged predictors"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the VectorByte Training Materials 2024",
    "section": "",
    "text": "As the VectorByte team has developed these materials, we’ve aimed to provide resources for both guided (during the workshop) and self-led learning. We assume basic familiarity with:\n\nThe R Programming Language\nBasic calculus (especially the mathematical idea of functions)\nBasic probability and statistics (e.g., what is a probability distribution, normal and binomial distributions, means, variances)\nBasics of regression\n\nWe’ve divided the materials into subject matter modules or units. Each module is designed to build on the previous one, and expects at least knowledge of all of the preceding modules in the sequence in addition to the background material.\nEach module consists of four kinds of materials:\n\nslides with presentation of materials\nlabs/hands-on materials to allow you to practice material in a practical way\nsolutions to exercises, when necessary\n\nWe also include links to additional resources/materials/references.\nFor more information about the goals and approach of VectorByte are available at vectorbyte.org."
  },
  {
    "objectID": "VB_RegRev.html",
    "href": "VB_RegRev.html",
    "title": "VectorByte Methods Training",
    "section": "",
    "text": "Main materials"
  },
  {
    "objectID": "VB_RegRev.html#mc-simulation-of-simple-system",
    "href": "VB_RegRev.html#mc-simulation-of-simple-system",
    "title": "VectorByte Methods Training",
    "section": "MC simulation of simple system",
    "text": "MC simulation of simple system"
  },
  {
    "objectID": "VB_RegRev.html#changes-in-population-size",
    "href": "VB_RegRev.html#changes-in-population-size",
    "title": "VectorByte Methods Training",
    "section": "Changes in population size",
    "text": "Changes in population size"
  },
  {
    "objectID": "VB_RegRev.html#sequential-observations",
    "href": "VB_RegRev.html#sequential-observations",
    "title": "VectorByte Methods Training",
    "section": "Sequential observations",
    "text": "Sequential observations\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we had just observed these data, how might we try to estimate parameters?"
  },
  {
    "objectID": "VB_RegRev.html#sampling-distribution-of-ls-line",
    "href": "VB_RegRev.html#sampling-distribution-of-ls-line",
    "title": "VectorByte Methods Training",
    "section": "Sampling distribution of LS line",
    "text": "Sampling distribution of LS line\nWhat did we just do?\n\nWe “imagined” through simulation the sampling distribution of a LS line.\n\nIn real life we get just one data set, and we don’t know the true generating model. But we can still imagine.\n\nWe first find the sampling distribution of our LS coefficients, b_0 and b_1…\n… which requires some review.\n\nIn the online reading and review materials you should have come across some useful probability/stats facts, including:\n\n\\mathbb{E}(X_1+X_2) = \\mathbb{E}(X_1)+ \\mathbb{E}(X_2)\n\\mathbb{E}(cX_1) = c \\mathbb{E}(X_1)\n\\text{var}(c X_1) = c^2\\text{var}(X_1)\n\\text{var}(X_1+X_2) = \\text{var}(X_1)+\\text{var}(X_2) + 2\\text{cov}(X_1 X_2).\n\n\nRecall: distribution of the sample mean\n\nStep back for a moment and consider the mean for an iid sample of n observations of a random variable \\{X_1,\\ldots,X_n\\}.\n\nSuppose that \\mathbb{E}(X_i) = \\mu and \\text{var}(X_i) = \\sigma^2, then\n\n\\mathbb{E}(\\bar{X}) = \\frac{1}{n} \\sum\\mathbb{E}(X_i) = \\mu\n\\text{var}(\\bar{X}) = \\text{var}\\left( \\frac{1}{n} \\sum X_i \\right) = \\frac{1}{n^2} \\sum \\text{var}\\left( X_i \\right) = \\displaystyle \\frac{\\sigma^2}{n}."
  },
  {
    "objectID": "VB_RegRev.html#central-limit-theorem",
    "href": "VB_RegRev.html#central-limit-theorem",
    "title": "VectorByte Methods Training",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe CLT states that for iid random variables, X, with mean \\mu and variance \\sigma^2, the distribution of the sample mean becomes normal as the number of observations, n, gets large.\n\nThat is, \\displaystyle \\bar{X} \\rightarrow_{n} \\mathcal{N}(\\mu, \\sigma^2/n) , and sample averages tend to be normally distributed in large samples.\n\nWe are now ready to describe the sampling distribution of the least squares line …\n… in terms of its effect on the sampling distributions of the coefficients\n\nb_1 = \\hat{\\beta~}_1, the slope of the line\nb_0 = \\hat{\\beta~}_0, the intercept,\nand how they covary together,\n\ngiven a particular (fixed) set of X-values."
  },
  {
    "objectID": "VB_RegRev.html#sampling-distribution-of-b_1",
    "href": "VB_RegRev.html#sampling-distribution-of-b_1",
    "title": "VectorByte Methods Training",
    "section": "Sampling distribution of b_1",
    "text": "Sampling distribution of b_1\nIt turns out that b_1 is normally distributed: b_1 \\sim \\mathcal{N}(\\beta_1, \\sigma^2_{b_1}).\n\nb_1 is unbiased: \\mathbb{E}[b_1] = \\beta_1.\nThe sampling sd \\sigma_{b_1} determines precision of b_1: \n\\sigma_{b_1}^2\n= \\text{var}(b_1) = \\frac{\\sigma^2}{\\sum (X_i - \\bar{X})^2} = \\frac{\\sigma^2}{(n-1)s_x^2}.\n It depends on three factors: 1) sample size (n); 2) error variance (\\sigma^2 = \\sigma_\\varepsilon^2); and 3)X-spread (s_x)."
  },
  {
    "objectID": "VB_RegRev.html#sampling-distribution-of-b_0",
    "href": "VB_RegRev.html#sampling-distribution-of-b_0",
    "title": "VectorByte Methods Training",
    "section": "Sampling Distribution of b_0",
    "text": "Sampling Distribution of b_0\nThe intercept is also normal and unbiased: b_0 \\sim \\mathcal{N}(\\beta_0, \\sigma^2_{b_0}), where \n\\sigma^2_{b_0} = \\text{var}(b_0)  = \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{(n-1)\n    s_x^2} \\right).\n\nWhat is the intuition here? \n\\text{var}(\\bar{Y} - \\bar{X} b_1)\n= \\text{var}(\\bar{Y}) + \\bar{X}^2\\text{var}(b_1) {~-~ 2\\mathrm{cov}(\\bar{Y},b_1) }\n\n\n\\bar{Y} and b_1 are uncorrelated because the slope (b_1) is invariant if you shift the data up or down (\\bar{Y}).\n\n\nOptional Practice Exercise\n\nShow that:\n\n\\mathbb{E}[b_1] = \\beta_1\n\\mathbb{E}[b_0] = \\beta_0\n\\text{var}(b_0) = \\sigma^2 \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{(n-1)  s_x^2} \\right)\n\nWhy is it that b_0 and b_1 are normally distributed?"
  },
  {
    "objectID": "VB_RegRev.html#joint-distribution-of-b_0-and-b_1",
    "href": "VB_RegRev.html#joint-distribution-of-b_0-and-b_1",
    "title": "VectorByte Methods Training",
    "section": "Joint Distribution of b_0 and b_1",
    "text": "Joint Distribution of b_0 and b_1\nWe know that b_0 and b_1 can be dependent, i.e., \n\\mathbb{E}[(b_0 -\\beta_0)(b_1 - \\beta_1)] \\ne 0.\n This means that estimation error in the slope is correlated with the estimation error in the intercept. \n\\mathrm{cov}(b_0,b_1) = -\\sigma^2 \\left(\\frac{\\bar{X}}{(n-1)s_x^2}\\right).\n\n\nInterpretation:\n\nUsually, if the slope estimate is too high, the intercept estimate is too low (negative correlation).\nThe correlation decreases with more X spread (s^2_x)."
  },
  {
    "objectID": "VB_RegRev.html#estimated-variance",
    "href": "VB_RegRev.html#estimated-variance",
    "title": "VectorByte Methods Training",
    "section": "Estimated variance",
    "text": "Estimated variance\nHowever, these formulas aren’t especially practical since they involve an unknown quantity: \\sigma.\n\nSolution: use s, the residual sample standard deviation estimator for \\sigma = \\sigma_\\varepsilon. \ns_{b_1} = \\sqrt{\\frac{s^2}{(n-1)s_x^2}} ~~~\ns_{b_0} = \\sqrt{s^2 \\left(\\frac{1}{n} + \\frac{\\bar{X}^2}{(n-1)s^2_x}\\right)}\n\ns_{b_1} = \\hat{\\sigma~}_{b_1} and s_{b_0} = \\hat{\\sigma~}_{b_0} are estimated coefficient sd’s.\n\n\nInterpretation:\n\nWe now have a notion of standard error for the LS estimates of the slope and intercept.\n\n\nSmall s_b values mean high info/precision/accuracy."
  },
  {
    "objectID": "VB_RegRev.html#normal-and-student-t",
    "href": "VB_RegRev.html#normal-and-student-t",
    "title": "VectorByte Methods Training",
    "section": "Normal and Student-t",
    "text": "Normal and Student-t\nAgain, recall what Student discovered:\nIf \\theta \\sim \\mathcal{N}(\\mu,\\sigma^2), but you estimate \\sigma^2 \\approx s^2 based on n-p degrees of freedom, then \\theta \\sim t_{n-p}(\\mu, s^2).\n\nFor SLR, for example:\n\n\\bar{Y} \\sim t_{n-1}(\\mu, s_y^2/n).\nb_0 \\sim t_{n-2}\\left(\\beta_0, s^2_{b_0}\\right) and b_1 \\sim t_{n-2}\\left(\\beta_1, s^2_{b_1}\\right)\n\n\nWe can use these distributions for drawing conclusions about the parameters via:\n\nConfidence intervals\nHypothesis tests"
  },
  {
    "objectID": "Stats_review.html",
    "href": "Stats_review.html",
    "title": "VectorByte Methods Training",
    "section": "",
    "text": "Main materials\nSolutions to exercises"
  },
  {
    "objectID": "Stats_review.html#some-probability-notation",
    "href": "Stats_review.html#some-probability-notation",
    "title": "VectorByte Methods Training",
    "section": "Some probability notation",
    "text": "Some probability notation\nWe have a set, S of all possible events. Let \\text{Pr}(A) (or alternatively \\text{Prob}(A)) be the probability of event A. Then:\n\nA^c is the complement to A (all events that are not A).\nA \\cup B is the union of events A and B (“A or B”).\nA \\cap B is the intersection of events A and B (“A and B”).\n\\text{Pr}(A|B) is the conditional probability of A given that B occurs."
  },
  {
    "objectID": "Stats_review.html#axioms-of-probability",
    "href": "Stats_review.html#axioms-of-probability",
    "title": "VectorByte Methods Training",
    "section": "Axioms of Probability",
    "text": "Axioms of Probability\nThese are the basic definitions that we use when we talk about probabilities. You’ve probably seen these before, but maybe not in mathematical notation. If the notation is new to you, I suggest that you use the notation above to translate these statements into words and confirm that you understand what they mean. I give you an example for the first statement.\n\n\\sum_{i \\in S} \\text{Pr}(A_i)=1, where 0 \\leq \\text{Pr}(A_i) \\leq 1 (the probabilities of all the events that can happen must sum to one, and all of the individual probabilities must be less than one)\n\\text{Pr}(A)=1-\\text{Pr}(A^c)\n\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B) -\\text{Pr}(A \\cap B)\n\\text{Pr}(A \\cap B) = \\text{Pr}(A|B)\\text{Pr}(B)\nIf A and B are independent, then \\text{Pr}(A|B) = \\text{Pr}(A)"
  },
  {
    "objectID": "Stats_review.html#bayes-theorem",
    "href": "Stats_review.html#bayes-theorem",
    "title": "VectorByte Methods Training",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nBayes Theorem allows us to related the conditional probabilities of two events A and B:\n\\begin{align*}\n\\text{Pr}(A|B) & = \\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B)}\\\\\n&\\\\\n& =  \\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B|A)\\text{Pr}(A) + \\text{Pr}(B|A^c)\\text{Pr}(A^c)}\n\\end{align*}"
  },
  {
    "objectID": "Stats_review.html#discrete-rvs-and-their-probability-distributions",
    "href": "Stats_review.html#discrete-rvs-and-their-probability-distributions",
    "title": "VectorByte Methods Training",
    "section": "Discrete RVs and their Probability Distributions",
    "text": "Discrete RVs and their Probability Distributions\nMany things that we observe are naturally discrete. For instance, whole numbers of chairs or win/loss outcomes for games. Discrete probability distributions are used to describe these kinds of events.\nFor discrete RVs, the distribution of probabilities is described by the probability mass function (pmf), f_k such that:\n\\begin{align*}\nf_k  \\equiv \\text{Pr}(X & = k) \\\\\n\\text{where } 0\\leq f_k \\leq 1 & \\text{ and } \\sum_k f_k = 1\n\\end{align*}\nFor example, for a fair 6-sided die:\nf_k = 1/6 for k= \\{1,2,3,4,5,6\\}.\n\\star Question 1: For the six-sided fair die, what is f_k if k=7? k=1.5?\nRelated to the pmf is the cumulative distribution function (cdf), F(x). F(x) \\equiv \\text{Pr}(X \\leq x)\nFor the 6-sided die F(x)= \\displaystyle\\sum_{k=1}^{x} f_k\nwhere x \\in 1\\dots 6.\n\\star Question 2: For the fair 6-sided die, what is F(3)? F(7)? F(1.5)?\n\nVisualizing distributions of discrete RVs in R\nExample: Imagine a RV can take values 1 through 10, each with probability 0.1:\n \n\nvals&lt;-seq(1,10, by=1)\npmf&lt;-rep(0.1, 10)\ncdf&lt;-pmf[1]\nfor(i in 2:10) cdf&lt;-c(cdf, cdf[i-1]+pmf[i])\npar(mfrow=c(1,2), bty=\"n\")\nbarplot(height=pmf, names.arg=vals, ylim=c(0, 1), main=\"pmf\", col=\"blue\")\nbarplot(height=cdf, names.arg=vals, ylim=c(0, 1), main=\"cdf\", col=\"red\")"
  },
  {
    "objectID": "Stats_review.html#continuous-rvs-and-their-probability-distributions",
    "href": "Stats_review.html#continuous-rvs-and-their-probability-distributions",
    "title": "VectorByte Methods Training",
    "section": "Continuous RVs and their Probability Distributions",
    "text": "Continuous RVs and their Probability Distributions\nThings are just a little different for continuous RVs. Instead we use the probability density function (pdf) of the RV, and denote it by f(x). It still describes how relatively likely are alternative values of an RV – that is, if the pdf his higher around one value than around another, then the first is more likely to happen. However, the pdf does not return a probability, it is a function that describes the probability density.\nAn analogy:\nProbabilities are like weights of objects. The PMF tells you how much weight each possible value or outcome contributes to a whole. The PDF tells you how dense it is around a value. To calculate the weight of a real object, you need to also know the size of the area that you’re interested in and the density there The probability that your RV takes exactly any value is zero, just like the probability that any atom in a very thin wire is lined up at exactly that position is zero (and to the amount of mass at that location is zero). However, you can take a very thin slice around that location to see how much material is there.\nRelated to the pdf is the cumulative distribution function (cdf), F(x). \nF(x) \\equiv \\text{Pr}(X \\leq x)\n For a continuous distribution: \nF(x)= \\int_{-\\infty}^x f(x')dx'\n\n \n For a normal distribution with mean 0, what is F(0)?\n \n\nVisualizing distributions of continuous RVs in R\nExample: exponential RV, where f(x) = re^{-rx}:\n\n\nvals&lt;-seq(0,10, length=1000)\nr&lt;-0.5\npar(mfrow=c(1,2), bty=\"n\")\nplot(vals, dexp(vals, rate=r), main=\"pdf\", col=\"blue\", type=\"l\", lwd=3, ylab=\"\", xlab=\"\")\nplot(vals, pexp(vals, rate=r), main=\"cdf\", ylim=c(0,1), col=\"red\",\n     type=\"l\", lwd=3, ylab=\"\", xlab=\"\")"
  },
  {
    "objectID": "Stats_review.html#confidence-intervals",
    "href": "Stats_review.html#confidence-intervals",
    "title": "VectorByte Methods Training",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nSuppose Z_{n-p} \\sim t_{n-p}(0,1). A centered interval is on this t distribution can be written as: \\text{Pr}(-t_{n-p,\\alpha/2} \\&lt; Z\\_{n-p} \\&lt; t_{n-p,\\alpha/2}) = 1-\\alpha. That is, between these values of the t distribution (1-\\alpha)\\times 100 percent of the probability is contained in that symmetric interval. We can visually indicate these location on a plot of the t distribution (here with df=5 and \\alpha=0.05):\n\nx&lt;-seq(-4.5, 4.5, length=1000)\nalpha=0.05\n\n## draw a line showing the normal pdf on the histogram\nplot(x, dt(x, df=5), col=\"black\", lwd=2, type=\"l\", xlab=\"x\", ylab=\"\")\nabline(v=qt(alpha/2, df=5), col=3, lty=2, lwd=2)\nabline(v=qt(1-alpha/2, df=5), col=2, lty=2, lwd=2)\n\nlegend(\"topright\", \n       legend=c(\"t, df=5\", \"lower a/2\", \"upper a/2\"),\n       col=c(1,3,2), lwd=2, lty=c(1, 2,2))\n\n\n\n\n\n\n\n\nIn the R code here, {\\tt qt} is the Student-t “quantile function”. The function {\\tt qt(alpha, df)} returns a value z such that \\alpha = P(Z_{\\mathrm{df}} &lt; z), i.e., t_{\\mathrm{df},\\alpha}.\nHow can we use this to determine the confidence interval for \\theta? Since \\theta \\sim t_{n-p}(\\mu, s^2), we can replace the Z_{n-p} in the interval above with the definition in terms of \\theta, \\mu and s and rearrange: \\begin{align*}\n1-\\alpha& = \\text{Pr}\\left(-t_{n-p,\\alpha/2} &lt; \\frac{\\mu - \\bar{\\theta}}{s} &lt;\nt_{n-p,\\alpha/2}\\right) \\\\\n&=\n\\text{Pr}(\\bar{\\theta}-t_{n-p,\\alpha/2}s &lt; \\mu &lt;\n\\bar{\\theta} + t_{n-p,\\alpha/2}s)\n\\end{align*}\nThus (1-\\alpha)*100% of the time, \\mu is within the confidence interval (written in two equivalent ways):\n\\bar{\\theta} \\pm t_{n-p,\\alpha/2} \\times s \\;\\;\\; \\Leftrightarrow \\;\\;\\; \\bar{\\theta}-t_{n-p,\\alpha/2} \\times s, \\bar{\\theta} + t_{n-p,\\alpha/2}\\times s\nWhy should we care about confidence intervals?\n\nThe confidence interval captures the amount of information in the data about the parameter.\nThe center of the interval tells you what your estimate is.\nThe length of the interval tells you how sure you are about your estimate."
  },
  {
    "objectID": "Stats_review.html#p-values",
    "href": "Stats_review.html#p-values",
    "title": "VectorByte Methods Training",
    "section": "p-Values",
    "text": "p-Values\nWhat is a p-value? The American Statistical Association issued a statement where they defined it in the following way:\n“Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.” (ASA Statement on Statistical Significance and P-Values.)\nMore formally, we formulate a p-value in terms of a null hypothesis/model and test whether or not our observed data are more extreme than we would expect under that specific null model. In your previous courses you’ve probably seen very specific null models, corresponding to, for instance the null hypothesis that the mean of your data is normally distributed with mean m (often m=0). We often denote the null model as H_0 and the alternative as H_a or H_1. For instance, for our example above with \\theta we might want to test the following:\nH_0: \\bar{\\theta}=0 \\;\\;\\; \\text{vs.} \\;\\;\\; H_a: \\bar{\\theta}\\neq 0\nTo perform the hypothesis test we would FIRST choose our rejection level, \\alpha. Although convention is to use \\alpha =0.05 corresponding to a 95% confidence region, one could choose based on how sure one needs to be for a particular application. Next we build our test statistic. There are two cases, first if we know \\sigma and second if we don’t.\nIf we knew the variance \\sigma^2, our test statistic would be Z=\\frac{\\bar{\\theta}-0}{\\sigma}, and we expect that this should have a standard normal distribution, i.e., Z\\sim\\mathcal{N}(0,1). If we don’t know \\sigma and instead estimate is as s (which is most of the time), our test statistic would be Z_{df}=\\frac{\\bar{\\theta}-0}{s} (i.e., it would have a t-distribution).\nWe calculate the value of the appropriate statistic (either Z or Z_{df}) for our data, and then we compare it to the values of the standard distribution (normal or t, respectively) corresponding to the \\alpha level that we chose, i.e., we see if the number that we got for our statistic is inside the horizontal lines that we drew on the standard distribution above. If it is, then the data are consistent with the null hypothesis and we cannot reject the null. If the statistic is outside the region the data are NOT consistent with the null, and instead we reject the null and use the alternative as our new working hypothesis.\nNotice that this process is focused on the null hypothesis. We cannot tell if the alternative hypothesis is true, or, really, if it’s actually better than the null. We can only say that the null is not consistent with our data (i.e., we can falsify the null) at a given level of certainty.\nAlso, the hypothesis testing process is the same as building a confidence interval, as above, and then seeing if the null hypothesis is within your confidence interval. If the null is outside of your confidence interval then you can reject your null at the level of certainty corresponding to the \\alpha that you used to build your CI. If the value for the null is within your CI, you cannot reject at that level."
  },
  {
    "objectID": "Stats_review.html#the-sampling-distribution-1",
    "href": "Stats_review.html#the-sampling-distribution-1",
    "title": "VectorByte Methods Training",
    "section": "The Sampling Distribution",
    "text": "The Sampling Distribution\nSuppose we have a random sample \\{Y_i, i=1,\\dots,N \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,9) for i=1,\\ldots,N.\n\nWhat is the variance of the sample mean?\nWhat is the expectation of the sample mean?\nWhat is the variance for another i.i.d. realization Y_{ N+1}?\nWhat is the standard error of \\bar{Y}?"
  },
  {
    "objectID": "Stats_review.html#hypothesis-testing-and-confidence-intervals",
    "href": "Stats_review.html#hypothesis-testing-and-confidence-intervals",
    "title": "VectorByte Methods Training",
    "section": "Hypothesis Testing and Confidence Intervals",
    "text": "Hypothesis Testing and Confidence Intervals\nSuppose we sample some data \\{Y_i, i=1,\\dots,n \\}, where Y_i \\stackrel{\\mathrm{i.i.d.}}{\\sim}N(\\mu,\\sigma^2) for i=1,\\ldots,n, and that you want to test the null hypothesis H_0: ~\\mu=12 vs. the alternative H_a: \\mu \\neq 12, at the 0.05 significance level.\n\nWhat test statistic would you use? How do you estimate \\sigma?\nWhat is the distribution for this test statistic if the null is true?\nWhat is the distribution for the test statistic if the null is true and n \\rightarrow \\infty?\nDefine the test rejection region. (I.e., for what values of the test statistic would you reject the null?)\nHow would compute the p-value associated with a particular sample?\nWhat is the 95% confidence interval for \\mu? How should one interpret this interval?\nIf \\bar{Y} = 11, s_y = 1, and n=9, what is the test result? What is the 95% CI for \\mu?"
  }
]